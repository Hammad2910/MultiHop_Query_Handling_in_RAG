{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964feaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884f696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_english_question(english_question):\n",
    "    prompt = f\"\"\"You are given a question in English. Your task is to classify it as either SINGLEHOP or MULTIHOP.\n",
    "\n",
    "Definitions:\n",
    "- A question is **SINGLEHOP** if it can be answered using a single fact, sentence, or document. The question may be long, but if it doesn't require combining or reasoning over multiple distinct pieces of information, it's SINGLEHOP.\n",
    "- A question is **MULTIHOP** if answering it requires combining multiple facts, reasoning over several steps, or connecting pieces of information from different sources.\n",
    "\n",
    "âš ï¸ Important:\n",
    "- A question's length does not determine its type. A long, descriptive question can still be SINGLEHOP.\n",
    "- MULTIHOP questions typically require you to first find one piece of information, then use that to find the next.\n",
    "\n",
    "Examples:\n",
    "\n",
    "ðŸ”¹ Example 1 (SINGLEHOP - factual entity lookup):\n",
    "Question: \"Who wrote Pride and Prejudice?\"\n",
    "Answer: SINGLEHOP\n",
    "\n",
    "ðŸ”¹ Example 2 (SINGLEHOP - simple date):\n",
    "Question: \"When did the Berlin Wall fall?\"\n",
    "Answer: SINGLEHOP\n",
    "\n",
    "ðŸ”¹ Example 3 (SINGLEHOP - location fact):\n",
    "Question: \"Where is the Eiffel Tower located?\"\n",
    "Answer: SINGLEHOP\n",
    "\n",
    "ðŸ”¹ Example 4 (SINGLEHOP - biography-style):\n",
    "Question: \"What is the profession of Elon Musk?\"\n",
    "Answer: SINGLEHOP\n",
    "\n",
    "ðŸ”¹ Example 5 (SINGLEHOP - short explanation):\n",
    "Question: \"What is photosynthesis?\"\n",
    "Answer: SINGLEHOP\n",
    "\n",
    "ðŸ”¸ Example 6 (MULTIHOP - indirect relationship):\n",
    "Question: \"Which country was ruled by the emperor who built the Taj Mahal?\"\n",
    "Answer: MULTIHOP\n",
    "\n",
    "ðŸ”¸ Example 7 (MULTIHOP - event inference):\n",
    "Question: \"Which city hosted the Olympics where Michael Phelps won 8 gold medals?\"\n",
    "Answer: MULTIHOP\n",
    "\n",
    "ðŸ”¸ Example 8 (MULTIHOP - entity resolution):\n",
    "Question: \"Who is the father of the current King of the United Kingdom?\"\n",
    "Answer: MULTIHOP\n",
    "\n",
    "ðŸ”¸ Example 9 (MULTIHOP - causal historical link):\n",
    "Question: \"What war led to the independence of the United States?\"\n",
    "Answer: MULTIHOP\n",
    "\n",
    "ðŸ”¸ Example 10 (MULTIHOP - layered facts):\n",
    "Question: \"Which university did the author of 'The Selfish Gene' attend, and what subject did he study there?\"\n",
    "Answer: MULTIHOP\n",
    "\n",
    "---\n",
    "\n",
    "Now read the following English question and classify it:\n",
    "\n",
    "Question: \"{english_question}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=prompt\n",
    "        )\n",
    "        reply = response['response'].strip().lower()\n",
    "\n",
    "        # Normalize and return classification\n",
    "        if 'multihop' in reply:\n",
    "            return 'multihop'\n",
    "        elif 'singlehop' in reply or 'simple' in reply:\n",
    "            return 'singlehop'\n",
    "\n",
    "        print(f\"âš ï¸ Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing question: {english_question}\\nâ†ª {e}\")\n",
    "        return \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0cae6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_english_query(english_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "You are an expert in breaking down complex English questions into two logically ordered sub-questions.\n",
    "\n",
    "Rules:\n",
    "- Extract exactly 2 sub-questions.\n",
    "- q1 should logically precede q2.\n",
    "- Use clear and grammatically correct English.\n",
    "- Output only in this format:\n",
    "  q1: [First sub-question]\n",
    "  q2: [Second sub-question]\n",
    "\n",
    "Input: {english_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048  # Now you have more room for longer inputs\n",
    "            }\n",
    "        )\n",
    "\n",
    "        output = response['response'].strip()\n",
    "\n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "\n",
    "        return result if len(result) == 2 else {}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0821194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_context_relevance_check_en(query: str, context: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "You are a binary classifier.\n",
    "\n",
    "Your task is to decide whether the following *context* is relevant to the *question*. You must answer ONLY with **True** or **False** â€” no explanation, no commentary, just one word: True or False.\n",
    "\n",
    "Criteria:\n",
    "- If the context helps answer the question directly or indirectly, reply: True\n",
    "- If the context is unrelated, confusing, or insufficient, reply: False\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT explain your answer.\n",
    "- Do NOT include any additional comments.\n",
    "- Just respond with: True or False\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer (True/False):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        import ollama\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        return answer == 'true'\n",
    "    except Exception as e:\n",
    "        print(f\"Error during relevance check: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a073cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop_Project\\MultiHop_Query_Handling_in_RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer MiniLM model (lighter than intfloat/e5-large)\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "    \n",
    "    # You can optionally configure max length\n",
    "    model.max_seq_length = 512  # if needed for long sentences\n",
    "    model.tokenizer.do_lower_case = False  # Keep for Urdu if using custom tokenizer\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fe99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_index.faiss\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_parachunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7583f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=3):\n",
    "    # Encode the query using MiniLM model\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    _, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Return the top-k retrieved chunks\n",
    "    return [chunks_list[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "487f8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_llama3(context, query):\n",
    "    prompt = f\"\"\"You are a helpful assistant designed to generate precise and informative answers based strictly on the given context.\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Instruction:\n",
    "Answer the query using only the information present in the retrieved context. If the answer is not directly stated, make the best possible inference from the available context. Do not say \"no information available\", \"cannot answer\", or provide disclaimers. Only return a clear and direct answer â€” no introductions, no explanations, and no repetition of the query.\"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=prompt\n",
    "        )\n",
    "        return response['response'].strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        return \"Error generating answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a0ac96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_of_multihop_without_parallel(query,model=model,index=index,chunks_list=chunks_list,k=3):\n",
    "\n",
    "\n",
    "    classification = classify_english_question(query)\n",
    "\n",
    "\n",
    "    if classification == \"singlehop\":\n",
    "        retrieved_context = retrieve_documents(query,k)\n",
    "        return retrieved_context\n",
    "        \n",
    "\n",
    "    if classification == \"multihop\":\n",
    "        decomposition = decompose_english_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "\n",
    "        main_context = retrieve_documents(q1, k)\n",
    "\n",
    "        for i in range(min(len(main_context), k)):\n",
    "            intermediate_ctx = main_context[i]\n",
    "            \n",
    "            combined_query = q1 + intermediate_ctx + q2\n",
    "            \n",
    "            second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "            \n",
    "            for ctx in second_hop_contexts:\n",
    "                if query_context_relevance_check_en(query, ctx):\n",
    "                    main_context.append(ctx)\n",
    "        \n",
    "        return main_context    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f5796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def expand_multihop_context(intermediate_ctx, query, q1, q2, k):\n",
    "    try:\n",
    "        combined_query = q1 + intermediate_ctx + q2\n",
    "        second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "\n",
    "        relevant_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as inner_executor:\n",
    "            futures = [\n",
    "                inner_executor.submit(query_context_relevance_check_en, query, ctx)\n",
    "                for ctx in second_hop_contexts\n",
    "            ]\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        relevant_contexts.append(second_hop_contexts[i])\n",
    "                except Exception as e:\n",
    "                    print(\"Error during relevance check:\", e)\n",
    "\n",
    "        return relevant_contexts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in expand_multihop_context:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_context_of_multihop(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Measure classification time\n",
    "    start_classification = time.time()\n",
    "    classification = classify_english_question(query)\n",
    "    classification_time = time.time() - start_classification\n",
    "\n",
    "    if type == \"easy\":\n",
    "        decomposition_time = 0.0\n",
    "        start_retrieval = time.time()\n",
    "        context = retrieve_documents(query, k)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "        return context, classification, classification_time, decomposition_time, retrieval_time\n",
    "\n",
    "    else:\n",
    "        start_decomposition = time.time()\n",
    "        decomposition = decompose_english_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "        decomposition_time = time.time() - start_decomposition\n",
    "\n",
    "        start_retrieval = time.time()\n",
    "        main_context = retrieve_documents(q1, k)\n",
    "        additional_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in main_context[:k]\n",
    "            ]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        main_context.extend(additional_contexts)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "\n",
    "        return main_context, classification, classification_time, decomposition_time, retrieval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34141a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multihop_handling_LQR(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Step 1: Get context and timings\n",
    "    context, classification, classification_time, decomposition_time, retrieval_time = get_context_of_multihop(\n",
    "        query, type, model=model, index=index, chunks_list=chunks_list, k=k\n",
    "    )\n",
    "\n",
    "    # Flatten context if it's a list of strings\n",
    "    if isinstance(context, list):\n",
    "        combined_context = \"\\n\".join(context)\n",
    "    else:\n",
    "        combined_context = context\n",
    "\n",
    "    # Step 2: Generate answer and measure time\n",
    "    start_gen = time.time()\n",
    "    final_answer = generate_using_llama3(combined_context,query)\n",
    "    generation_time = time.time() - start_gen\n",
    "\n",
    "    # Step 3: Compute total time\n",
    "    total_time = classification_time + decomposition_time + retrieval_time + generation_time\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"timings\": {\n",
    "            \"classification_time\": classification_time,\n",
    "            \"decomposition_time\": decomposition_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcf285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/LQR_processed_results_en.csv\"\n",
    "\n",
    "# Initialize variables\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop over each question in the DataFrame\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row[\"question\"]\n",
    "    answer = row[\"answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Run the pipeline\n",
    "        result = multihop_handling_LQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Save and report every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(idx), index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save any remaining results at the end\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(len(df) % 100), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Final stats\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / len(df)\n",
    "print(f\"\\nâœ… All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66c35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
