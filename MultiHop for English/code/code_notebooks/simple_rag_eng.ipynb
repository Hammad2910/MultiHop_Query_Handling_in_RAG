{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00973ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop_Project\\MultiHop_Query_Handling_in_RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947ac4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer MiniLM model (lighter than intfloat/e5-large)\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "    \n",
    "    # You can optionally configure max length\n",
    "    model.max_seq_length = 512  # if needed for long sentences\n",
    "    model.tokenizer.do_lower_case = False  # Keep for Urdu if using custom tokenizer\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f96567",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_index.faiss\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_parachunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55770ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=3):\n",
    "    # Encode the query using MiniLM model\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    _, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Return the top-k retrieved chunks\n",
    "    return [chunks_list[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba1074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_llama3(context, query):\n",
    "    prompt = f\"\"\"You are a helpful assistant designed to generate precise and informative answers based strictly on the given context.\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Instruction:\n",
    "Answer the query using only the information present in the retrieved context. If the answer is not directly stated, make the best possible inference from the available context. Do not say \"no information available\", \"cannot answer\", or provide disclaimers. Only return a clear and direct answer — no introductions, no explanations, and no repetition of the query.\"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=prompt\n",
    "        )\n",
    "        return response['response'].strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        return \"Error generating answer.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2864dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query: str, k=3) -> str:\n",
    "    retrieved_chunks = retrieve_documents(query,k=k)\n",
    "    print(\"retrieved_chunks: \", retrieved_chunks)\n",
    "    answer = generate_using_llama3(query, retrieved_chunks)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f60cf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_chunks:  ['He is consistently ranked as one of the greatest and most influential filmmakers in American cinema.', 'A five-time nominee of the Academy Award for Best Director and an enduring figure from the New Hollywood era', 'and is cited in the media as one of the most popular and commercially successful actors of Indian cinema']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Consistently ranked as one of the greatest and most influential filmmakers in American cinema.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"what is best thing about hollywood\"\n",
    "ans=rag_pipeline(question)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e456788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  level                                           question          answer  \\\n",
      "0  easy  The 2017–18 Serie B (known as the Serie B ConT...            1929   \n",
      "1  easy  What specialism did Alec Naylor Dakin, fellow ...    cryptologist   \n",
      "2  easy  Which actor from Jurassic Park assisted David ...         BD Wong   \n",
      "3  easy  What character, voiced by Dan Castellaneta, st...  Grampa Simpson   \n",
      "4  easy  Who was a part of S#arp, Lee Ji-hye or Curtis ...      Lee Ji-hye   \n",
      "\n",
      "                                             context  \n",
      "0   A total of 22 teams are contesting the league...  \n",
      "1  Alec Naylor Dakin (3 April 1912 – 14 June 2003...  \n",
      "2   It was directed by Jerry Zaks, with B. D. Won...  \n",
      "3  \"Loan-a Lisa\" is the second episode of \"The Si...  \n",
      "4  Lee Ji-hye (born January 11, 1980) is a South ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load only the required columns\n",
    "df = pd.read_csv('../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/English/598_QnAs.csv', usecols=[\n",
    "    'level', 'question', 'answer', 'actual_retrieved_sentences'\n",
    "])\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={'actual_retrieved_sentences': 'context'}, inplace=True)\n",
    "\n",
    "# Optional: View the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize empty columns\n",
    "df['retrieved_context'] = \"\"\n",
    "df['final_answer'] = \"\"\n",
    "df['retriever_time'] = 0.0\n",
    "df['generator_time'] = 0.0\n",
    "df['total_time'] = 0.0\n",
    "\n",
    "# Relative path to output directory\n",
    "output_dir = \"../../results/pipeline results/5884paras_598qna\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_csv = os.path.join(output_dir, \"simple_rag_qna_results_GPU_version.csv\")\n",
    "\n",
    "# Timing variables\n",
    "total_start_time = time.time()\n",
    "batch_start_time = time.time()\n",
    "processed_count = 0\n",
    "\n",
    "print(f\"Starting processing of {len(df)} records at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    record_start_time = time.time()\n",
    "    query = row['translated_question']\n",
    "    \n",
    "    # Print current record being processed\n",
    "    print(f\"\\nProcessing record {i+1}...\")  # Show first 50 chars of query\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retriever_start = time.time()\n",
    "    retrieved_chunks = retrieve_documents(query, k=3)\n",
    "    retriever_time = time.time() - retriever_start\n",
    "    \n",
    "    # Generate answer\n",
    "    generator_start = time.time()\n",
    "    final_answer = generate_using_llama3(query, \"\\n\".join(retrieved_chunks))\n",
    "    generator_time = time.time() - generator_start\n",
    "    \n",
    "    # Update dataframe\n",
    "    df.at[i, 'retrieved_context'] = \"\\n\".join(retrieved_chunks)\n",
    "    df.at[i, 'final_answer'] = final_answer\n",
    "    df.at[i, 'retriever_time'] = retriever_time\n",
    "    df.at[i, 'generator_time'] = generator_time\n",
    "    df.at[i, 'total_time'] = time.time() - record_start_time\n",
    "    \n",
    "    # Print record processing time\n",
    "    print(f\"Completed record {i+1} in {df.at[i, 'total_time']:.2f}s \"\n",
    "          f\"(Retriever: {retriever_time:.2f}s, Generator: {generator_time:.2f}s)\")\n",
    "    \n",
    "    # Save progress every 100 records\n",
    "    if (i + 1) % 100 == 0 or (i + 1) == len(df):\n",
    "        batch_end_time = time.time()\n",
    "        batch_duration = batch_end_time - batch_start_time\n",
    "        processed_count = min(100, (i+1) - (i//100)*100)  # Handle partial batches\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"BATCH SUMMARY: Records {(i//100)*100 + 1}-{i+1}\")\n",
    "        print(f\"Batch processing time: {timedelta(seconds=batch_duration)}\")\n",
    "        print(f\"Average time per record: {batch_duration/processed_count:.2f}s\")\n",
    "        print(f\"Current timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save batch\n",
    "        df.iloc[max(0, i-99):i+1].to_csv(\n",
    "            output_csv,\n",
    "            mode='a',\n",
    "            header=not os.path.exists(output_csv),\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\"\n",
    "        )\n",
    "        print(f\"Saved batch to: {os.path.abspath(output_csv)}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "\n",
    "# Final statistics\n",
    "total_duration = time.time() - total_start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PROCESSING COMPLETED: {len(df)} records\")\n",
    "print(f\"Total processing time: {timedelta(seconds=total_duration)}\")\n",
    "print(f\"Average time per record: {total_duration/len(df):.2f}s\")\n",
    "print(f\"Total retriever time: {df['retriever_time'].sum():.2f}s\")\n",
    "print(f\"Total generator time: {df['generator_time'].sum():.2f}s\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
