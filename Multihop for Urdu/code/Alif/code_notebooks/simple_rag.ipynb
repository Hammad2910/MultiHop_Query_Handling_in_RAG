{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b32e1b",
   "metadata": {},
   "source": [
    "RAG Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bac8ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb501f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str,\n",
    "    model_path: str = 'C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\embedding_model'\n",
    "):\n",
    "    # Load sentence transformer model\n",
    "    if os.path.exists(model_path):\n",
    "        model = SentenceTransformer(model_path)\n",
    "    else:\n",
    "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "\n",
    "    return model, index, chunks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c4bed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list = load_retriever(\n",
    "        index_path=\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\vector_db\\\\paragraphs\\\\urdu_faiss_index_for_100_para.index\",\n",
    "        chunks_path=\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\data_storage\\\\parachunks\\\\urdu_chunks_for_100_para.pkl\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "554fc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, model, index, chunks_list, k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    retrieved_chunks = [chunks_list[i] for i in I[0]]\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928916c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:03<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\Lughat_model\"\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e050c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_lughat(context, query, max_new_tokens=200):\n",
    "    prompt = f\"\"\"آپ کو ایک سوال اور اس سے متعلق ایک سیاق و سباق دیا گیا ہے۔ براہ کرم سیاق و سباق کا بغور مطالعہ کریں اور اسی کی بنیاد پر درست، مختصر اور جامع جواب دیں۔\n",
    "\n",
    "### سوال:\n",
    "{query}\n",
    "\n",
    "### سیاق و سباق:\n",
    "{context}\n",
    "\n",
    "### جواب:\n",
    "\"\"\"\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n",
    "    outputs = gen_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return response.split(\"### جواب:\")[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9eb36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_alif(query, context, alif_model='hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16'):\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"آپ کو ایک سوال اور اس سے متعلق ایک سیاق و سباق دیا گیا ہے۔ براہ کرم سیاق و سباق کا بغور مطالعہ کریں اور اسی کی بنیاد پر درست، مختصر اور جامع جواب دیں۔\n",
    "\n",
    "### سوال:\n",
    "{query}\n",
    "\n",
    "### سیاق و سباق:\n",
    "{context}\n",
    "\n",
    "### جواب:\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=alif_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query: str, k=3) -> str:\n",
    "\n",
    "    retrieved_chunks = retrieve_documents(query, model, index, chunks_list, k=k)\n",
    "    # answer = generate_using_lughat(query, retrieved_chunks)\n",
    "    answer=generate_using_alif(query, retrieved_chunks)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfe43bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve_documents: ['سیری بی (اسپانسرشپ وجوہات کی بناء پر سیری بی یورو بی) کا مقابلہ کیا گیا تھا 82 ویں سیزن اس کے قیام کے بعد سے ، مجموعی طور پر 22 ٹیموں نے لیگ میں مقابلہ کیا: جن میں سے 15 2012-2013 کے سیزن سے واپس آرہی تھیں ، جن میں سے 4 کو لیگا پرو پریما ڈویژن سے', 'سیری بی (اسپانسرشپ کی وجوہات کی بناء پر سیری بی کے نام سے جانا جاتا ہے) اس کی بنیاد پر 81 ویں سیزن ہے ، جس میں مجموعی طور پر 22 ٹیمیں مقابلہ کریں گی: جن میں سے 15 2011 سے واپس آ رہی ہیں ، جن میں سے 4 لیگ پرو پریما ڈویژن سے ترقی یافتہ ہیں ، اور سیری', 'سیری بی (اسپانسرشپ وجوہات کی بناء پر سیری بی) کا مقابلہ کرنے والی لیگ کی 86 ویں سیزن ہے۔ اس میں مجموعی طور پر 22 ٹیمیں حصہ لے رہی ہیں: 15 2016 سے واپس آ رہی ہیں ، 4 لیگا پرو سے ترقی یافتہ ہیں ، اور 3 سیری اے سے نیچے گر چکے ہیں۔']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'سری بی (اسپانسرشپ کی وجوہات کی بنا پر سری بی کونٹیننٹ) لیگ کا 86 واں سیزن 2017 - 17 (اسپانسر شپ کی وجوہات کی بناء پر سیری بی کونٹیننٹ کے نام سے جانا جاتا ہے)، جو اگست 2017 میں شروع ہوا تھا، اور اس وقت تک چل رہا ہے.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"سیری بی 2017 - 2017 (اسپانسرشپ کی وجوہات کی بناء پر سیری بی کونٹٹ) لیگ کا 86 واں سیزن ہے ، جس میں مجموعی طور پر 22 ٹیمیں مقابلہ کر رہی ہیں: 15 2016 - 17 سیزن سے واپس آرہی ہیں ، 2016 - 17 سیری بی ، جو اسپانسرشپ کی وجوہات کی بناء پر سیری بی کونٹٹٹ کے نام سے جانا جاتا ہے ، اور اس کے قیام کے بعد سے 85 واں سیزن تھا۔\"\n",
    "ans=rag_pipeline(question)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e273272",
   "metadata": {},
   "source": [
    "RAG Pipeline testing on Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b2c50c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  level                                translated_question translated_answer  \\\n",
      "0  easy  سیری بی 2017 - 2017 (اسپانسرشپ کی وجوہات کی بن...         1929ء میں   \n",
      "1  easy  \"آکسفورڈ کالج کے ایک ساتھی ایلک نیلر ڈکن نے کہ...       کرپٹوولوجسٹ   \n",
      "2  easy  \"جراسک پارک کے اداکار ڈیوڈ ہنری ہوانگ نے \"\"دی ...        بی ڈی وانگ   \n",
      "3  easy  کون سا کردار ، ڈین کاسٹیلینیٹا کی آواز ، سمپسن...        دادا سمپسن   \n",
      "4  easy     کون تھا ایک حصہ S#arp، لی Ji-hye یا کرٹس رائٹ؟          لی جی ہے   \n",
      "\n",
      "                                  translated_context  \n",
      "0  لیگ میں مجموعی طور پر 22 ٹیمیں حصہ لے رہی ہیں:...  \n",
      "1  ایلکلر ڈکن (انگریزی: Alec Dakin) (۳ اپریل ۱۹۱۲...  \n",
      "2  \"جیرز زکس نے اس فلم کی ہدایت کاری کی تھی جس می...  \n",
      "3  \"سیمپسنز کے 22 ویں سیزن کی دوسری قسط \"\" لون اے...  \n",
      "4  لی جی ہیو (پیدائش 11 جنوری ، 1980) ایک جنوبی ک...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load only the required columns\n",
    "df = pd.read_csv('C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\Dataset\\\\Hotpotqa\\\\1000_paras_100_queries\\\\translated_dataset_100_qna.csv', usecols=[\n",
    "    'level', 'translated_question', 'translated_answer', 'translated_retrieved_sentences'\n",
    "])\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={'translated_retrieved_sentences': 'translated_context'}, inplace=True)\n",
    "\n",
    "# Optional: View the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05047910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/98 rows\n",
      "Processed 2/98 rows\n",
      "Processed 3/98 rows\n",
      "Processed 4/98 rows\n",
      "Processed 5/98 rows\n",
      "Processed 6/98 rows\n",
      "Processed 7/98 rows\n",
      "Processed 8/98 rows\n",
      "Processed 9/98 rows\n",
      "Processed 10/98 rows\n",
      "Processed 11/98 rows\n",
      "Processed 12/98 rows\n",
      "Processed 13/98 rows\n",
      "Processed 14/98 rows\n",
      "Processed 15/98 rows\n",
      "Processed 16/98 rows\n",
      "Processed 17/98 rows\n",
      "Processed 18/98 rows\n",
      "Processed 19/98 rows\n",
      "Processed 20/98 rows\n",
      "Processed 21/98 rows\n",
      "Processed 22/98 rows\n",
      "Processed 23/98 rows\n",
      "Processed 24/98 rows\n",
      "Processed 25/98 rows\n",
      "Processed 26/98 rows\n",
      "Processed 27/98 rows\n",
      "Processed 28/98 rows\n",
      "Processed 29/98 rows\n",
      "Processed 30/98 rows\n",
      "Processed 31/98 rows\n",
      "Processed 32/98 rows\n",
      "Processed 33/98 rows\n",
      "Processed 34/98 rows\n",
      "Processed 35/98 rows\n",
      "Processed 36/98 rows\n",
      "Processed 37/98 rows\n",
      "Processed 38/98 rows\n",
      "Processed 39/98 rows\n",
      "Processed 40/98 rows\n",
      "Processed 41/98 rows\n",
      "Processed 42/98 rows\n",
      "Processed 43/98 rows\n",
      "Processed 44/98 rows\n",
      "Processed 45/98 rows\n",
      "Processed 46/98 rows\n",
      "Processed 47/98 rows\n",
      "Processed 48/98 rows\n",
      "Processed 49/98 rows\n",
      "Processed 50/98 rows\n",
      "Processed 51/98 rows\n",
      "Processed 52/98 rows\n",
      "Processed 53/98 rows\n",
      "Processed 54/98 rows\n",
      "Processed 55/98 rows\n",
      "Processed 56/98 rows\n",
      "Processed 57/98 rows\n",
      "Processed 58/98 rows\n",
      "Processed 59/98 rows\n",
      "Processed 60/98 rows\n",
      "Processed 61/98 rows\n",
      "Processed 62/98 rows\n",
      "Processed 63/98 rows\n",
      "Processed 64/98 rows\n",
      "Processed 65/98 rows\n",
      "Processed 66/98 rows\n",
      "Processed 67/98 rows\n",
      "Processed 68/98 rows\n",
      "Processed 69/98 rows\n",
      "Processed 70/98 rows\n",
      "Processed 71/98 rows\n",
      "Processed 72/98 rows\n",
      "Processed 73/98 rows\n",
      "Processed 74/98 rows\n",
      "Processed 75/98 rows\n",
      "Processed 76/98 rows\n",
      "Processed 77/98 rows\n",
      "Processed 78/98 rows\n",
      "Processed 79/98 rows\n",
      "Processed 80/98 rows\n",
      "Processed 81/98 rows\n",
      "Processed 82/98 rows\n",
      "Processed 83/98 rows\n",
      "Processed 84/98 rows\n",
      "Processed 85/98 rows\n",
      "Processed 86/98 rows\n",
      "Processed 87/98 rows\n",
      "Processed 88/98 rows\n",
      "Processed 89/98 rows\n",
      "Processed 90/98 rows\n",
      "Processed 91/98 rows\n",
      "Processed 92/98 rows\n",
      "Processed 93/98 rows\n",
      "Processed 94/98 rows\n",
      "Processed 95/98 rows\n",
      "Processed 96/98 rows\n",
      "Processed 97/98 rows\n",
      "Processed 98/98 rows\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Add empty columns first (optional, for visibility)\n",
    "df['retrieved_context'] = \"\"\n",
    "df['final_answer'] = \"\"\n",
    "df['retriever_time'] = 0.0\n",
    "df['generator_time'] = 0.0\n",
    "df['total_time'] = 0.0\n",
    "\n",
    "# Start processing rows\n",
    "for i, row in df.iterrows():\n",
    "    query = row['translated_question']\n",
    "    \n",
    "    # Track total processing time\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # ⏱️ Retrieve\n",
    "    retriever_start = time.time()\n",
    "    retrieved_chunks = retrieve_documents(query, model, index, chunks_list, k=3)\n",
    "    retriever_end = time.time()\n",
    "    \n",
    "    retrieved_context = \"\\n\".join(retrieved_chunks)  # Combine if it's a list\n",
    "    df.at[i, 'retrieved_context'] = retrieved_context\n",
    "    df.at[i, 'retriever_time'] = retriever_end - retriever_start\n",
    "\n",
    "    # ⏱️ Generate\n",
    "    generator_start = time.time()\n",
    "    final_answer = generate_using_alif(query, retrieved_context)\n",
    "    generator_end = time.time()\n",
    "     \n",
    "    df.at[i, 'final_answer'] = final_answer\n",
    "    df.at[i, 'generator_time'] = generator_end - generator_start\n",
    "\n",
    "    # ⏱️ Total Time\n",
    "    df.at[i, 'total_time'] = time.time() - total_start\n",
    "\n",
    "    # Print progress (how many processed so far)\n",
    "    print(f\"Processed {i + 1}/{len(df)} rows\")\n",
    "\n",
    "# Optionally save the results to a new CSV\n",
    "df.to_csv('C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\results\\\\Simple RAG QNA results\\\\simple_rag_qna_results.csv', index=False,encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b6ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
