{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e01d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import sacrebleu\n",
    "from evaluate import load\n",
    "from bert_score import score\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fd3d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e87fc335ce64bc98f6821767ca31b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:06:08 INFO: Downloaded file to C:\\Users\\hamma\\stanza_resources\\resources.json\n",
      "2025-05-16 03:06:08 INFO: Downloading default packages for language: ur (Urdu) ...\n",
      "2025-05-16 03:06:10 INFO: File exists: C:\\Users\\hamma\\stanza_resources\\ur\\default.zip\n",
      "2025-05-16 03:06:16 INFO: Finished downloading models and saved to C:\\Users\\hamma\\stanza_resources\n",
      "2025-05-16 03:06:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e816e4de309c42c8a8a2437b1b7d1a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:06:16 INFO: Downloaded file to C:\\Users\\hamma\\stanza_resources\\resources.json\n",
      "2025-05-16 03:06:18 INFO: Loading these models for language: ur (Urdu):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | udtb          |\n",
      "| pos       | udtb_nocharlm |\n",
      "| lemma     | udtb_nocharlm |\n",
      "| depparse  | udtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-05-16 03:06:19 INFO: Using device: cuda\n",
      "2025-05-16 03:06:19 INFO: Loading: tokenize\n",
      "2025-05-16 03:06:20 INFO: Loading: pos\n",
      "2025-05-16 03:06:22 INFO: Loading: lemma\n",
      "2025-05-16 03:06:22 INFO: Loading: depparse\n",
      "2025-05-16 03:06:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"ur\")\n",
    "nlp = stanza.Pipeline(\"ur\")\n",
    "\n",
    "def urdu_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4880a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80aa15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Your CSVs ---\n",
    "vanilla_df = pd.read_csv(\"E:\\\\Study\\\\Thesis work\\\\Multihop for Urdu\\\\code\\\\evaluation\\\\processed results\\\\simple_rag_qna_results.csv\")\n",
    "lqr_df = pd.read_csv(\"E:\\\\Study\\\\Thesis work\\\\Multihop for Urdu\\\\code\\\\evaluation\\\\processed results\\\\LQR_processed_results_with_Alif.csv\")\n",
    "mod_lqr_df = pd.read_csv(\"E:\\\\Study\\\\Thesis work\\\\Multihop for Urdu\\\\code\\\\evaluation\\\\processed results\\\\modLQR_processed_results_with_Alif.csv\")\n",
    "\n",
    "# --- Ground Truth ---\n",
    "references = vanilla_df['translated_answer'].astype(str).tolist()\n",
    "\n",
    "# --- Predictions ---\n",
    "vanilla_preds = vanilla_df['final_answer'].astype(str).tolist()\n",
    "lqr_preds = lqr_df['final_answer'].astype(str).tolist()\n",
    "mod_lqr_preds = mod_lqr_df['final_answer'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5050fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Metric Functions ---\n",
    "def calculate_nltk_bleu(references, hypotheses):\n",
    "    scores = []\n",
    "    smooth = SmoothingFunction().method1\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        score = sentence_bleu([ref.split()], hyp.split(), smoothing_function=smooth)\n",
    "        scores.append(score * 100)\n",
    "    return scores\n",
    "\n",
    "def calculate_sacrebleu(references, hypotheses):\n",
    "    return [sacrebleu.sentence_bleu(hyp, [ref]).score for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    results = rouge.compute(\n",
    "        predictions=hypotheses,\n",
    "        references=references,\n",
    "        tokenizer=lambda x: x.split(),\n",
    "        use_aggregator=False\n",
    "    )\n",
    "    return results['rouge1'], results['rouge2'], results['rougeL']\n",
    "\n",
    "def calculate_meteor(references, hypotheses, urdu_tokenizer):\n",
    "    tokenized_references = [urdu_tokenizer(ref) for ref in references]\n",
    "    tokenized_hypotheses = [urdu_tokenizer(hyp) for hyp in hypotheses]\n",
    "    return [meteor_score([ref], hyp) for ref, hyp in zip(tokenized_references, tokenized_hypotheses)]\n",
    "\n",
    "def calculate_bert_score(references, hypotheses):\n",
    "    P, R, F1 = score(hypotheses, references, lang=\"ur\", model_type=\"xlm-roberta-large\")\n",
    "    return P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "def calculate_exact_match(references, hypotheses):\n",
    "    return [1 if ref.strip() == hyp.strip() else 0 for ref, hyp in zip(references, hypotheses)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ba4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(name, references, predictions):\n",
    "    print(f\"\\nðŸš€ Evaluating pipeline: {name}\")\n",
    "\n",
    "    # --- Metric Calculations ---\n",
    "    nltk_bleu = calculate_nltk_bleu(references, predictions)\n",
    "    sacrebleu_scores = calculate_sacrebleu(references, predictions)\n",
    "    rouge1, rouge2, rougeL = calculate_rouge(references, predictions)\n",
    "    meteor = calculate_meteor(references, predictions, urdu_tokenizer)\n",
    "    exact_match = calculate_exact_match(references, predictions)\n",
    "    bert_p, bert_r, bert_f1 = calculate_bert_score(references, predictions)\n",
    "\n",
    "    print(f\"âœ… All scores calculated for: {name}\")\n",
    "\n",
    "    # --- Build DataFrame ---\n",
    "    data = {\n",
    "        'pipeline': [name] * len(references),\n",
    "        'reference': references,\n",
    "        'prediction': predictions,\n",
    "        'nltk_bleu': nltk_bleu,\n",
    "        'sacrebleu': sacrebleu_scores,\n",
    "        'rouge1': rouge1,\n",
    "        'rouge2': rouge2,\n",
    "        'rougeL': rougeL,\n",
    "        'meteor': meteor,\n",
    "        'exact_match': exact_match,\n",
    "        'bert_precision': bert_p,\n",
    "        'bert_recall': bert_r,\n",
    "        'bert_f1': bert_f1\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for all 3 pipelines\n",
    "vanilla_results = evaluate_pipeline(\"Vanilla RAG\", references, vanilla_preds)\n",
    "lqr_results = evaluate_pipeline(\"LQR\", references, lqr_preds)\n",
    "mod_lqr_results = evaluate_pipeline(\"Modified LQR\", references, mod_lqr_preds)\n",
    "\n",
    "# Combine all detailed results\n",
    "all_results_df = pd.concat([vanilla_results, lqr_results, mod_lqr_results], ignore_index=True)\n",
    "\n",
    "# Preview or save\n",
    "print(\"\\nðŸ“‹ Sample of evaluation results:\")\n",
    "print(all_results_df.head())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "all_results_df.to_csv(\"all_rag_nlp_evaluation_results_with_alif.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
