{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf4b6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b32e1b",
   "metadata": {},
   "source": [
    "RAG Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb501f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer model (E5-large)\n",
    "    model = SentenceTransformer(\"intfloat/e5-large\", device=device)\n",
    "    \n",
    "    # Configure for Urdu text\n",
    "    model.max_seq_length = 512  # Set based on your earlier analysis\n",
    "    model.tokenizer.do_lower_case = False  # Preserve Urdu characters\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c4bed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_faiss_index.index\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_chunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "554fc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_documents(query,k=3):\n",
    "   \n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,  # Return numpy array for FAISS\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Search FAISS index\n",
    "    _, indices = index.search(query_embedding, k)  # Dummy variable _ for distances\n",
    "    \n",
    "    # Return only the chunks\n",
    "    return [chunks_list[i] for i in indices[0]]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prompt_llama_gemma_balanced_english(query, context):\n",
    "    return f\"\"\"\n",
    "You are an expert assistant. Please answer the following question in fluent and clear Urdu. If possible, avoid using English words. Do not use bullet points or lists. Write your response in concise paragraphs.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_using_gemma(query, context):\n",
    "    model_name = \"gemma3:4b\"\n",
    "    prompt = prompt_llama_gemma_balanced_english(query, context)\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        output = response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        output = f\"Error: {e}\"\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c97cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query: str, k=3) -> str:\n",
    "    retrieved_chunks = retrieve_documents(query,k=k)\n",
    "    print(\"retrieved_chunks: \", retrieved_chunks)\n",
    "    answer = generate_using_gemma(query, retrieved_chunks)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cfe43bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_chunks:  ['سیری بی (اسپانسرشپ وجوہات کی بناء پر سیری بی) کا مقابلہ کرنے والی لیگ کی 86 ویں سیزن ہے۔ اس میں مجموعی طور پر 22 ٹیمیں حصہ لے رہی ہیں: 15 2016 سے واپس آ رہی ہیں ، 4 لیگا پرو سے ترقی یافتہ ہیں ، اور 3 سیری اے سے نیچے گر چکے ہیں۔', '۔ اس کا بڑا بھائی ، اساک برینسٹروم ، بھی ایک ایس ایچ ایل کھلاڑی ہے ، فی الحال ایچ ایچ ایل کے ساتھ۔ وہ 2017 کے این ایچ ایل داخلہ ڈرافٹ میں گولڈن نائٹس کے ذریعہ مجموعی طور پر 15 ویں نمبر پر منتخب کیا گیا تھا۔', '۔ اس سیریز کو سارہ ڈن نے تخلیق کیا اور لکھا اور ہارون کیپلن ، کینی شوارٹز ، ریک وینر ، اور روبن فلیشر کے ساتھ شریک ایگزیکٹو پروڈیوسر ، کیپٹل انٹرٹینمنٹ اسٹوڈیوز کے ساتھ مشترکہ پروڈکشن۔ اس سیریز کا ایک پیش نظارہ 17 مئی ، 2016 کو جاری کیا گیا تھا']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'سیری بی، ایک طویل عرصے سے جاری رہنے والی مشہور لیگ ہے اور اس کا 86واں سیزن رواں سال کھیلا جارہا ہے۔ اس لیگ میں مجموعی طور پر 22 ٹیمیں حصہ لے رہی ہیں۔ اس میں 15 ٹیمیں 2016 کے سیزن سے دوبارہ شامل ہوئی ہیں، جبکہ چار ٹیمیں لیگا پرو سے ترقی کے ذریعے اور تین ٹیمیں سیری اے سے گر کر اس میں شامل ہوئی ہیں۔ یہ لیگ، جو اسپانسرشپ کی بنیاد پر سیری بی کونٹٹٹ کے نام سے بھی معروف ہے، اپنے قیام کے بعد سے مسلسل اپنے سیزن جاری رکھے ہوئے ہے۔ یہ سیزن سارہ ڈن کی تخلیق اور ہارون کیپلن اور دیگر کے تعاون سے ایک اہم قدم ثابت ہوگا۔'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"سیری بی 2017 - 2017 (اسپانسرشپ کی وجوہات کی بناء پر سیری بی کونٹٹ) لیگ کا 86 واں سیزن ہے ، جس میں مجموعی طور پر 22 ٹیمیں مقابلہ کر رہی ہیں: 15 2016 - 17 سیزن سے واپس آرہی ہیں ، 2016 - 17 سیری بی ، جو اسپانسرشپ کی وجوہات کی بناء پر سیری بی کونٹٹٹ کے نام سے جانا جاتا ہے ، اور اس کے قیام کے بعد سے 85 واں سیزن تھا۔\"\n",
    "ans=rag_pipeline(question)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e273272",
   "metadata": {},
   "source": [
    "RAG Pipeline testing on Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6a6cc4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100paras_100qna', '5884paras_598qna']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../../results/pipeline results\"))  # Show top-level folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1b2c50c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  level                                translated_question translated_answer  \\\n",
      "0  easy  سیری بی 2017 - 2017 (اسپانسرشپ کی وجوہات کی بن...         1929ء میں   \n",
      "1  easy  \"آکسفورڈ کالج کے ایک ساتھی ایلک نیلر ڈکن نے کہ...       کرپٹوولوجسٹ   \n",
      "2  easy  \"جراسک پارک کے اداکار ڈیوڈ ہنری ہوانگ نے \"\"دی ...        بی ڈی وانگ   \n",
      "3  easy  کون سا کردار ، ڈین کاسٹیلینیٹا کی آواز ، سمپسن...        دادا سمپسن   \n",
      "4  easy     کون تھا ایک حصہ S#arp، لی Ji-hye یا کرٹس رائٹ؟          لی جی ہے   \n",
      "\n",
      "                                  translated_context  \n",
      "0  لیگ میں مجموعی طور پر 22 ٹیمیں حصہ لے رہی ہیں:...  \n",
      "1  ایلکلر ڈکن (انگریزی: Alec Dakin) (۳ اپریل ۱۹۱۲...  \n",
      "2  \"جیرز زکس نے اس فلم کی ہدایت کاری کی تھی جس می...  \n",
      "3  \"سیمپسنز کے 22 ویں سیزن کی دوسری قسط \"\" لون اے...  \n",
      "4  لی جی ہیو (پیدائش 11 جنوری ، 1980) ایک جنوبی ک...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load only the required columns\n",
    "df = pd.read_csv('../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv', usecols=[\n",
    "    'level', 'translated_question', 'translated_answer', 'translated_retrieved_sentences'\n",
    "])\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={'translated_retrieved_sentences': 'translated_context'}, inplace=True)\n",
    "\n",
    "# Optional: View the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f845ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing of 598 records at 2025-05-18 06:22:32\n",
      "================================================================================\n",
      "\n",
      "Processing record 1...\n",
      "Completed record 1 in 30.30s (Retriever: 0.25s, Generator: 30.05s)\n",
      "\n",
      "Processing record 2...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize empty columns\n",
    "df['retrieved_context'] = \"\"\n",
    "df['final_answer'] = \"\"\n",
    "df['retriever_time'] = 0.0\n",
    "df['generator_time'] = 0.0\n",
    "df['total_time'] = 0.0\n",
    "\n",
    "# Relative path to output directory\n",
    "output_dir = \"../../results/pipeline results/5884paras_598qna\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_csv = os.path.join(output_dir, \"simple_rag_qna_results.csv\")\n",
    "\n",
    "# Timing variables\n",
    "total_start_time = time.time()\n",
    "batch_start_time = time.time()\n",
    "processed_count = 0\n",
    "\n",
    "print(f\"Starting processing of {len(df)} records at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    record_start_time = time.time()\n",
    "    query = row['translated_question']\n",
    "    \n",
    "    # Print current record being processed\n",
    "    print(f\"\\nProcessing record {i+1}...\")  # Show first 50 chars of query\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retriever_start = time.time()\n",
    "    retrieved_chunks = retrieve_documents(query, k=3)\n",
    "    retriever_time = time.time() - retriever_start\n",
    "    \n",
    "    # Generate answer\n",
    "    generator_start = time.time()\n",
    "    final_answer = generate_using_gemma(query, \"\\n\".join(retrieved_chunks))\n",
    "    generator_time = time.time() - generator_start\n",
    "    \n",
    "    # Update dataframe\n",
    "    df.at[i, 'retrieved_context'] = \"\\n\".join(retrieved_chunks)\n",
    "    df.at[i, 'final_answer'] = final_answer\n",
    "    df.at[i, 'retriever_time'] = retriever_time\n",
    "    df.at[i, 'generator_time'] = generator_time\n",
    "    df.at[i, 'total_time'] = time.time() - record_start_time\n",
    "    \n",
    "    # Print record processing time\n",
    "    print(f\"Completed record {i+1} in {df.at[i, 'total_time']:.2f}s \"\n",
    "          f\"(Retriever: {retriever_time:.2f}s, Generator: {generator_time:.2f}s)\")\n",
    "    \n",
    "    # Save progress every 100 records\n",
    "    if (i + 1) % 100 == 0 or (i + 1) == len(df):\n",
    "        batch_end_time = time.time()\n",
    "        batch_duration = batch_end_time - batch_start_time\n",
    "        processed_count = min(100, (i+1) - (i//100)*100)  # Handle partial batches\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"BATCH SUMMARY: Records {(i//100)*100 + 1}-{i+1}\")\n",
    "        print(f\"Batch processing time: {timedelta(seconds=batch_duration)}\")\n",
    "        print(f\"Average time per record: {batch_duration/processed_count:.2f}s\")\n",
    "        print(f\"Current timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Save batch\n",
    "        df.iloc[max(0, i-99):i+1].to_csv(\n",
    "            output_csv,\n",
    "            mode='a',\n",
    "            header=not os.path.exists(output_csv),\n",
    "            index=False,\n",
    "            encoding=\"utf-8-sig\"\n",
    "        )\n",
    "        print(f\"Saved batch to: {os.path.abspath(output_csv)}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "\n",
    "# Final statistics\n",
    "total_duration = time.time() - total_start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PROCESSING COMPLETED: {len(df)} records\")\n",
    "print(f\"Total processing time: {timedelta(seconds=total_duration)}\")\n",
    "print(f\"Average time per record: {total_duration/len(df):.2f}s\")\n",
    "print(f\"Total retriever time: {df['retriever_time'].sum():.2f}s\")\n",
    "print(f\"Total generator time: {df['generator_time'].sum():.2f}s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_alif(query, context, alif_model='hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16'):\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"آپ کو ایک سوال اور اس سے متعلق ایک سیاق و سباق دیا گیا ہے۔ براہ کرم سیاق و سباق کا بغور مطالعہ کریں اور اسی کی بنیاد پر درست، مختصر اور جامع جواب دیں۔\n",
    "\n",
    "### سوال:\n",
    "{query}\n",
    "\n",
    "### سیاق و سباق:\n",
    "{context}\n",
    "\n",
    "### جواب:\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=alif_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
