{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8749f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Example 1: Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ø¨Ø§Ù†ÛŒ Ú©ÙˆÙ† ØªÚ¾Û’ØŸ\n",
      "ğŸ“„ Context: Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ø¨Ø§Ù†ÛŒ Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ Ø¬Ù†Ø§Ø­ ØªÚ¾Û’Û” Ø§Ù†ÛÙˆÚº Ù†Û’ 14 Ø§Ú¯Ø³Øª 1947 Ú©Ùˆ ...\n",
      "\n",
      "ğŸ§  LLAMA3 (19.24s):\n",
      "Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ Ø¬Ù†Ø§Ø­ ØªÚ¾Û’\n",
      "\n",
      "ğŸ§  GEMMA (8.8s):\n",
      "Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ Ø¬Ù†Ø§Ø­ ØªÚ¾Û’Û”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import ollama\n",
    "\n",
    "# === Urdu Examples ===\n",
    "examples = [\n",
    "    {\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ø¨Ø§Ù†ÛŒ Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ Ø¬Ù†Ø§Ø­ ØªÚ¾Û’Û” Ø§Ù†ÛÙˆÚº Ù†Û’ 14 Ø§Ú¯Ø³Øª 1947 Ú©Ùˆ Ø¢Ø²Ø§Ø¯ÛŒ Ø­Ø§ØµÙ„ Ú©ÛŒÛ”\",\n",
    "        \"question\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ø¨Ø§Ù†ÛŒ Ú©ÙˆÙ† ØªÚ¾Û’ØŸ\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Ø§Ø±Ø¯Ùˆ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ù‚ÙˆÙ…ÛŒ Ø²Ø¨Ø§Ù† ÛÛ’ Ø§ÙˆØ± ÛŒÛ ÙØ§Ø±Ø³ÛŒØŒ Ø¹Ø±Ø¨ÛŒ Ø§ÙˆØ± ØªØ±Ú©ÛŒ Ú©Û’ Ø§Ù„ÙØ§Ø¸ Ø³Û’ Ù…ØªØ§Ø«Ø± ÛÛ’Û”\",\n",
    "        \"question\": \"Ø§Ø±Ø¯Ùˆ Ø²Ø¨Ø§Ù† Ù¾Ø± Ú©Ù† Ø²Ø¨Ø§Ù†ÙˆÚº Ú©Ø§ Ø§Ø«Ø± ÛÛ’ØŸ\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Ù„Ø§ÛÙˆØ± Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø§ÛŒÚ© ØªØ§Ø±ÛŒØ®ÛŒ Ø´ÛØ± ÛÛ’ Ø¬ÛØ§Úº Ù…ØºÙ„ Ø¯ÙˆØ± Ú©ÛŒ Ú©Ø¦ÛŒ Ø¹Ù…Ø§Ø±ØªÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºÛ”\",\n",
    "        \"question\": \"Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº Ú©Ø³ Ø¯ÙˆØ± Ú©ÛŒ Ø¹Ù…Ø§Ø±ØªÛŒÚº Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºØŸ\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# === Prompt Templates ===\n",
    "\n",
    "llm_prompt_template = (\n",
    "    \"You are given a question and a context in Urdu. Please carefully read the context and provide a correct, short, and concise answer in **Urdu** only.\\n\\n\"\n",
    "    \"### Question:\\n{question}\\n\\n### Context:\\n{context}\\n\\n### Answer:\"\n",
    ")\n",
    "\n",
    "alif_prompt_template = (\n",
    "    \"Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø§Ø³ Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø§ÛŒÚ© Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Ø§ Ø¨ØºÙˆØ± Ù…Ø·Ø§Ù„Ø¹Û Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ø³ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø¯Ø±Ø³ØªØŒ Ù…Ø®ØªØµØ± Ø§ÙˆØ± Ø¬Ø§Ù…Ø¹ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\\n\\n\"\n",
    "    \"### Ø³ÙˆØ§Ù„:\\n{question}\\n\\n### Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚:\\n{context}\\n\\n### Ø¬ÙˆØ§Ø¨:\"\n",
    ")\n",
    "\n",
    "# === Model Inference ===\n",
    "def run_ollama_chat(model_name, prompt):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        output = response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        output = f\"âŒ Error: {e}\"\n",
    "    duration = round(time.time() - start, 2)\n",
    "    return output, duration\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"llama3\": {\n",
    "        \"name\": \"llama3:8b\",\n",
    "        \"prompt_template\": llm_prompt_template\n",
    "    },\n",
    "    \"gemma\": {\n",
    "        \"name\": \"gemma3:4b\",\n",
    "        \"prompt_template\": llm_prompt_template\n",
    "    },\n",
    "    \"alif\": {\n",
    "        \"name\": \"hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16\",\n",
    "        \"prompt_template\": alif_prompt_template\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop over examples and models\n",
    "for i, example in enumerate(examples):\n",
    "    print(f\"\\nğŸ”¹ Example {i+1}: {example['question']}\")\n",
    "    print(f\"ğŸ“„ Context: {example['context'][:60]}...\\n\")\n",
    "\n",
    "    for key, config in models.items():\n",
    "        model_name = config[\"name\"]\n",
    "        prompt_template = config[\"prompt_template\"]\n",
    "        prompt = prompt_template.format(**example)\n",
    "\n",
    "        output, duration = run_ollama_chat(model_name, prompt)\n",
    "        print(f\"ğŸ§  {key.upper()} ({duration}s):\\n{output}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b32e1b",
   "metadata": {},
   "source": [
    "RAG Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bac8ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb501f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str,\n",
    "    model_path: str = 'C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\embedding_model'\n",
    "):\n",
    "    # Load sentence transformer model\n",
    "    if os.path.exists(model_path):\n",
    "        model = SentenceTransformer(model_path)\n",
    "    else:\n",
    "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "\n",
    "    return model, index, chunks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c4bed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list = load_retriever(\n",
    "        index_path=\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\vector_db\\\\paragraphs\\\\urdu_faiss_index_for_100_para.index\",\n",
    "        chunks_path=\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\data_storage\\\\parachunks\\\\urdu_chunks_for_100_para.pkl\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "554fc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, model, index, chunks_list, k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    retrieved_chunks = [chunks_list[i] for i in I[0]]\n",
    "    return retrieved_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928916c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:03<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\Lughat_model\"\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e050c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_lughat(context, query, max_new_tokens=200):\n",
    "    prompt = f\"\"\"Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø§Ø³ Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø§ÛŒÚ© Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Ø§ Ø¨ØºÙˆØ± Ù…Ø·Ø§Ù„Ø¹Û Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ø³ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø¯Ø±Ø³ØªØŒ Ù…Ø®ØªØµØ± Ø§ÙˆØ± Ø¬Ø§Ù…Ø¹ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\n",
    "\n",
    "### Ø³ÙˆØ§Ù„:\n",
    "{query}\n",
    "\n",
    "### Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚:\n",
    "{context}\n",
    "\n",
    "### Ø¬ÙˆØ§Ø¨:\n",
    "\"\"\"\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n",
    "    outputs = gen_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return response.split(\"### Ø¬ÙˆØ§Ø¨:\")[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9eb36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_alif(query, context, alif_model='hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16'):\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"Ø¢Ù¾ Ú©Ùˆ Ø§ÛŒÚ© Ø³ÙˆØ§Ù„ Ø§ÙˆØ± Ø§Ø³ Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø§ÛŒÚ© Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ø¯ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Ø§ Ø¨ØºÙˆØ± Ù…Ø·Ø§Ù„Ø¹Û Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ø³ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø¯Ø±Ø³ØªØŒ Ù…Ø®ØªØµØ± Ø§ÙˆØ± Ø¬Ø§Ù…Ø¹ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”\n",
    "\n",
    "### Ø³ÙˆØ§Ù„:\n",
    "{query}\n",
    "\n",
    "### Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚:\n",
    "{context}\n",
    "\n",
    "### Ø¬ÙˆØ§Ø¨:\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=alif_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query: str, k=3) -> str:\n",
    "\n",
    "    retrieved_chunks = retrieve_documents(query, model, index, chunks_list, k=k)\n",
    "    # answer = generate_using_lughat(query, retrieved_chunks)\n",
    "    answer=generate_using_alif(query, retrieved_chunks)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfe43bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve_documents: ['Ø³ÛŒØ±ÛŒ Ø¨ÛŒ (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§Ø¡ Ù¾Ø± Ø³ÛŒØ±ÛŒ Ø¨ÛŒ ÛŒÙˆØ±Ùˆ Ø¨ÛŒ) Ú©Ø§ Ù…Ù‚Ø§Ø¨Ù„Û Ú©ÛŒØ§ Ú¯ÛŒØ§ ØªÚ¾Ø§ 82 ÙˆÛŒÚº Ø³ÛŒØ²Ù† Ø§Ø³ Ú©Û’ Ù‚ÛŒØ§Ù… Ú©Û’ Ø¨Ø¹Ø¯ Ø³Û’ ØŒ Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø·ÙˆØ± Ù¾Ø± 22 Ù¹ÛŒÙ…ÙˆÚº Ù†Û’ Ù„ÛŒÚ¯ Ù…ÛŒÚº Ù…Ù‚Ø§Ø¨Ù„Û Ú©ÛŒØ§: Ø¬Ù† Ù…ÛŒÚº Ø³Û’ 15 2012-2013 Ú©Û’ Ø³ÛŒØ²Ù† Ø³Û’ ÙˆØ§Ù¾Ø³ Ø¢Ø±ÛÛŒ ØªÚ¾ÛŒÚº ØŒ Ø¬Ù† Ù…ÛŒÚº Ø³Û’ 4 Ú©Ùˆ Ù„ÛŒÚ¯Ø§ Ù¾Ø±Ùˆ Ù¾Ø±ÛŒÙ…Ø§ ÚˆÙˆÛŒÚ˜Ù† Ø³Û’', 'Ø³ÛŒØ±ÛŒ Ø¨ÛŒ (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§Ø¡ Ù¾Ø± Ø³ÛŒØ±ÛŒ Ø¨ÛŒ Ú©Û’ Ù†Ø§Ù… Ø³Û’ Ø¬Ø§Ù†Ø§ Ø¬Ø§ØªØ§ ÛÛ’) Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± 81 ÙˆÛŒÚº Ø³ÛŒØ²Ù† ÛÛ’ ØŒ Ø¬Ø³ Ù…ÛŒÚº Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø·ÙˆØ± Ù¾Ø± 22 Ù¹ÛŒÙ…ÛŒÚº Ù…Ù‚Ø§Ø¨Ù„Û Ú©Ø±ÛŒÚº Ú¯ÛŒ: Ø¬Ù† Ù…ÛŒÚº Ø³Û’ 15 2011 Ø³Û’ ÙˆØ§Ù¾Ø³ Ø¢ Ø±ÛÛŒ ÛÛŒÚº ØŒ Ø¬Ù† Ù…ÛŒÚº Ø³Û’ 4 Ù„ÛŒÚ¯ Ù¾Ø±Ùˆ Ù¾Ø±ÛŒÙ…Ø§ ÚˆÙˆÛŒÚ˜Ù† Ø³Û’ ØªØ±Ù‚ÛŒ ÛŒØ§ÙØªÛ ÛÛŒÚº ØŒ Ø§ÙˆØ± Ø³ÛŒØ±ÛŒ', 'Ø³ÛŒØ±ÛŒ Ø¨ÛŒ (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§Ø¡ Ù¾Ø± Ø³ÛŒØ±ÛŒ Ø¨ÛŒ) Ú©Ø§ Ù…Ù‚Ø§Ø¨Ù„Û Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ Ù„ÛŒÚ¯ Ú©ÛŒ 86 ÙˆÛŒÚº Ø³ÛŒØ²Ù† ÛÛ’Û” Ø§Ø³ Ù…ÛŒÚº Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø·ÙˆØ± Ù¾Ø± 22 Ù¹ÛŒÙ…ÛŒÚº Ø­ØµÛ Ù„Û’ Ø±ÛÛŒ ÛÛŒÚº: 15 2016 Ø³Û’ ÙˆØ§Ù¾Ø³ Ø¢ Ø±ÛÛŒ ÛÛŒÚº ØŒ 4 Ù„ÛŒÚ¯Ø§ Ù¾Ø±Ùˆ Ø³Û’ ØªØ±Ù‚ÛŒ ÛŒØ§ÙØªÛ ÛÛŒÚº ØŒ Ø§ÙˆØ± 3 Ø³ÛŒØ±ÛŒ Ø§Û’ Ø³Û’ Ù†ÛŒÚ†Û’ Ú¯Ø± Ú†Ú©Û’ ÛÛŒÚºÛ”']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ø³Ø±ÛŒ Ø¨ÛŒ (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§ Ù¾Ø± Ø³Ø±ÛŒ Ø¨ÛŒ Ú©ÙˆÙ†Ù¹ÛŒÙ†Ù†Ù¹) Ù„ÛŒÚ¯ Ú©Ø§ 86 ÙˆØ§Úº Ø³ÛŒØ²Ù† 2017 - 17 (Ø§Ø³Ù¾Ø§Ù†Ø³Ø± Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§Ø¡ Ù¾Ø± Ø³ÛŒØ±ÛŒ Ø¨ÛŒ Ú©ÙˆÙ†Ù¹ÛŒÙ†Ù†Ù¹ Ú©Û’ Ù†Ø§Ù… Ø³Û’ Ø¬Ø§Ù†Ø§ Ø¬Ø§ØªØ§ ÛÛ’)ØŒ Ø¬Ùˆ Ø§Ú¯Ø³Øª 2017 Ù…ÛŒÚº Ø´Ø±ÙˆØ¹ ÛÙˆØ§ ØªÚ¾Ø§ØŒ Ø§ÙˆØ± Ø§Ø³ ÙˆÙ‚Øª ØªÚ© Ú†Ù„ Ø±ÛØ§ ÛÛ’.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Ø³ÛŒØ±ÛŒ Ø¨ÛŒ 2017 - 2017 (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§Ø¡ Ù¾Ø± Ø³ÛŒØ±ÛŒ Ø¨ÛŒ Ú©ÙˆÙ†Ù¹Ù¹) Ù„ÛŒÚ¯ Ú©Ø§ 86 ÙˆØ§Úº Ø³ÛŒØ²Ù† ÛÛ’ ØŒ Ø¬Ø³ Ù…ÛŒÚº Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø·ÙˆØ± Ù¾Ø± 22 Ù¹ÛŒÙ…ÛŒÚº Ù…Ù‚Ø§Ø¨Ù„Û Ú©Ø± Ø±ÛÛŒ ÛÛŒÚº: 15 2016 - 17 Ø³ÛŒØ²Ù† Ø³Û’ ÙˆØ§Ù¾Ø³ Ø¢Ø±ÛÛŒ ÛÛŒÚº ØŒ 2016 - 17 Ø³ÛŒØ±ÛŒ Ø¨ÛŒ ØŒ Ø¬Ùˆ Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†Ø§Ø¡ Ù¾Ø± Ø³ÛŒØ±ÛŒ Ø¨ÛŒ Ú©ÙˆÙ†Ù¹Ù¹Ù¹ Ú©Û’ Ù†Ø§Ù… Ø³Û’ Ø¬Ø§Ù†Ø§ Ø¬Ø§ØªØ§ ÛÛ’ ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ù‚ÛŒØ§Ù… Ú©Û’ Ø¨Ø¹Ø¯ Ø³Û’ 85 ÙˆØ§Úº Ø³ÛŒØ²Ù† ØªÚ¾Ø§Û”\"\n",
    "ans=rag_pipeline(question)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e273272",
   "metadata": {},
   "source": [
    "RAG Pipeline testing on Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b2c50c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  level                                translated_question translated_answer  \\\n",
      "0  easy  Ø³ÛŒØ±ÛŒ Ø¨ÛŒ 2017 - 2017 (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†...         1929Ø¡ Ù…ÛŒÚº   \n",
      "1  easy  \"Ø¢Ú©Ø³ÙÙˆØ±Úˆ Ú©Ø§Ù„Ø¬ Ú©Û’ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ÛŒ Ø§ÛŒÙ„Ú© Ù†ÛŒÙ„Ø± ÚˆÚ©Ù† Ù†Û’ Ú©Û...       Ú©Ø±Ù¾Ù¹ÙˆÙˆÙ„ÙˆØ¬Ø³Ù¹   \n",
      "2  easy  \"Ø¬Ø±Ø§Ø³Ú© Ù¾Ø§Ø±Ú© Ú©Û’ Ø§Ø¯Ø§Ú©Ø§Ø± ÚˆÛŒÙˆÚˆ ÛÙ†Ø±ÛŒ ÛÙˆØ§Ù†Ú¯ Ù†Û’ \"\"Ø¯ÛŒ ...        Ø¨ÛŒ ÚˆÛŒ ÙˆØ§Ù†Ú¯   \n",
      "3  easy  Ú©ÙˆÙ† Ø³Ø§ Ú©Ø±Ø¯Ø§Ø± ØŒ ÚˆÛŒÙ† Ú©Ø§Ø³Ù¹ÛŒÙ„ÛŒÙ†ÛŒÙ¹Ø§ Ú©ÛŒ Ø¢ÙˆØ§Ø² ØŒ Ø³Ù…Ù¾Ø³Ù†...        Ø¯Ø§Ø¯Ø§ Ø³Ù…Ù¾Ø³Ù†   \n",
      "4  easy     Ú©ÙˆÙ† ØªÚ¾Ø§ Ø§ÛŒÚ© Ø­ØµÛ S#arpØŒ Ù„ÛŒ Ji-hye ÛŒØ§ Ú©Ø±Ù¹Ø³ Ø±Ø§Ø¦Ù¹ØŸ          Ù„ÛŒ Ø¬ÛŒ ÛÛ’   \n",
      "\n",
      "                                  translated_context  \n",
      "0  Ù„ÛŒÚ¯ Ù…ÛŒÚº Ù…Ø¬Ù…ÙˆØ¹ÛŒ Ø·ÙˆØ± Ù¾Ø± 22 Ù¹ÛŒÙ…ÛŒÚº Ø­ØµÛ Ù„Û’ Ø±ÛÛŒ ÛÛŒÚº:...  \n",
      "1  Ø§ÛŒÙ„Ú©Ù„Ø± ÚˆÚ©Ù† (Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ: Alec Dakin) (Û³ Ø§Ù¾Ø±ÛŒÙ„ Û±Û¹Û±Û²...  \n",
      "2  \"Ø¬ÛŒØ±Ø² Ø²Ú©Ø³ Ù†Û’ Ø§Ø³ ÙÙ„Ù… Ú©ÛŒ ÛØ¯Ø§ÛŒØª Ú©Ø§Ø±ÛŒ Ú©ÛŒ ØªÚ¾ÛŒ Ø¬Ø³ Ù…ÛŒ...  \n",
      "3  \"Ø³ÛŒÙ…Ù¾Ø³Ù†Ø² Ú©Û’ 22 ÙˆÛŒÚº Ø³ÛŒØ²Ù† Ú©ÛŒ Ø¯ÙˆØ³Ø±ÛŒ Ù‚Ø³Ø· \"\" Ù„ÙˆÙ† Ø§Û’...  \n",
      "4  Ù„ÛŒ Ø¬ÛŒ ÛÛŒÙˆ (Ù¾ÛŒØ¯Ø§Ø¦Ø´ 11 Ø¬Ù†ÙˆØ±ÛŒ ØŒ 1980) Ø§ÛŒÚ© Ø¬Ù†ÙˆØ¨ÛŒ Ú©...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load only the required columns\n",
    "df = pd.read_csv('C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\Dataset\\\\Hotpotqa\\\\1000_paras_100_queries\\\\translated_dataset_100_qna.csv', usecols=[\n",
    "    'level', 'translated_question', 'translated_answer', 'translated_retrieved_sentences'\n",
    "])\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={'translated_retrieved_sentences': 'translated_context'}, inplace=True)\n",
    "\n",
    "# Optional: View the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05047910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/98 rows\n",
      "Processed 2/98 rows\n",
      "Processed 3/98 rows\n",
      "Processed 4/98 rows\n",
      "Processed 5/98 rows\n",
      "Processed 6/98 rows\n",
      "Processed 7/98 rows\n",
      "Processed 8/98 rows\n",
      "Processed 9/98 rows\n",
      "Processed 10/98 rows\n",
      "Processed 11/98 rows\n",
      "Processed 12/98 rows\n",
      "Processed 13/98 rows\n",
      "Processed 14/98 rows\n",
      "Processed 15/98 rows\n",
      "Processed 16/98 rows\n",
      "Processed 17/98 rows\n",
      "Processed 18/98 rows\n",
      "Processed 19/98 rows\n",
      "Processed 20/98 rows\n",
      "Processed 21/98 rows\n",
      "Processed 22/98 rows\n",
      "Processed 23/98 rows\n",
      "Processed 24/98 rows\n",
      "Processed 25/98 rows\n",
      "Processed 26/98 rows\n",
      "Processed 27/98 rows\n",
      "Processed 28/98 rows\n",
      "Processed 29/98 rows\n",
      "Processed 30/98 rows\n",
      "Processed 31/98 rows\n",
      "Processed 32/98 rows\n",
      "Processed 33/98 rows\n",
      "Processed 34/98 rows\n",
      "Processed 35/98 rows\n",
      "Processed 36/98 rows\n",
      "Processed 37/98 rows\n",
      "Processed 38/98 rows\n",
      "Processed 39/98 rows\n",
      "Processed 40/98 rows\n",
      "Processed 41/98 rows\n",
      "Processed 42/98 rows\n",
      "Processed 43/98 rows\n",
      "Processed 44/98 rows\n",
      "Processed 45/98 rows\n",
      "Processed 46/98 rows\n",
      "Processed 47/98 rows\n",
      "Processed 48/98 rows\n",
      "Processed 49/98 rows\n",
      "Processed 50/98 rows\n",
      "Processed 51/98 rows\n",
      "Processed 52/98 rows\n",
      "Processed 53/98 rows\n",
      "Processed 54/98 rows\n",
      "Processed 55/98 rows\n",
      "Processed 56/98 rows\n",
      "Processed 57/98 rows\n",
      "Processed 58/98 rows\n",
      "Processed 59/98 rows\n",
      "Processed 60/98 rows\n",
      "Processed 61/98 rows\n",
      "Processed 62/98 rows\n",
      "Processed 63/98 rows\n",
      "Processed 64/98 rows\n",
      "Processed 65/98 rows\n",
      "Processed 66/98 rows\n",
      "Processed 67/98 rows\n",
      "Processed 68/98 rows\n",
      "Processed 69/98 rows\n",
      "Processed 70/98 rows\n",
      "Processed 71/98 rows\n",
      "Processed 72/98 rows\n",
      "Processed 73/98 rows\n",
      "Processed 74/98 rows\n",
      "Processed 75/98 rows\n",
      "Processed 76/98 rows\n",
      "Processed 77/98 rows\n",
      "Processed 78/98 rows\n",
      "Processed 79/98 rows\n",
      "Processed 80/98 rows\n",
      "Processed 81/98 rows\n",
      "Processed 82/98 rows\n",
      "Processed 83/98 rows\n",
      "Processed 84/98 rows\n",
      "Processed 85/98 rows\n",
      "Processed 86/98 rows\n",
      "Processed 87/98 rows\n",
      "Processed 88/98 rows\n",
      "Processed 89/98 rows\n",
      "Processed 90/98 rows\n",
      "Processed 91/98 rows\n",
      "Processed 92/98 rows\n",
      "Processed 93/98 rows\n",
      "Processed 94/98 rows\n",
      "Processed 95/98 rows\n",
      "Processed 96/98 rows\n",
      "Processed 97/98 rows\n",
      "Processed 98/98 rows\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Add empty columns first (optional, for visibility)\n",
    "df['retrieved_context'] = \"\"\n",
    "df['final_answer'] = \"\"\n",
    "df['retriever_time'] = 0.0\n",
    "df['generator_time'] = 0.0\n",
    "df['total_time'] = 0.0\n",
    "\n",
    "# Start processing rows\n",
    "for i, row in df.iterrows():\n",
    "    query = row['translated_question']\n",
    "    \n",
    "    # Track total processing time\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # â±ï¸ Retrieve\n",
    "    retriever_start = time.time()\n",
    "    retrieved_chunks = retrieve_documents(query, model, index, chunks_list, k=3)\n",
    "    retriever_end = time.time()\n",
    "    \n",
    "    retrieved_context = \"\\n\".join(retrieved_chunks)  # Combine if it's a list\n",
    "    df.at[i, 'retrieved_context'] = retrieved_context\n",
    "    df.at[i, 'retriever_time'] = retriever_end - retriever_start\n",
    "\n",
    "    # â±ï¸ Generate\n",
    "    generator_start = time.time()\n",
    "    final_answer = generate_using_alif(query, retrieved_context)\n",
    "    generator_end = time.time()\n",
    "     \n",
    "    df.at[i, 'final_answer'] = final_answer\n",
    "    df.at[i, 'generator_time'] = generator_end - generator_start\n",
    "\n",
    "    # â±ï¸ Total Time\n",
    "    df.at[i, 'total_time'] = time.time() - total_start\n",
    "\n",
    "    # Print progress (how many processed so far)\n",
    "    print(f\"Processed {i + 1}/{len(df)} rows\")\n",
    "\n",
    "# Optionally save the results to a new CSV\n",
    "df.to_csv('C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\results\\\\Simple RAG QNA results\\\\simple_rag_qna_results.csv', index=False,encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b6ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
