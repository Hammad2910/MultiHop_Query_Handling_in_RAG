{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0731d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull(\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02440cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Function ---\n",
    "def classify_urdu_question(urdu_question):\n",
    "    prompt = f\"\"\"You are given a question in Urdu. Your task is to classify it as either SINGLEHOP or MULTIHOP.\n",
    "\n",
    "Definitions:\n",
    "- A question is **SINGLEHOP** if it can be answered using a single fact, sentence, or document. The question may be long, but if it doesn't require combining or reasoning over multiple distinct pieces of information, it's SINGLEHOP.\n",
    "- A question is **MULTIHOP** if answering it requires combining multiple facts, reasoning over several steps, or connecting pieces of information from different sources.\n",
    "\n",
    "⚠️ Important:\n",
    "- A question's length does not determine its type. A long, descriptive question can still be SINGLEHOP.\n",
    "- MULTIHOP questions typically require you to first find one piece of information, then use that to find the next.\n",
    "\n",
    "Examples:\n",
    "\n",
    "🔹 Example 1 (SINGLEHOP - short and factual):\n",
    "سوال: \"پاکستان کے وزیر اعظم کا نام کیا ہے؟\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔹 Example 2 (SINGLEHOP - long, descriptive):\n",
    "سوال: \"میں بلیک ہیٹ پہنتا ہوں: بدمعاشوں کے ساتھ جدوجہد (حقیقی اور خیالی) ایک کتاب ہے جو چاک کلوسٹر مین نے لکھی ہے ، جو پہلی بار اسکربر نے شائع کی تھی۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔹 Example 3 (SINGLEHOP - event description):\n",
    "سوال: \"یہ فلم سنہ 2010 میں ریلیز ہوئی تھی اور اس میں 16 قتلوں کی کہانی بیان کی گئی ہے جو سنہ 1828 میں سکاٹ لینڈ کے شہر ایڈنبرا میں ہوئے تھے۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔹 Example 4 (SINGLEHOP - company and product):\n",
    "سوال: \"والگریریا فرانزی ایک اطالوی چمڑے کے بیگ اور سامان کی کمپنی تھی جس کی بنیاد 1864 میں روکو فرانزی نے رکھی تھی۔ گچی نے اس کمپنی کے لئے کام کیا جب تک کہ اس نے گچی کو قائم نہیں کیا۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔸 Example 5 (MULTIHOP - indirect question):\n",
    "سوال: \"جارج واشنگٹن کی پیدائش کے وقت امریکہ کا صدر کون تھا؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 6 (MULTIHOP - location relation):\n",
    "سوال: \"نلسن منڈیلا کو کس جیل میں رکھا گیا اور وہ جیل کس جزیرے پر تھی؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 7 (MULTIHOP - historical causality):\n",
    "سوال: \"انگریزوں کے خلاف 1857 کی جنگ کے بعد ہندوستان میں کونسا سیاسی نظام قائم ہوا؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 8 (MULTIHOP - lengthy, layered facts):\n",
    "سوال: \"ایپل کمپنی کے شریک بانی اسٹیو جابز نے کس یونیورسٹی سے تعلیم حاصل کی، اور اس یونیورسٹی کے سب سے معروف گریجویٹس میں سے ایک کون ہے؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 9 (MULTIHOP - lengthy with connections):\n",
    "سوال: \"شیکسپیئر کے وہ ڈرامے جو بادشاہوں پر مبنی ہیں، ان میں سے ایک کے مرکزی کردار کی موت کس جنگ میں ہوئی تھی؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "---\n",
    "\n",
    "Now read the following Urdu question and classify it:\n",
    "\n",
    "سوال: \"{urdu_question}\"\n",
    "جواب:\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response['message']['content'].strip().lower()\n",
    "\n",
    "        # Normalize output\n",
    "        if 'multihop' in reply:\n",
    "            return 'multihop'\n",
    "        elif 'singlehop' in reply or 'simple' in reply:\n",
    "            return 'singlehop'\n",
    "\n",
    "        print(f\"⚠️ Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing question: {urdu_question}\\n↪ {e}\")\n",
    "        return \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba6c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_urdu_query(urdu_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "**Role**: You are an expert Urdu linguistic analyst specializing in question decomposition. Your task is to break down complex Urdu questions into their fundamental components.\n",
    "\n",
    "**Task Instructions**:\n",
    "1. Carefully analyze the given Urdu question to identify its core components\n",
    "2. Extract exactly 2 sub-questions that:\n",
    "   - Are necessary to answer the main question\n",
    "   - Cover distinct aspects of the problem\n",
    "   - Have clear logical progression (answer to q1 helps answer q2)\n",
    "3. Both sub-questions must:\n",
    "   - Be in proper Urdu language\n",
    "   - Be grammatically correct\n",
    "   - Be clear and concise\n",
    "   - Use relevant domain terminology\n",
    "\n",
    "**Output Format Requirements**:\n",
    "- Use EXACTLY this format:\n",
    "  q1: [پہلا ذیلی سوال]\n",
    "  q2: [دوسرا ذیلی سوال]\n",
    "- Each sub-question must be on a new line\n",
    "- Do not include any additional commentary or explanation\n",
    "- Do not number the questions (use only q1:/q2: prefixes)\n",
    "\n",
    "**Example 1**:\n",
    "Input: اگر لاہور میں فضائی آلودگی کی سطح دہلی سے زیادہ ہے اور فضائی آلودگی پھیپھڑوں کے کینسر کا سبب بن سکتی ہے، تو لاہور کے رہائشیوں کو کس قسم کے طبی چیک اپ کروانے چاہئیں؟\n",
    "Output:\n",
    "q1: لاہور اور دہلی میں فضائی آلودگی کی سطح کا موازنہ کیا ہے؟\n",
    "q2: فضائی آلودگی پھیپھڑوں کے کینسر کا سبب کیسے بنتی ہے؟\n",
    "\n",
    "**Example 2**:\n",
    "Input: اگر کراچی میں بجلی کے نرخ 30% بڑھ گئے ہیں اور یہ صنعتوں کو متاثر کر رہا ہے، تو حکومت کو کون سی سبسڈیاں دینی چاہئیں؟\n",
    "Output:\n",
    "q1: کراچی میں بجلی کے نرخوں میں اضافے کی موجودہ شرح کیا ہے؟\n",
    "q2: بجلی کے مہنگے ہونے سے صنعتوں پر کس قسم کے اثرات مرتب ہو رہے ہیں؟\n",
    "\n",
    "**Current Task**:\n",
    "Input: {urdu_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = response['response'].strip()\n",
    "        \n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "        \n",
    "        return result if len(result) == 2 else {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd41305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_context_relevance_check(query_urdu: str, context_urdu: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "You are a binary classifier.\n",
    "\n",
    "Your task is to decide whether the following Urdu *context* is relevant to the Urdu *question*. You must answer ONLY with **True** or **False** — no explanation, no commentary, just one word: True or False.\n",
    "\n",
    "Criteria:\n",
    "- If the context helps answer the question directly or indirectly, reply: True\n",
    "- If the context is unrelated, confusing, or insufficient, reply: False\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT explain your answer.\n",
    "- Do NOT include any additional comments.\n",
    "- Just respond with: True or False\n",
    "\n",
    "---\n",
    "\n",
    "Question (Urdu): {query_urdu}\n",
    "\n",
    "Context (Urdu): {context_urdu}\n",
    "\n",
    "Answer (True/False):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        return answer == 'true'\n",
    "    except Exception as e:\n",
    "        print(f\"Error during relevance check: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca9445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer model (E5-large)\n",
    "    model = SentenceTransformer(\"intfloat/e5-large\", device=device)\n",
    "    \n",
    "    # Configure for Urdu text\n",
    "    model.max_seq_length = 512  # Set based on your earlier analysis\n",
    "    model.tokenizer.do_lower_case = False  # Preserve Urdu characters\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972cbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_faiss_index.index\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_chunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614a4748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66bae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query,k=3):\n",
    "   \n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,  # Return numpy array for FAISS\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Search FAISS index\n",
    "    _, indices = index.search(query_embedding, k)  # Dummy variable _ for distances\n",
    "    \n",
    "    # Return only the chunks\n",
    "    return [chunks_list[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852230d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def expand_multihop_context(intermediate_ctx, query, q1, q2, k):\n",
    "    try:\n",
    "        combined_query = q1 + intermediate_ctx + q2\n",
    "        second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "\n",
    "        relevant_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as inner_executor:\n",
    "            futures = [\n",
    "                inner_executor.submit(query_context_relevance_check, query, ctx)\n",
    "                for ctx in second_hop_contexts\n",
    "            ]\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        relevant_contexts.append(second_hop_contexts[i])\n",
    "                except Exception as e:\n",
    "                    print(\"Error during relevance check:\", e)\n",
    "\n",
    "        return relevant_contexts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in expand_multihop_context:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_context_of_multihop(query, type, k=3):\n",
    "    # Step 1: Classification\n",
    "    start_classification = time.time()\n",
    "    classification = classify_urdu_question(query)\n",
    "    classification_time = time.time() - start_classification\n",
    "\n",
    "    if type == \"easy\":\n",
    "        decomposition_time = 0.0\n",
    "        start_retrieval = time.time()\n",
    "        context = retrieve_documents(query, k)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "        return context, classification, classification_time, decomposition_time, retrieval_time\n",
    "\n",
    "    else:\n",
    "        # Step 2: Decomposition\n",
    "        start_decomposition = time.time()\n",
    "        decomposition = decompose_urdu_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "        decomposition_time = time.time() - start_decomposition\n",
    "\n",
    "        # Step 3: First-hop retrieval for q1 and q2\n",
    "        start_retrieval = time.time()\n",
    "        context_q1 = retrieve_documents(q1, k)\n",
    "        context_q2 = retrieve_documents(q2, k)\n",
    "\n",
    "        # Combine base context\n",
    "        main_context = context_q1 + context_q2\n",
    "\n",
    "        additional_contexts = []\n",
    "\n",
    "        # Expand q1\n",
    "        with ThreadPoolExecutor() as executor_q1:\n",
    "            futures_q1 = [\n",
    "                executor_q1.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in context_q1[:k]\n",
    "            ]\n",
    "            for future in as_completed(futures_q1):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        # Expand q2\n",
    "        with ThreadPoolExecutor() as executor_q2:\n",
    "            futures_q2 = [\n",
    "                executor_q2.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in context_q2[:k]\n",
    "            ]\n",
    "            for future in as_completed(futures_q2):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        # Merge all retrieved content\n",
    "        main_context.extend(additional_contexts)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "\n",
    "        return main_context, classification, classification_time, decomposition_time, retrieval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b7e1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull('hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0933e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_llama3(context, query):\n",
    "    prompt = f\"\"\"You are a helpful assistant. You will be given a context and a question, both in Urdu.\n",
    "Your task is to answer the question using the context only. \n",
    "Your answer should be **clear, concise, and entirely in Urdu**.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer in Urdu:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        return \"جواب پیدا کرنے میں خرابی ہوئی۔\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dddc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multihop_handling_modLQR(query, type, k=3):\n",
    "    # Step 1: Get context and timings\n",
    "    context, classification, classification_time, decomposition_time, retrieval_time = get_context_of_multihop(\n",
    "        query, type, k=k\n",
    "    )\n",
    "\n",
    "    # Flatten context if it's a list of strings\n",
    "    if isinstance(context, list):\n",
    "        combined_context = \"\\n\".join(context)\n",
    "    else:\n",
    "        combined_context = context\n",
    "\n",
    "    # Step 2: Generate answer and measure time\n",
    "    start_gen = time.time()\n",
    "    final_answer = generate_using_llama3(combined_context,query)\n",
    "    generation_time = time.time() - start_gen\n",
    "\n",
    "    # Step 3: Compute total time\n",
    "    total_time = classification_time + decomposition_time + retrieval_time + generation_time\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"timings\": {\n",
    "            \"classification_time\": classification_time,\n",
    "            \"decomposition_time\": decomposition_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfa00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/598 [00:15<2:31:30, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 0: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 1/598 in 15.23 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/598 [00:17<1:17:31,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 1: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 2/598 in 2.61 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/598 [00:19<51:20,  5.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 2: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 3/598 in 2.05 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/598 [00:33<1:24:02,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 3: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 4/598 in 13.56 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/598 [00:49<1:50:21, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 4: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 5/598 in 15.91 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/598 [01:03<1:59:56, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 5: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 6/598 in 14.08 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/598 [01:08<1:36:47,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 6: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 7/598 in 5.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 8/598 [01:30<2:15:43, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 7: retrieve_documents() takes from 1 to 2 positional arguments but 5 were given\n",
      "Processed record 8/598 in 22.32 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 8/598 [01:54<2:20:27, 14.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmultihop_handling_modLQR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     classification \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m     retrieved_context \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved_context\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mmultihop_handling_modLQR\u001b[1;34m(query, type, model, index, chunks_list, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmultihop_handling_modLQR\u001b[39m(query, \u001b[38;5;28mtype\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, index\u001b[38;5;241m=\u001b[39mindex, chunks_list\u001b[38;5;241m=\u001b[39mchunks_list, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Step 1: Get context and timings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     context, classification, classification_time, decomposition_time, retrieval_time \u001b[38;5;241m=\u001b[39m \u001b[43mget_context_of_multihop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Flatten context if it's a list of strings\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(context, \u001b[38;5;28mlist\u001b[39m):\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mget_context_of_multihop\u001b[1;34m(query, type, model, index, chunks_list, k)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_context_of_multihop\u001b[39m(query, \u001b[38;5;28mtype\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, index\u001b[38;5;241m=\u001b[39mindex, chunks_list\u001b[38;5;241m=\u001b[39mchunks_list, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Step 1: Classification\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     start_classification \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 34\u001b[0m     classification \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_urdu_question_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     classification_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_classification\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124measy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m, in \u001b[0;36mclassify_urdu_question_gemma\u001b[1;34m(urdu_question)\u001b[0m\n\u001b[0;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are given a question in Urdu. Classify it as SINGLEHOP or MULTIHOP.\u001b[39m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mDefinitions:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124mسوال: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murdu_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124mجواب:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgemma3:4b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         reply \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultihop\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m reply:\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\ollama\\_client.py:118\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    117\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32md:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/modLQR_processed_results.csv\"\n",
    "\n",
    "# Initialize storage\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop over each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        result = multihop_handling_modLQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Write batch every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(idx), index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save any remaining records\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(len(df) % 100), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Final stats\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / len(df)\n",
    "print(f\"\\n✅ All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 99/598 [02:14<11:18,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 99/598 in 134.66 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 100/598 [04:34<27:38,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed record 100/598 in 139.77 seconds.\n",
      "\n",
      "--- Saved batch up to record 100\n",
      "Average time for last 100 records: 137.21 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/modLQR_processed_results.csv\"\n",
    "\n",
    "# Initialize storage\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop over each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if idx < 98:\n",
    "        continue  # Skip the first 98 records\n",
    "\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        result = multihop_handling_modLQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Write batch every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(idx), index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save any remaining records\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(len(df) % 100), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Final stats\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / (len(df) - 98)  # Adjusted for skipped records\n",
    "print(f\"\\n✅ All applicable records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017cd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc443057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
