{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0731d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull(\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02440cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Function ---\n",
    "def classify_urdu_question_gemma(urdu_question):\n",
    "    prompt = f\"\"\"You are given a question in Urdu. Classify it as SINGLEHOP or MULTIHOP.\n",
    "\n",
    "Definitions:\n",
    "- SINGLEHOP: Can be answered using one fact, sentence, or document.\n",
    "- MULTIHOP: Requires combining multiple facts or reasoning over multiple steps.\n",
    "\n",
    "Examples:\n",
    "\n",
    "سوال: \"پاکستان کے وزیر اعظم کا نام کیا ہے؟\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "سوال: \"جارج واشنگٹن کی پیدائش کے وقت امریکہ کا صدر کون تھا؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "سوال: \"ایپل کمپنی کے شریک بانی نے کہاں تعلیم حاصل کی، اور اس یونیورسٹی کا مشہور فارغ التحصیل کون ہے؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "سوال: \"یہ فلم 2010 میں ریلیز ہوئی اور سکاٹ لینڈ میں 1828 کے قتلوں پر مبنی ہے۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "---\n",
    "\n",
    "Now classify the following:\n",
    "\n",
    "سوال: \"{urdu_question}\"\n",
    "جواب:\"\"\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='gemma3:4b',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response['message']['content'].strip().lower()\n",
    "\n",
    "        if 'multihop' in reply:\n",
    "            return 'multihop'\n",
    "        elif 'singlehop' in reply or 'simple' in reply:\n",
    "            return 'singlehop'\n",
    "\n",
    "        print(f\"⚠️ Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing question: {urdu_question}\\n↪ {e}\")\n",
    "        return \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba6c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_urdu_query(urdu_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "**Role**: You are an expert Urdu linguistic analyst specializing in question decomposition. Your task is to break down complex Urdu questions into their fundamental components.\n",
    "\n",
    "**Task Instructions**:\n",
    "1. Carefully analyze the given Urdu question to identify its core components\n",
    "2. Extract exactly 2 sub-questions that:\n",
    "   - Are necessary to answer the main question\n",
    "   - Cover distinct aspects of the problem\n",
    "   - Have clear logical progression (answer to q1 helps answer q2)\n",
    "3. Both sub-questions must:\n",
    "   - Be in proper Urdu language\n",
    "   - Be grammatically correct\n",
    "   - Be clear and concise\n",
    "   - Use relevant domain terminology\n",
    "\n",
    "**Output Format Requirements**:\n",
    "- Use EXACTLY this format:\n",
    "  q1: [پہلا ذیلی سوال]\n",
    "  q2: [دوسرا ذیلی سوال]\n",
    "- Each sub-question must be on a new line\n",
    "- Do not include any additional commentary or explanation\n",
    "- Do not number the questions (use only q1:/q2: prefixes)\n",
    "\n",
    "**Example 1**:\n",
    "Input: اگر لاہور میں فضائی آلودگی کی سطح دہلی سے زیادہ ہے اور فضائی آلودگی پھیپھڑوں کے کینسر کا سبب بن سکتی ہے، تو لاہور کے رہائشیوں کو کس قسم کے طبی چیک اپ کروانے چاہئیں؟\n",
    "Output:\n",
    "q1: لاہور اور دہلی میں فضائی آلودگی کی سطح کا موازنہ کیا ہے؟\n",
    "q2: فضائی آلودگی پھیپھڑوں کے کینسر کا سبب کیسے بنتی ہے؟\n",
    "\n",
    "**Example 2**:\n",
    "Input: اگر کراچی میں بجلی کے نرخ 30% بڑھ گئے ہیں اور یہ صنعتوں کو متاثر کر رہا ہے، تو حکومت کو کون سی سبسڈیاں دینی چاہئیں؟\n",
    "Output:\n",
    "q1: کراچی میں بجلی کے نرخوں میں اضافے کی موجودہ شرح کیا ہے؟\n",
    "q2: بجلی کے مہنگے ہونے سے صنعتوں پر کس قسم کے اثرات مرتب ہو رہے ہیں؟\n",
    "\n",
    "**Current Task**:\n",
    "Input: {urdu_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = response['response'].strip()\n",
    "        \n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "        \n",
    "        return result if len(result) == 2 else {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd41305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_context_relevance_check(query_urdu: str, context_urdu: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "You are a binary classifier.\n",
    "\n",
    "Your task is to decide whether the following Urdu *context* is relevant to the Urdu *question*. You must answer ONLY with **True** or **False** — no explanation, no commentary, just one word: True or False.\n",
    "\n",
    "Criteria:\n",
    "- If the context helps answer the question directly or indirectly, reply: True\n",
    "- If the context is unrelated, confusing, or insufficient, reply: False\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT explain your answer.\n",
    "- Do NOT include any additional comments.\n",
    "- Just respond with: True or False\n",
    "\n",
    "---\n",
    "\n",
    "Question (Urdu): {query_urdu}\n",
    "\n",
    "Context (Urdu): {context_urdu}\n",
    "\n",
    "Answer (True/False):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        return answer == 'true'\n",
    "    except Exception as e:\n",
    "        print(f\"Error during relevance check: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop for Urdu\\Multihop for Urdu\\code\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer model (E5-large)\n",
    "    model = SentenceTransformer(\"intfloat/e5-large\", device=device)\n",
    "    \n",
    "    # Configure for Urdu text\n",
    "    model.max_seq_length = 512  # Set based on your earlier analysis\n",
    "    model.tokenizer.do_lower_case = False  # Preserve Urdu characters\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_faiss_index.index\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_chunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query,k=3):\n",
    "   \n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,  # Return numpy array for FAISS\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Search FAISS index\n",
    "    _, indices = index.search(query_embedding, k)  # Dummy variable _ for distances\n",
    "    \n",
    "    # Return only the chunks\n",
    "    return [chunks_list[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852230d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def expand_multihop_context(intermediate_ctx, query, q1, q2, k):\n",
    "    try:\n",
    "        combined_query = q1 + intermediate_ctx + q2\n",
    "        second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "\n",
    "        relevant_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as inner_executor:\n",
    "            futures = [\n",
    "                inner_executor.submit(query_context_relevance_check, query, ctx)\n",
    "                for ctx in second_hop_contexts\n",
    "            ]\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        relevant_contexts.append(second_hop_contexts[i])\n",
    "                except Exception as e:\n",
    "                    print(\"Error during relevance check:\", e)\n",
    "\n",
    "        return relevant_contexts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in expand_multihop_context:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_context_of_multihop(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Step 1: Classification\n",
    "    start_classification = time.time()\n",
    "    classification = classify_urdu_question_gemma(query)\n",
    "    classification_time = time.time() - start_classification\n",
    "\n",
    "    if type == \"easy\":\n",
    "        decomposition_time = 0.0\n",
    "        start_retrieval = time.time()\n",
    "        context = retrieve_documents(query, model, index, chunks_list, k)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "        return context, classification, classification_time, decomposition_time, retrieval_time\n",
    "\n",
    "    else:\n",
    "        # Step 2: Decomposition\n",
    "        start_decomposition = time.time()\n",
    "        decomposition = decompose_urdu_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "        decomposition_time = time.time() - start_decomposition\n",
    "\n",
    "        # Step 3: First-hop retrieval for q1 and q2\n",
    "        start_retrieval = time.time()\n",
    "        context_q1 = retrieve_documents(q1, k)\n",
    "        context_q2 = retrieve_documents(q2, k)\n",
    "\n",
    "        # Combine base context\n",
    "        main_context = context_q1 + context_q2\n",
    "\n",
    "        additional_contexts = []\n",
    "\n",
    "        # Expand q1\n",
    "        with ThreadPoolExecutor() as executor_q1:\n",
    "            futures_q1 = [\n",
    "                executor_q1.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in context_q1[:k]\n",
    "            ]\n",
    "            for future in as_completed(futures_q1):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        # Expand q2\n",
    "        with ThreadPoolExecutor() as executor_q2:\n",
    "            futures_q2 = [\n",
    "                executor_q2.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in context_q2[:k]\n",
    "            ]\n",
    "            for future in as_completed(futures_q2):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        # Merge all retrieved content\n",
    "        main_context.extend(additional_contexts)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "\n",
    "        return main_context, classification, classification_time, decomposition_time, retrieval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b7e1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull('hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_llama3(context, query):\n",
    "    prompt = f\"\"\"You are a helpful assistant. You will be given a context and a question, both in Urdu.\n",
    "Your task is to answer the question using the context only. \n",
    "Your answer should be **clear, concise, and entirely in Urdu**.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer in Urdu:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        return \"جواب پیدا کرنے میں خرابی ہوئی۔\"\n",
    "\n",
    "import ollama\n",
    "\n",
    "def generate_using_alif(context, query, alif_model='hf.co/large-traversaal/Alif-1.0-8B-Instruct:f16'):\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"آپ کو ایک سوال اور اس سے متعلق ایک سیاق و سباق دیا گیا ہے۔ براہ کرم سیاق و سباق کا بغور مطالعہ کریں اور اسی کی بنیاد پر درست، مختصر اور جامع جواب دیں۔\n",
    "\n",
    "### سوال:\n",
    "{query}\n",
    "\n",
    "### سیاق و سباق:\n",
    "{context}\n",
    "\n",
    "### جواب:\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=alif_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prompt_llama_gemma_balanced_english(query, context):\n",
    "    return f\"\"\"\n",
    "You are an expert assistant. Please answer the following question in fluent and clear Urdu. If possible, avoid using English words. Do not use bullet points or lists. Write your response in concise paragraphs.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_using_gemma(query, context):\n",
    "    model_name = \"gemma3:4b\"\n",
    "    prompt = prompt_llama_gemma_balanced_english(query, context)\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        output = response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        output = f\"Error: {e}\"\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dddc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multihop_handling_modLQR(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Step 1: Get context and timings\n",
    "    context, classification, classification_time, decomposition_time, retrieval_time = get_context_of_multihop(\n",
    "        query, type, model=model, index=index, chunks_list=chunks_list, k=k\n",
    "    )\n",
    "\n",
    "    # Flatten context if it's a list of strings\n",
    "    if isinstance(context, list):\n",
    "        combined_context = \"\\n\".join(context)\n",
    "    else:\n",
    "        combined_context = context\n",
    "\n",
    "    # Step 2: Generate answer and measure time\n",
    "    start_gen = time.time()\n",
    "    final_answer = generate_using_gemma(combined_context,query)\n",
    "    generation_time = time.time() - start_gen\n",
    "\n",
    "    # Step 3: Compute total time\n",
    "    total_time = classification_time + decomposition_time + retrieval_time + generation_time\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"timings\": {\n",
    "            \"classification_time\": classification_time,\n",
    "            \"decomposition_time\": decomposition_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfa00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98/98 [1:41:15<00:00, 61.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 translated_question translated_answer  \\\n",
      "0  سیری بی 2017 - 2017 (اسپانسرشپ کی وجوہات کی بن...         1929ء میں   \n",
      "1  \"آکسفورڈ کالج کے ایک ساتھی ایلک نیلر ڈکن نے کہ...       کرپٹوولوجسٹ   \n",
      "2  \"جراسک پارک کے اداکار ڈیوڈ ہنری ہوانگ نے \"\"دی ...        بی ڈی وانگ   \n",
      "3  کون سا کردار ، ڈین کاسٹیلینیٹا کی آواز ، سمپسن...        دادا سمپسن   \n",
      "4     کون تھا ایک حصہ S#arp، لی Ji-hye یا کرٹس رائٹ؟          لی جی ہے   \n",
      "\n",
      "  classification                                  retrieved_context  \\\n",
      "0       multihop  [سیری بی (اسپانسرشپ وجوہات کی بناء پر سیری بی ...   \n",
      "1       multihop  [۔ ریچر اس وقت اسٹینفورڈ یونیورسٹی میں اسٹینفو...   \n",
      "2      singlehop  [۔ یہ فلم والٹر ونجر نے پروڈیوس کی ہے۔ اس فلم ...   \n",
      "3       multihop  [۔ اس کے کریڈٹ میں \"\"نائٹ آف دی لونگ ڈیڈ\"\" ، \"...   \n",
      "4       multihop  [۔ اس میں ممبر کیم لیپ کا تعارف کیا گیا ہے اور...   \n",
      "\n",
      "                                        final_answer  classification_time  \\\n",
      "0  سیریز بی ایک فٹ بال لیگ ہے جو اٹلی میں مقامی س...             6.936363   \n",
      "1  برطانوی کوڈ بریکرز کی طرف سے کام کرنے والا ایک...             7.014243   \n",
      "2  \"جراسک پارک\" فلم \"دی ڈے\" کے لئے ایک شاندار، مض...             4.835996   \n",
      "3  ڈینیئل کاسٹیلینوٹا سمپسنز کے بیسویں سیزن میں پ...             6.626251   \n",
      "4  اس سیاق و سباق سے ، یہ معلوم ہوتا ہے کہ ملوکیت...             6.008085   \n",
      "\n",
      "   decomposition_time  retrieval_time  generation_time  total_time level  \n",
      "0                 0.0        0.039769        25.695454   32.671586  easy  \n",
      "1                 0.0        0.009520        30.078613   37.102376  easy  \n",
      "2                 0.0        0.005161        48.812821   53.653979  easy  \n",
      "3                 0.0        0.009174        30.746777   37.382202  easy  \n",
      "4                 0.0        0.013025        39.971152   45.992261  easy  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/modLQR_processed_results.csv\"\n",
    "\n",
    "# Initialize storage\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop over each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        result = multihop_handling_modLQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Write batch every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(idx), index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save any remaining records\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(len(df) % 100), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Final stats\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / len(df)\n",
    "print(f\"\\n✅ All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6e591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
