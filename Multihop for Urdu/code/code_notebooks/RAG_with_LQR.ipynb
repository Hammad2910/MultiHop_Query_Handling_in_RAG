{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08690e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb89170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Function ---\n",
    "def classify_urdu_question(urdu_question):\n",
    "    prompt = f\"\"\"You are given a question in Urdu. Your task is to classify it as either SINGLEHOP or MULTIHOP.\n",
    "\n",
    "Definitions:\n",
    "- A question is **SINGLEHOP** if it can be answered using a single fact, sentence, or document. The question may be long, but if it doesn't require combining or reasoning over multiple distinct pieces of information, it's SINGLEHOP.\n",
    "- A question is **MULTIHOP** if answering it requires combining multiple facts, reasoning over several steps, or connecting pieces of information from different sources.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "- A question's length does not determine its type. A long, descriptive question can still be SINGLEHOP.\n",
    "- MULTIHOP questions typically require you to first find one piece of information, then use that to find the next.\n",
    "\n",
    "Examples:\n",
    "\n",
    "üîπ Example 1 (SINGLEHOP - short and factual):\n",
    "ÿ≥ŸàÿßŸÑ: \"Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ ⁄©€í Ÿàÿ≤€åÿ± ÿßÿπÿ∏ŸÖ ⁄©ÿß ŸÜÿßŸÖ ⁄©€åÿß €Å€íÿü\"\n",
    "ÿ¨Ÿàÿßÿ®: SINGLEHOP\n",
    "\n",
    "üîπ Example 2 (SINGLEHOP - long, descriptive):\n",
    "ÿ≥ŸàÿßŸÑ: \"ŸÖ€å⁄∫ ÿ®ŸÑ€å⁄© €Å€åŸπ Ÿæ€ÅŸÜÿ™ÿß €ÅŸà⁄∫: ÿ®ÿØŸÖÿπÿßÿ¥Ÿà⁄∫ ⁄©€í ÿ≥ÿßÿ™⁄æ ÿ¨ÿØŸàÿ¨€ÅÿØ (ÿ≠ŸÇ€åŸÇ€å ÿßŸàÿ± ÿÆ€åÿßŸÑ€å) ÿß€å⁄© ⁄©ÿ™ÿßÿ® €Å€í ÿ¨Ÿà ⁄Üÿß⁄© ⁄©ŸÑŸàÿ≥Ÿπÿ± ŸÖ€åŸÜ ŸÜ€í ŸÑ⁄©⁄æ€å €Å€í ÿå ÿ¨Ÿà Ÿæ€ÅŸÑ€å ÿ®ÿßÿ± ÿßÿ≥⁄©ÿ±ÿ®ÿ± ŸÜ€í ÿ¥ÿßÿ¶ÿπ ⁄©€å ÿ™⁄æ€å€î\"\n",
    "ÿ¨Ÿàÿßÿ®: SINGLEHOP\n",
    "\n",
    "üîπ Example 3 (SINGLEHOP - event description):\n",
    "ÿ≥ŸàÿßŸÑ: \"€å€Å ŸÅŸÑŸÖ ÿ≥ŸÜ€Å 2010 ŸÖ€å⁄∫ ÿ±€åŸÑ€åÿ≤ €ÅŸàÿ¶€å ÿ™⁄æ€å ÿßŸàÿ± ÿßÿ≥ ŸÖ€å⁄∫ 16 ŸÇÿ™ŸÑŸà⁄∫ ⁄©€å ⁄©€ÅÿßŸÜ€å ÿ®€åÿßŸÜ ⁄©€å ⁄Øÿ¶€å €Å€í ÿ¨Ÿà ÿ≥ŸÜ€Å 1828 ŸÖ€å⁄∫ ÿ≥⁄©ÿßŸπ ŸÑ€åŸÜ⁄à ⁄©€í ÿ¥€Åÿ± ÿß€å⁄àŸÜÿ®ÿ±ÿß ŸÖ€å⁄∫ €ÅŸàÿ¶€í ÿ™⁄æ€í€î\"\n",
    "ÿ¨Ÿàÿßÿ®: SINGLEHOP\n",
    "\n",
    "üîπ Example 4 (SINGLEHOP - company and product):\n",
    "ÿ≥ŸàÿßŸÑ: \"ŸàÿßŸÑ⁄Øÿ±€åÿ±€åÿß ŸÅÿ±ÿßŸÜÿ≤€å ÿß€å⁄© ÿßÿ∑ÿßŸÑŸà€å ⁄ÜŸÖ⁄ë€í ⁄©€í ÿ®€å⁄Ø ÿßŸàÿ± ÿ≥ÿßŸÖÿßŸÜ ⁄©€å ⁄©ŸÖŸæŸÜ€å ÿ™⁄æ€å ÿ¨ÿ≥ ⁄©€å ÿ®ŸÜ€åÿßÿØ 1864 ŸÖ€å⁄∫ ÿ±Ÿà⁄©Ÿà ŸÅÿ±ÿßŸÜÿ≤€å ŸÜ€í ÿ±⁄©⁄æ€å ÿ™⁄æ€å€î ⁄Ø⁄Ü€å ŸÜ€í ÿßÿ≥ ⁄©ŸÖŸæŸÜ€å ⁄©€í ŸÑÿ¶€í ⁄©ÿßŸÖ ⁄©€åÿß ÿ¨ÿ® ÿ™⁄© ⁄©€Å ÿßÿ≥ ŸÜ€í ⁄Ø⁄Ü€å ⁄©Ÿà ŸÇÿßÿ¶ŸÖ ŸÜ€Å€å⁄∫ ⁄©€åÿß€î\"\n",
    "ÿ¨Ÿàÿßÿ®: SINGLEHOP\n",
    "\n",
    "üî∏ Example 5 (MULTIHOP - indirect question):\n",
    "ÿ≥ŸàÿßŸÑ: \"ÿ¨ÿßÿ±ÿ¨ Ÿàÿßÿ¥ŸÜ⁄ØŸπŸÜ ⁄©€å Ÿæ€åÿØÿßÿ¶ÿ¥ ⁄©€í ŸàŸÇÿ™ ÿßŸÖÿ±€å⁄©€Å ⁄©ÿß ÿµÿØÿ± ⁄©ŸàŸÜ ÿ™⁄æÿßÿü\"\n",
    "ÿ¨Ÿàÿßÿ®: MULTIHOP\n",
    "\n",
    "üî∏ Example 6 (MULTIHOP - location relation):\n",
    "ÿ≥ŸàÿßŸÑ: \"ŸÜŸÑÿ≥ŸÜ ŸÖŸÜ⁄à€åŸÑÿß ⁄©Ÿà ⁄©ÿ≥ ÿ¨€åŸÑ ŸÖ€å⁄∫ ÿ±⁄©⁄æÿß ⁄Ø€åÿß ÿßŸàÿ± Ÿà€Å ÿ¨€åŸÑ ⁄©ÿ≥ ÿ¨ÿ≤€åÿ±€í Ÿæÿ± ÿ™⁄æ€åÿü\"\n",
    "ÿ¨Ÿàÿßÿ®: MULTIHOP\n",
    "\n",
    "üî∏ Example 7 (MULTIHOP - historical causality):\n",
    "ÿ≥ŸàÿßŸÑ: \"ÿßŸÜ⁄Øÿ±€åÿ≤Ÿà⁄∫ ⁄©€í ÿÆŸÑÿßŸÅ 1857 ⁄©€å ÿ¨ŸÜ⁄Ø ⁄©€í ÿ®ÿπÿØ €ÅŸÜÿØŸàÿ≥ÿ™ÿßŸÜ ŸÖ€å⁄∫ ⁄©ŸàŸÜÿ≥ÿß ÿ≥€åÿßÿ≥€å ŸÜÿ∏ÿßŸÖ ŸÇÿßÿ¶ŸÖ €ÅŸàÿßÿü\"\n",
    "ÿ¨Ÿàÿßÿ®: MULTIHOP\n",
    "\n",
    "üî∏ Example 8 (MULTIHOP - lengthy, layered facts):\n",
    "ÿ≥ŸàÿßŸÑ: \"ÿß€åŸæŸÑ ⁄©ŸÖŸæŸÜ€å ⁄©€í ÿ¥ÿ±€å⁄© ÿ®ÿßŸÜ€å ÿßÿ≥Ÿπ€åŸà ÿ¨ÿßÿ®ÿ≤ ŸÜ€í ⁄©ÿ≥ €åŸàŸÜ€åŸàÿ±ÿ≥Ÿπ€å ÿ≥€í ÿ™ÿπŸÑ€åŸÖ ÿ≠ÿßÿµŸÑ ⁄©€åÿå ÿßŸàÿ± ÿßÿ≥ €åŸàŸÜ€åŸàÿ±ÿ≥Ÿπ€å ⁄©€í ÿ≥ÿ® ÿ≥€í ŸÖÿπÿ±ŸàŸÅ ⁄Øÿ±€åÿ¨Ÿà€åŸπÿ≥ ŸÖ€å⁄∫ ÿ≥€í ÿß€å⁄© ⁄©ŸàŸÜ €Å€íÿü\"\n",
    "ÿ¨Ÿàÿßÿ®: MULTIHOP\n",
    "\n",
    "üî∏ Example 9 (MULTIHOP - lengthy with connections):\n",
    "ÿ≥ŸàÿßŸÑ: \"ÿ¥€å⁄©ÿ≥Ÿæ€åÿ¶ÿ± ⁄©€í Ÿà€Å ⁄àÿ±ÿßŸÖ€í ÿ¨Ÿà ÿ®ÿßÿØÿ¥ÿß€ÅŸà⁄∫ Ÿæÿ± ŸÖÿ®ŸÜ€å €Å€å⁄∫ÿå ÿßŸÜ ŸÖ€å⁄∫ ÿ≥€í ÿß€å⁄© ⁄©€í ŸÖÿ±⁄©ÿ≤€å ⁄©ÿ±ÿØÿßÿ± ⁄©€å ŸÖŸàÿ™ ⁄©ÿ≥ ÿ¨ŸÜ⁄Ø ŸÖ€å⁄∫ €ÅŸàÿ¶€å ÿ™⁄æ€åÿü\"\n",
    "ÿ¨Ÿàÿßÿ®: MULTIHOP\n",
    "\n",
    "---\n",
    "\n",
    "Now read the following Urdu question and classify it:\n",
    "\n",
    "ÿ≥ŸàÿßŸÑ: \"{urdu_question}\"\n",
    "ÿ¨Ÿàÿßÿ®:\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response['message']['content'].strip().lower()\n",
    "\n",
    "        # Normalize output\n",
    "        if 'multihop' in reply:\n",
    "            return 'multihop'\n",
    "        elif 'singlehop' in reply or 'simple' in reply:\n",
    "            return 'singlehop'\n",
    "\n",
    "        print(f\"‚ö†Ô∏è Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing question: {urdu_question}\\n‚Ü™ {e}\")\n",
    "        return \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134f1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_urdu_query(urdu_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "**Role**: You are an expert Urdu linguistic analyst specializing in question decomposition. Your task is to break down complex Urdu questions into their fundamental components.\n",
    "\n",
    "**Task Instructions**:\n",
    "1. Carefully analyze the given Urdu question to identify its core components\n",
    "2. Extract exactly 2 sub-questions that:\n",
    "   - Are necessary to answer the main question\n",
    "   - Cover distinct aspects of the problem\n",
    "   - Have clear logical progression (answer to q1 helps answer q2)\n",
    "3. Both sub-questions must:\n",
    "   - Be in proper Urdu language\n",
    "   - Be grammatically correct\n",
    "   - Be clear and concise\n",
    "   - Use relevant domain terminology\n",
    "\n",
    "**Output Format Requirements**:\n",
    "- Use EXACTLY this format:\n",
    "  q1: [Ÿæ€ÅŸÑÿß ÿ∞€åŸÑ€å ÿ≥ŸàÿßŸÑ]\n",
    "  q2: [ÿØŸàÿ≥ÿ±ÿß ÿ∞€åŸÑ€å ÿ≥ŸàÿßŸÑ]\n",
    "- Each sub-question must be on a new line\n",
    "- Do not include any additional commentary or explanation\n",
    "- Do not number the questions (use only q1:/q2: prefixes)\n",
    "\n",
    "**Example 1**:\n",
    "Input: ÿß⁄Øÿ± ŸÑÿß€ÅŸàÿ± ŸÖ€å⁄∫ ŸÅÿ∂ÿßÿ¶€å ÿ¢ŸÑŸàÿØ⁄Ø€å ⁄©€å ÿ≥ÿ∑ÿ≠ ÿØ€ÅŸÑ€å ÿ≥€í ÿ≤€åÿßÿØ€Å €Å€í ÿßŸàÿ± ŸÅÿ∂ÿßÿ¶€å ÿ¢ŸÑŸàÿØ⁄Ø€å Ÿæ⁄æ€åŸæ⁄æ⁄ëŸà⁄∫ ⁄©€í ⁄©€åŸÜÿ≥ÿ± ⁄©ÿß ÿ≥ÿ®ÿ® ÿ®ŸÜ ÿ≥⁄©ÿ™€å €Å€íÿå ÿ™Ÿà ŸÑÿß€ÅŸàÿ± ⁄©€í ÿ±€Åÿßÿ¶ÿ¥€åŸà⁄∫ ⁄©Ÿà ⁄©ÿ≥ ŸÇÿ≥ŸÖ ⁄©€í ÿ∑ÿ®€å ⁄Ü€å⁄© ÿßŸæ ⁄©ÿ±ŸàÿßŸÜ€í ⁄Üÿß€Åÿ¶€å⁄∫ÿü\n",
    "Output:\n",
    "q1: ŸÑÿß€ÅŸàÿ± ÿßŸàÿ± ÿØ€ÅŸÑ€å ŸÖ€å⁄∫ ŸÅÿ∂ÿßÿ¶€å ÿ¢ŸÑŸàÿØ⁄Ø€å ⁄©€å ÿ≥ÿ∑ÿ≠ ⁄©ÿß ŸÖŸàÿßÿ≤ŸÜ€Å ⁄©€åÿß €Å€íÿü\n",
    "q2: ŸÅÿ∂ÿßÿ¶€å ÿ¢ŸÑŸàÿØ⁄Ø€å Ÿæ⁄æ€åŸæ⁄æ⁄ëŸà⁄∫ ⁄©€í ⁄©€åŸÜÿ≥ÿ± ⁄©ÿß ÿ≥ÿ®ÿ® ⁄©€åÿ≥€í ÿ®ŸÜÿ™€å €Å€íÿü\n",
    "\n",
    "**Example 2**:\n",
    "Input: ÿß⁄Øÿ± ⁄©ÿ±ÿß⁄Ü€å ŸÖ€å⁄∫ ÿ®ÿ¨ŸÑ€å ⁄©€í ŸÜÿ±ÿÆ 30% ÿ®⁄ë⁄æ ⁄Øÿ¶€í €Å€å⁄∫ ÿßŸàÿ± €å€Å ÿµŸÜÿπÿ™Ÿà⁄∫ ⁄©Ÿà ŸÖÿ™ÿßÿ´ÿ± ⁄©ÿ± ÿ±€Åÿß €Å€íÿå ÿ™Ÿà ÿ≠⁄©ŸàŸÖÿ™ ⁄©Ÿà ⁄©ŸàŸÜ ÿ≥€å ÿ≥ÿ®ÿ≥⁄à€åÿß⁄∫ ÿØ€åŸÜ€å ⁄Üÿß€Åÿ¶€å⁄∫ÿü\n",
    "Output:\n",
    "q1: ⁄©ÿ±ÿß⁄Ü€å ŸÖ€å⁄∫ ÿ®ÿ¨ŸÑ€å ⁄©€í ŸÜÿ±ÿÆŸà⁄∫ ŸÖ€å⁄∫ ÿßÿ∂ÿßŸÅ€í ⁄©€å ŸÖŸàÿ¨ŸàÿØ€Å ÿ¥ÿ±ÿ≠ ⁄©€åÿß €Å€íÿü\n",
    "q2: ÿ®ÿ¨ŸÑ€å ⁄©€í ŸÖ€ÅŸÜ⁄Ø€í €ÅŸàŸÜ€í ÿ≥€í ÿµŸÜÿπÿ™Ÿà⁄∫ Ÿæÿ± ⁄©ÿ≥ ŸÇÿ≥ŸÖ ⁄©€í ÿßÿ´ÿ±ÿßÿ™ ŸÖÿ±ÿ™ÿ® €ÅŸà ÿ±€Å€í €Å€å⁄∫ÿü\n",
    "\n",
    "**Current Task**:\n",
    "Input: {urdu_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = response['response'].strip()\n",
    "        \n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "        \n",
    "        return result if len(result) == 2 else {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c6ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_context_relevance_check(query_urdu: str, context_urdu: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "You are a binary classifier.\n",
    "\n",
    "Your task is to decide whether the following Urdu *context* is relevant to the Urdu *question*. You must answer ONLY with **True** or **False** ‚Äî no explanation, no commentary, just one word: True or False.\n",
    "\n",
    "Criteria:\n",
    "- If the context helps answer the question directly or indirectly, reply: True\n",
    "- If the context is unrelated, confusing, or insufficient, reply: False\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT explain your answer.\n",
    "- Do NOT include any additional comments.\n",
    "- Just respond with: True or False\n",
    "\n",
    "---\n",
    "\n",
    "Question (Urdu): {query_urdu}\n",
    "\n",
    "Context (Urdu): {context_urdu}\n",
    "\n",
    "Answer (True/False):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        return answer == 'true'\n",
    "    except Exception as e:\n",
    "        print(f\"Error during relevance check: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad404d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer model (E5-large)\n",
    "    model = SentenceTransformer(\"intfloat/e5-large\", device=device)\n",
    "    \n",
    "    # Configure for Urdu text\n",
    "    model.max_seq_length = 512  # Set based on your earlier analysis\n",
    "    model.tokenizer.do_lower_case = False  # Preserve Urdu characters\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd8112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_faiss_index.index\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_chunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ec7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query,k=3):\n",
    "   \n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,  # Return numpy array for FAISS\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Search FAISS index\n",
    "    _, indices = index.search(query_embedding, k)  # Dummy variable _ for distances\n",
    "    \n",
    "    # Return only the chunks\n",
    "    return [chunks_list[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b61a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_of_multihop_without_parallel(query,model=model,index=index,chunks_list=chunks_list,k=3):\n",
    "\n",
    "\n",
    "    classification = classify_urdu_question(query)\n",
    "\n",
    "\n",
    "    if classification == \"singlehop\":\n",
    "        retrieved_context = retrieve_documents(query,k)\n",
    "        return retrieved_context\n",
    "        \n",
    "\n",
    "    if classification == \"multihop\":\n",
    "        decomposition = decompose_urdu_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "\n",
    "        main_context = retrieve_documents(q1, k)\n",
    "\n",
    "        for i in range(min(len(main_context), k)):\n",
    "            intermediate_ctx = main_context[i]\n",
    "            \n",
    "            combined_query = q1 + intermediate_ctx + q2\n",
    "            \n",
    "            second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "            \n",
    "            for ctx in second_hop_contexts:\n",
    "                if query_context_relevance_check(query, ctx):\n",
    "                    main_context.append(ctx)\n",
    "        \n",
    "        return main_context    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b0ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def expand_multihop_context(intermediate_ctx, query, q1, q2, k):\n",
    "    try:\n",
    "        combined_query = q1 + intermediate_ctx + q2\n",
    "        second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "\n",
    "        relevant_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as inner_executor:\n",
    "            futures = [\n",
    "                inner_executor.submit(query_context_relevance_check, query, ctx)\n",
    "                for ctx in second_hop_contexts\n",
    "            ]\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        relevant_contexts.append(second_hop_contexts[i])\n",
    "                except Exception as e:\n",
    "                    print(\"Error during relevance check:\", e)\n",
    "\n",
    "        return relevant_contexts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in expand_multihop_context:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_context_of_multihop(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Measure classification time\n",
    "    start_classification = time.time()\n",
    "    classification = classify_urdu_question(query)\n",
    "    classification_time = time.time() - start_classification\n",
    "\n",
    "    if type == \"easy\":\n",
    "        decomposition_time = 0.0\n",
    "        start_retrieval = time.time()\n",
    "        context = retrieve_documents(query, k)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "        return context, classification, classification_time, decomposition_time, retrieval_time\n",
    "\n",
    "    else:\n",
    "        start_decomposition = time.time()\n",
    "        decomposition = decompose_urdu_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "        decomposition_time = time.time() - start_decomposition\n",
    "\n",
    "        start_retrieval = time.time()\n",
    "        main_context = retrieve_documents(q1, k)\n",
    "        additional_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in main_context[:k]\n",
    "            ]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        main_context.extend(additional_contexts)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "\n",
    "        return main_context, classification, classification_time, decomposition_time, retrieval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab86347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_llama3(context, query):\n",
    "    prompt = f\"\"\"You are a helpful assistant. You will be given a context and a question, both written in Urdu.\n",
    "Your task is to answer the question using only the information from the context.\n",
    "Your response must:\n",
    "- Be written entirely in Urdu\n",
    "- Not include any English words\n",
    "- Not include translations\n",
    "- Be clear and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer (in Urdu only):\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        return \"ÿ¨Ÿàÿßÿ® Ÿæ€åÿØÿß ⁄©ÿ±ŸÜ€í ŸÖ€å⁄∫ ÿÆÿ±ÿßÿ®€å €ÅŸàÿ¶€å€î\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6790a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multihop_handling_LQR(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Step 1: Get context and timings\n",
    "    context, classification, classification_time, decomposition_time, retrieval_time = get_context_of_multihop(\n",
    "        query, type, model=model, index=index, chunks_list=chunks_list, k=k\n",
    "    )\n",
    "\n",
    "    # Flatten context if it's a list of strings\n",
    "    if isinstance(context, list):\n",
    "        combined_context = \"\\n\".join(context)\n",
    "    else:\n",
    "        combined_context = context\n",
    "\n",
    "    # Step 2: Generate answer and measure time\n",
    "    start_gen = time.time()\n",
    "    final_answer = generate_using_llama3(combined_context,query)\n",
    "    generation_time = time.time() - start_gen\n",
    "\n",
    "    # Step 3: Compute total time\n",
    "    total_time = classification_time + decomposition_time + retrieval_time + generation_time\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"timings\": {\n",
    "            \"classification_time\": classification_time,\n",
    "            \"decomposition_time\": decomposition_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/LQR_processed_results.csv\"\n",
    "\n",
    "# Initialize variables\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop over each question in the DataFrame\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Run the pipeline\n",
    "        result = multihop_handling_LQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Save and report every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(idx), index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save any remaining results at the end\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(len(df) % 100), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Final stats\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / len(df)\n",
    "print(f\"\\n‚úÖ All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/LQR_processed_results.csv\"\n",
    "\n",
    "# Start processing from record 99 (0-based index 98)\n",
    "start_index = 98\n",
    "\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop using range for exact control over indices\n",
    "for idx in tqdm(range(start_index, len(df)), total=len(df) - start_index):\n",
    "    row = df.iloc[idx]\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Run the pipeline\n",
    "        result = multihop_handling_LQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Save and report every 100 records\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        # Append without header\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save remaining results\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / (len(df) - start_index)\n",
    "print(f\"\\n‚úÖ All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time  per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
