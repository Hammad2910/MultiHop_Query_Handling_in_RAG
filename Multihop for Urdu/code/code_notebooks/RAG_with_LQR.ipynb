{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08690e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb89170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classification Function ---\n",
    "def classify_urdu_question(urdu_question):\n",
    "    prompt = f\"\"\"You are given a question in Urdu. Your task is to classify it as either SINGLEHOP or MULTIHOP.\n",
    "\n",
    "Definitions:\n",
    "- A question is **SINGLEHOP** if it can be answered using a single fact, sentence, or document. The question may be long, but if it doesn't require combining or reasoning over multiple distinct pieces of information, it's SINGLEHOP.\n",
    "- A question is **MULTIHOP** if answering it requires combining multiple facts, reasoning over several steps, or connecting pieces of information from different sources.\n",
    "\n",
    "⚠️ Important:\n",
    "- A question's length does not determine its type. A long, descriptive question can still be SINGLEHOP.\n",
    "- MULTIHOP questions typically require you to first find one piece of information, then use that to find the next.\n",
    "\n",
    "Examples:\n",
    "\n",
    "🔹 Example 1 (SINGLEHOP - short and factual):\n",
    "سوال: \"پاکستان کے وزیر اعظم کا نام کیا ہے؟\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔹 Example 2 (SINGLEHOP - long, descriptive):\n",
    "سوال: \"میں بلیک ہیٹ پہنتا ہوں: بدمعاشوں کے ساتھ جدوجہد (حقیقی اور خیالی) ایک کتاب ہے جو چاک کلوسٹر مین نے لکھی ہے ، جو پہلی بار اسکربر نے شائع کی تھی۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔹 Example 3 (SINGLEHOP - event description):\n",
    "سوال: \"یہ فلم سنہ 2010 میں ریلیز ہوئی تھی اور اس میں 16 قتلوں کی کہانی بیان کی گئی ہے جو سنہ 1828 میں سکاٹ لینڈ کے شہر ایڈنبرا میں ہوئے تھے۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔹 Example 4 (SINGLEHOP - company and product):\n",
    "سوال: \"والگریریا فرانزی ایک اطالوی چمڑے کے بیگ اور سامان کی کمپنی تھی جس کی بنیاد 1864 میں روکو فرانزی نے رکھی تھی۔ گچی نے اس کمپنی کے لئے کام کیا جب تک کہ اس نے گچی کو قائم نہیں کیا۔\"\n",
    "جواب: SINGLEHOP\n",
    "\n",
    "🔸 Example 5 (MULTIHOP - indirect question):\n",
    "سوال: \"جارج واشنگٹن کی پیدائش کے وقت امریکہ کا صدر کون تھا؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 6 (MULTIHOP - location relation):\n",
    "سوال: \"نلسن منڈیلا کو کس جیل میں رکھا گیا اور وہ جیل کس جزیرے پر تھی؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 7 (MULTIHOP - historical causality):\n",
    "سوال: \"انگریزوں کے خلاف 1857 کی جنگ کے بعد ہندوستان میں کونسا سیاسی نظام قائم ہوا؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 8 (MULTIHOP - lengthy, layered facts):\n",
    "سوال: \"ایپل کمپنی کے شریک بانی اسٹیو جابز نے کس یونیورسٹی سے تعلیم حاصل کی، اور اس یونیورسٹی کے سب سے معروف گریجویٹس میں سے ایک کون ہے؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "🔸 Example 9 (MULTIHOP - lengthy with connections):\n",
    "سوال: \"شیکسپیئر کے وہ ڈرامے جو بادشاہوں پر مبنی ہیں، ان میں سے ایک کے مرکزی کردار کی موت کس جنگ میں ہوئی تھی؟\"\n",
    "جواب: MULTIHOP\n",
    "\n",
    "---\n",
    "\n",
    "Now read the following Urdu question and classify it:\n",
    "\n",
    "سوال: \"{urdu_question}\"\n",
    "جواب:\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response['message']['content'].strip().lower()\n",
    "\n",
    "        # Normalize output\n",
    "        if 'multihop' in reply:\n",
    "            return 'multihop'\n",
    "        elif 'singlehop' in reply or 'simple' in reply:\n",
    "            return 'singlehop'\n",
    "\n",
    "        print(f\"⚠️ Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing question: {urdu_question}\\n↪ {e}\")\n",
    "        return \"error\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134f1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_urdu_query(urdu_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "**Role**: You are an expert Urdu linguistic analyst specializing in question decomposition. Your task is to break down complex Urdu questions into their fundamental components.\n",
    "\n",
    "**Task Instructions**:\n",
    "1. Carefully analyze the given Urdu question to identify its core components\n",
    "2. Extract exactly 2 sub-questions that:\n",
    "   - Are necessary to answer the main question\n",
    "   - Cover distinct aspects of the problem\n",
    "   - Have clear logical progression (answer to q1 helps answer q2)\n",
    "3. Both sub-questions must:\n",
    "   - Be in proper Urdu language\n",
    "   - Be grammatically correct\n",
    "   - Be clear and concise\n",
    "   - Use relevant domain terminology\n",
    "\n",
    "**Output Format Requirements**:\n",
    "- Use EXACTLY this format:\n",
    "  q1: [پہلا ذیلی سوال]\n",
    "  q2: [دوسرا ذیلی سوال]\n",
    "- Each sub-question must be on a new line\n",
    "- Do not include any additional commentary or explanation\n",
    "- Do not number the questions (use only q1:/q2: prefixes)\n",
    "\n",
    "**Example 1**:\n",
    "Input: اگر لاہور میں فضائی آلودگی کی سطح دہلی سے زیادہ ہے اور فضائی آلودگی پھیپھڑوں کے کینسر کا سبب بن سکتی ہے، تو لاہور کے رہائشیوں کو کس قسم کے طبی چیک اپ کروانے چاہئیں؟\n",
    "Output:\n",
    "q1: لاہور اور دہلی میں فضائی آلودگی کی سطح کا موازنہ کیا ہے؟\n",
    "q2: فضائی آلودگی پھیپھڑوں کے کینسر کا سبب کیسے بنتی ہے؟\n",
    "\n",
    "**Example 2**:\n",
    "Input: اگر کراچی میں بجلی کے نرخ 30% بڑھ گئے ہیں اور یہ صنعتوں کو متاثر کر رہا ہے، تو حکومت کو کون سی سبسڈیاں دینی چاہئیں؟\n",
    "Output:\n",
    "q1: کراچی میں بجلی کے نرخوں میں اضافے کی موجودہ شرح کیا ہے؟\n",
    "q2: بجلی کے مہنگے ہونے سے صنعتوں پر کس قسم کے اثرات مرتب ہو رہے ہیں؟\n",
    "\n",
    "**Current Task**:\n",
    "Input: {urdu_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = response['response'].strip()\n",
    "        \n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "        \n",
    "        return result if len(result) == 2 else {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c6ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_context_relevance_check(query_urdu: str, context_urdu: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "You are a binary classifier.\n",
    "\n",
    "Your task is to decide whether the following Urdu *context* is relevant to the Urdu *question*. You must answer ONLY with **True** or **False** — no explanation, no commentary, just one word: True or False.\n",
    "\n",
    "Criteria:\n",
    "- If the context helps answer the question directly or indirectly, reply: True\n",
    "- If the context is unrelated, confusing, or insufficient, reply: False\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT explain your answer.\n",
    "- Do NOT include any additional comments.\n",
    "- Just respond with: True or False\n",
    "\n",
    "---\n",
    "\n",
    "Question (Urdu): {query_urdu}\n",
    "\n",
    "Context (Urdu): {context_urdu}\n",
    "\n",
    "Answer (True/False):\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3:8b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().lower()\n",
    "        return answer == 'true'\n",
    "    except Exception as e:\n",
    "        print(f\"Error during relevance check: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad404d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Academic Work\\Multihop_Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_retriever(\n",
    "    index_path: str,\n",
    "    chunks_path: str\n",
    "):\n",
    "    # Initialize device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Load SentenceTransformer model (E5-large)\n",
    "    model = SentenceTransformer(\"intfloat/e5-large\", device=device)\n",
    "    \n",
    "    # Configure for Urdu text\n",
    "    model.max_seq_length = 512  # Set based on your earlier analysis\n",
    "    model.tokenizer.do_lower_case = False  # Preserve Urdu characters\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load stored chunks\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks_list = pickle.load(f)\n",
    "    \n",
    "    return model, index, chunks_list, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd8112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, index, chunks_list, device = load_retriever(\n",
    "    index_path=\"../../vector_db/paragraphs/5884_paras/5884_paras_faiss_index.index\",\n",
    "    chunks_path=\"../../data_storage/Paragraph_chunks/5884_paragraphs/5884_chunks.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ec7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query,k=3):\n",
    "   \n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,  # Return numpy array for FAISS\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_tensor=False,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    # Search FAISS index\n",
    "    _, indices = index.search(query_embedding, k)  # Dummy variable _ for distances\n",
    "    \n",
    "    # Return only the chunks\n",
    "    return [chunks_list[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b61a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_of_multihop_without_parallel(query,model=model,index=index,chunks_list=chunks_list,k=3):\n",
    "\n",
    "\n",
    "    classification = classify_urdu_question(query)\n",
    "\n",
    "\n",
    "    if classification == \"singlehop\":\n",
    "        retrieved_context = retrieve_documents(query,k)\n",
    "        return retrieved_context\n",
    "        \n",
    "\n",
    "    if classification == \"multihop\":\n",
    "        decomposition = decompose_urdu_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "\n",
    "        main_context = retrieve_documents(q1, k)\n",
    "\n",
    "        for i in range(min(len(main_context), k)):\n",
    "            intermediate_ctx = main_context[i]\n",
    "            \n",
    "            combined_query = q1 + intermediate_ctx + q2\n",
    "            \n",
    "            second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "            \n",
    "            for ctx in second_hop_contexts:\n",
    "                if query_context_relevance_check(query, ctx):\n",
    "                    main_context.append(ctx)\n",
    "        \n",
    "        return main_context    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b0ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def expand_multihop_context(intermediate_ctx, query, q1, q2, k):\n",
    "    try:\n",
    "        combined_query = q1 + intermediate_ctx + q2\n",
    "        second_hop_contexts = retrieve_documents(combined_query, k)\n",
    "\n",
    "        relevant_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as inner_executor:\n",
    "            futures = [\n",
    "                inner_executor.submit(query_context_relevance_check, query, ctx)\n",
    "                for ctx in second_hop_contexts\n",
    "            ]\n",
    "\n",
    "            for i, future in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        relevant_contexts.append(second_hop_contexts[i])\n",
    "                except Exception as e:\n",
    "                    print(\"Error during relevance check:\", e)\n",
    "\n",
    "        return relevant_contexts\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in expand_multihop_context:\", e)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_context_of_multihop(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Measure classification time\n",
    "    start_classification = time.time()\n",
    "    classification = classify_urdu_question(query)\n",
    "    classification_time = time.time() - start_classification\n",
    "\n",
    "    if type == \"easy\":\n",
    "        decomposition_time = 0.0\n",
    "        start_retrieval = time.time()\n",
    "        context = retrieve_documents(query, k)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "        return context, classification, classification_time, decomposition_time, retrieval_time\n",
    "\n",
    "    else:\n",
    "        start_decomposition = time.time()\n",
    "        decomposition = decompose_urdu_query(query)\n",
    "        q1 = decomposition.get(\"q1\", \"\")\n",
    "        q2 = decomposition.get(\"q2\", \"\")\n",
    "        decomposition_time = time.time() - start_decomposition\n",
    "\n",
    "        start_retrieval = time.time()\n",
    "        main_context = retrieve_documents(q1, k)\n",
    "        additional_contexts = []\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(expand_multihop_context, ctx, query, q1, q2, k)\n",
    "                for ctx in main_context[:k]\n",
    "            ]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                additional_contexts.extend(result)\n",
    "\n",
    "        main_context.extend(additional_contexts)\n",
    "        retrieval_time = time.time() - start_retrieval\n",
    "\n",
    "        return main_context, classification, classification_time, decomposition_time, retrieval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab86347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_using_llama3(context, query):\n",
    "    prompt = f\"\"\"You are a helpful assistant. You will be given a context and a question, both written in Urdu.\n",
    "Your task is to answer the question using only the information from the context.\n",
    "Your response must:\n",
    "- Be written entirely in Urdu\n",
    "- Not include any English words\n",
    "- Not include translations\n",
    "- Be clear and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer (in Urdu only):\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error during generation:\", e)\n",
    "        return \"جواب پیدا کرنے میں خرابی ہوئی۔\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6790a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multihop_handling_LQR(query, type, model=model, index=index, chunks_list=chunks_list, k=3):\n",
    "    # Step 1: Get context and timings\n",
    "    context, classification, classification_time, decomposition_time, retrieval_time = get_context_of_multihop(\n",
    "        query, type, model=model, index=index, chunks_list=chunks_list, k=k\n",
    "    )\n",
    "\n",
    "    # Flatten context if it's a list of strings\n",
    "    if isinstance(context, list):\n",
    "        combined_context = \"\\n\".join(context)\n",
    "    else:\n",
    "        combined_context = context\n",
    "\n",
    "    # Step 2: Generate answer and measure time\n",
    "    start_gen = time.time()\n",
    "    final_answer = generate_using_llama3(combined_context,query)\n",
    "    generation_time = time.time() - start_gen\n",
    "\n",
    "    # Step 3: Compute total time\n",
    "    total_time = classification_time + decomposition_time + retrieval_time + generation_time\n",
    "\n",
    "    return {\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"timings\": {\n",
    "            \"classification_time\": classification_time,\n",
    "            \"decomposition_time\": decomposition_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/LQR_processed_results.csv\"\n",
    "\n",
    "# Initialize variables\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop over each question in the DataFrame\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Run the pipeline\n",
    "        result = multihop_handling_LQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Save and report every 100 queries\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(idx), index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save any remaining results at the end\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=not bool(len(df) % 100), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Final stats\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / len(df)\n",
    "print(f\"\\n✅ All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your source CSV\n",
    "df = pd.read_csv(\"../../../Dataset_code_csvs/hotpotQA/hotpotQA_dataset_versions/5884paras_598queries/Urdu/598_QnAs_translated.csv\")\n",
    "\n",
    "# Output CSV path\n",
    "output_path = \"../../results/pipeline results/5884paras_598qna/LQR_processed_results.csv\"\n",
    "\n",
    "# Start processing from record 99 (0-based index 98)\n",
    "start_index = 98\n",
    "\n",
    "results = []\n",
    "batch_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "# Loop using range for exact control over indices\n",
    "for idx in tqdm(range(start_index, len(df)), total=len(df) - start_index):\n",
    "    row = df.iloc[idx]\n",
    "    query = row[\"translated_question\"]\n",
    "    answer = row[\"translated_answer\"]\n",
    "    question_type = row[\"level\"]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Run the pipeline\n",
    "        result = multihop_handling_LQR(query, question_type)\n",
    "\n",
    "        classification = result[\"classification\"]\n",
    "        retrieved_context = result[\"retrieved_context\"]\n",
    "        final_answer = result[\"final_answer\"]\n",
    "        timings = result[\"timings\"]\n",
    "        total_time_one = timings[\"total_time\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query {idx}: {e}\")\n",
    "        classification = \"Error\"\n",
    "        retrieved_context = \"Error\"\n",
    "        final_answer = \"Error\"\n",
    "        timings = {\n",
    "            \"classification_time\": 0,\n",
    "            \"decomposition_time\": 0,\n",
    "            \"retrieval_time\": 0,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "        total_time_one = 0\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    batch_times.append(elapsed)\n",
    "\n",
    "    results.append({\n",
    "        \"translated_question\": query,\n",
    "        \"translated_answer\": answer,\n",
    "        \"classification\": classification,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"classification_time\": timings[\"classification_time\"],\n",
    "        \"decomposition_time\": timings[\"decomposition_time\"],\n",
    "        \"retrieval_time\": timings[\"retrieval_time\"],\n",
    "        \"generation_time\": timings[\"generation_time\"],\n",
    "        \"total_time\": timings[\"total_time\"],\n",
    "        \"level\": question_type\n",
    "    })\n",
    "\n",
    "    print(f\"Processed record {idx+1}/{len(df)} in {elapsed:.2f} seconds.\")\n",
    "\n",
    "    # Save and report every 100 records\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        # Append without header\n",
    "        pd.DataFrame(results).to_csv(output_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\")\n",
    "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"\\n--- Saved batch up to record {idx+1}\")\n",
    "        print(f\"Average time for last 100 records: {avg_batch_time:.2f} seconds\\n\")\n",
    "        results = []\n",
    "        batch_times = []\n",
    "\n",
    "# Save remaining results\n",
    "if results:\n",
    "    pd.DataFrame(results).to_csv(output_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "avg_total_time = total_elapsed / (len(df) - start_index)\n",
    "print(f\"\\n✅ All records processed.\")\n",
    "print(f\"Total processing time: {total_elapsed:.2f} seconds.\")\n",
    "print(f\"Average time  per record: {avg_total_time:.2f} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
