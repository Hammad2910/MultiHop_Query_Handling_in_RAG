{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17428c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop for Urdu\\Multihop for Urdu\\code\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\hammad workings\\Thesis\\Multihop for Urdu\\Multihop for Urdu\\code\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\23030013\\.cache\\huggingface\\hub\\models--bigscience--bloomz-560m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to English: Je tâ€™aime. I love you.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\bloomz-560m\\\\tokenizer_config.json',\n",
       " 'C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\bloomz-560m\\\\special_tokens_map.json',\n",
       " 'C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\bloomz-560m\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_directory = \"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\bloomz-560m\"\n",
    "# model.save_pretrained(save_directory)\n",
    "# tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80276ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_directory = \"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\model_weights\\\\bloomz-560m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(load_directory, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9237efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_urdu_question(urdu_question):\n",
    "    # Create the prompt with English instructions and Urdu question\n",
    "    prompt = f\"\"\"Classify the following Urdu question as either \"simple\" (requires no context), \"singlehop\" (requires one inference step), or \"multihop\" (requires multiple reasoning steps):\n",
    "\n",
    "{urdu_question}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=10,  # We only need a short answer\n",
    "        temperature=0.1,    # Lower temperature for more deterministic output\n",
    "        do_sample=False,    # Use greedy decoding for consistent results\n",
    "        num_beams=3         # Simple beam search for better quality\n",
    "    )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the classification from the result\n",
    "    # This will return everything after the prompt\n",
    "    classification = result.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ab2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_urdu_question(urdu_question):\n",
    "    prompt = f\"\"\"\n",
    "You are a reasoning expert. Your task is to classify the complexity of a question written in Urdu into one of three categories:\n",
    "\n",
    "- \"simple\": A fact-based question that can be answered without needing additional context or inference.\n",
    "- \"singlehop\": A question that requires one reasoning step or fact connection.\n",
    "- \"multihop\": A question that needs multiple reasoning steps or combining information from different parts.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Question (Urdu): Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "Classification: simple\n",
    "\n",
    "Example 2:\n",
    "Question (Urdu): ÙˆÛ Ø´Ø®Øµ Ú©ÙˆÙ† ØªÚ¾Ø§ Ø¬Ùˆ Ù‚Ø§Ø¦Ø¯ Ø§Ø¹Ø¸Ù… Ú©Û’ Ø¨Ø¹Ø¯ Ú¯ÙˆØ±Ù†Ø± Ø¬Ù†Ø±Ù„ Ø¨Ù†Ø§ØŸ\n",
    "Classification: singlehop\n",
    "\n",
    "Example 3:\n",
    "Question (Urdu): ÙˆÛ Ø³Ø§Ø¦Ù†Ø³Ø¯Ø§Ù† Ú©ÙˆÙ† ÛÛ’ Ø¬Ø³ Ù†Û’ Ù†Ø¸Ø±ÛŒÛ Ø§Ø±ØªÙ‚Ø§Ø¡ Ù¾ÛŒØ´ Ú©ÛŒØ§ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ Ø§Ø«Ø± Ø¨ÛŒØ³ÙˆÛŒÚº ØµØ¯ÛŒ Ú©ÛŒ Ø­ÛŒØ§ØªÛŒØ§Øª Ù¾Ø± Ú©ÛŒØ§ Ù¾Ú‘Ø§ØŸ\n",
    "Classification: multihop\n",
    "\n",
    "Now classify the following question:\n",
    "\n",
    "Question (Urdu): {urdu_question}\n",
    "Classification:\"\"\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.1,\n",
    "        do_sample=False,\n",
    "        num_beams=3\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    classification = result.replace(prompt, \"\").strip()\n",
    "\n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187b392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop for Urdu\\Multihop for Urdu\\code\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "Classification: simple\n",
      "Question: Ø¬Ø³ Ù…Ù„Ú© Ú©ÛŒ Ø³Ø±Ø­Ø¯ Ú†ÛŒÙ† Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ØªÛŒ ÛÛ’ Ø§Ø³ Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "Classification: singlehop\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "urdu_question = \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\"  # \"What is the capital of Pakistan?\"\n",
    "classification = classify_urdu_question(urdu_question)\n",
    "print(f\"Question: {urdu_question}\")\n",
    "print(f\"Classification: {classification}\")\n",
    "\n",
    "# You can test with different complexity questions\n",
    "urdu_question2 = \"Ø¬Ø³ Ù…Ù„Ú© Ú©ÛŒ Ø³Ø±Ø­Ø¯ Ú†ÛŒÙ† Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ØªÛŒ ÛÛ’ Ø§Ø³ Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\"  # Multihop: \"What is the capital of the country that borders China?\"\n",
    "classification2 = classify_urdu_question(urdu_question2)\n",
    "print(f\"Question: {urdu_question2}\")\n",
    "print(f\"Classification: {classification2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70df3653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop for Urdu\\Multihop for Urdu\\code\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [13:57<00:00,  8.55s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\Dataset\\\\Hotpotqa\\\\1000_paras_100_queries\\\\translated_dataset_100_qna.csv\")  # Replace with your actual file name\n",
    "\n",
    "# Ensure 'translated_question' column exists\n",
    "if 'translated_question' not in df.columns:\n",
    "    raise ValueError(\"Column 'translated_question' not found in the CSV.\")\n",
    "\n",
    "# Add a new column for the classification results\n",
    "tqdm.pandas()  # Enables progress bar with apply\n",
    "df['question_type'] = df['translated_question'].progress_apply(classify_urdu_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "540daffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [04:09<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Total questions: 98\n",
      "âœ… Correctly classified: 39\n",
      "âœ… Accuracy: 39.80%\n",
      "\n",
      "ğŸ” Accuracy by level:\n",
      "  easy: 44.44% (8/18)\n",
      "  medium: 46.67% (14/30)\n",
      "  hard: 34.00% (17/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def classify_urdu_question(urdu_question):\n",
    "    prompt = f\"\"\"\n",
    "You are a reasoning expert. Your task is to classify the complexity of a question written in Urdu into one of three categories:\n",
    "\n",
    "- \"simple\": A fact-based question that can be answered without needing additional context or inference.\n",
    "- \"singlehop\": A question that requires one reasoning step or fact connection.\n",
    "- \"multihop\": A question that needs multiple reasoning steps or combining information from different parts.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Question (Urdu): Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "Classification: simple\n",
    "\n",
    "Example 2:\n",
    "Question (Urdu): ÙˆÛ Ø´Ø®Øµ Ú©ÙˆÙ† ØªÚ¾Ø§ Ø¬Ùˆ Ù‚Ø§Ø¦Ø¯ Ø§Ø¹Ø¸Ù… Ú©Û’ Ø¨Ø¹Ø¯ Ú¯ÙˆØ±Ù†Ø± Ø¬Ù†Ø±Ù„ Ø¨Ù†Ø§ØŸ\n",
    "Classification: singlehop\n",
    "\n",
    "Example 3:\n",
    "Question (Urdu): ÙˆÛ Ø³Ø§Ø¦Ù†Ø³Ø¯Ø§Ù† Ú©ÙˆÙ† ÛÛ’ Ø¬Ø³ Ù†Û’ Ù†Ø¸Ø±ÛŒÛ Ø§Ø±ØªÙ‚Ø§Ø¡ Ù¾ÛŒØ´ Ú©ÛŒØ§ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ Ø§Ø«Ø± Ø¨ÛŒØ³ÙˆÛŒÚº ØµØ¯ÛŒ Ú©ÛŒ Ø­ÛŒØ§ØªÛŒØ§Øª Ù¾Ø± Ú©ÛŒØ§ Ù¾Ú‘Ø§ØŸ\n",
    "Classification: multihop\n",
    "\n",
    "Now classify the following question:\n",
    "\n",
    "Question (Urdu): {urdu_question}\n",
    "Classification:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response['message']['content'].strip().lower()\n",
    "\n",
    "        # Normalize to allowed values\n",
    "        for label in ['simple', 'singlehop', 'multihop']:\n",
    "            if label in reply:\n",
    "                return label\n",
    "\n",
    "        print(f\"âš ï¸ Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing question: {urdu_question}\\nâ†ª {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\Dataset\\\\Hotpotqa\\\\1000_paras_100_queries\\\\translated_dataset_100_qna.csv\")\n",
    "\n",
    "# Make sure necessary columns exist\n",
    "required_columns = {'translated_question', 'level'}\n",
    "if not required_columns.issubset(df.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "# Enable tqdm for apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Generate temporary 'question_type' in memory (NOT saving to CSV)\n",
    "df_temp = df.copy()\n",
    "df_temp['question_type'] = df_temp['translated_question'].progress_apply(classify_urdu_question)\n",
    "\n",
    "# Define classification check logic\n",
    "def is_correct(row):\n",
    "    level = str(row['level']).strip().lower()\n",
    "    q_type = str(row['question_type']).strip().lower()\n",
    "\n",
    "    if level == 'easy' and q_type in {'simple', 'singlehop'}:\n",
    "        return True\n",
    "    elif level in {'medium', 'hard'} and q_type == 'multihop':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Apply correctness check\n",
    "df_temp['correct_classification'] = df_temp.apply(is_correct, axis=1)\n",
    "\n",
    "# Summary stats\n",
    "total = len(df_temp)\n",
    "correct = df_temp['correct_classification'].sum()\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"\\nâœ… Total questions: {total}\")\n",
    "print(f\"âœ… Correctly classified: {correct}\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Per-level analysis\n",
    "print(\"\\nğŸ” Accuracy by level:\")\n",
    "for level in df_temp['level'].unique():\n",
    "    subset = df_temp[df_temp['level'].str.lower() == level.lower()]\n",
    "    correct_subset = subset['correct_classification'].sum()\n",
    "    total_subset = len(subset)\n",
    "    acc = correct_subset / total_subset * 100 if total_subset else 0\n",
    "    print(f\"  {level}: {acc:.2f}% ({correct_subset}/{total_subset})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b394367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull(\"llama3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7aa576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [04:35<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Total questions: 98\n",
      "âœ… Correctly classified: 64\n",
      "âœ… Accuracy: 65.31%\n",
      "\n",
      "ğŸ” Accuracy by level:\n",
      "  easy: 16.67% (3/18)\n",
      "  medium: 73.33% (22/30)\n",
      "  hard: 78.00% (39/50)\n",
      "\n",
      "âŒ Misclassified questions:\n",
      "                                  translated_question level question_type\n",
      "0   Ø³ÛŒØ±ÛŒ Ø¨ÛŒ 2017 - 2017 (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†...  easy      multihop\n",
      "1   \"Ø¢Ú©Ø³ÙÙˆØ±Úˆ Ú©Ø§Ù„Ø¬ Ú©Û’ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ÛŒ Ø§ÛŒÙ„Ú© Ù†ÛŒÙ„Ø± ÚˆÚ©Ù† Ù†Û’ Ú©Û...  easy      multihop\n",
      "2   \"Ø¬Ø±Ø§Ø³Ú© Ù¾Ø§Ø±Ú© Ú©Û’ Ø§Ø¯Ø§Ú©Ø§Ø± ÚˆÛŒÙˆÚˆ ÛÙ†Ø±ÛŒ ÛÙˆØ§Ù†Ú¯ Ù†Û’ \"\"Ø¯ÛŒ ...  easy      multihop\n",
      "3   Ú©ÙˆÙ† Ø³Ø§ Ú©Ø±Ø¯Ø§Ø± ØŒ ÚˆÛŒÙ† Ú©Ø§Ø³Ù¹ÛŒÙ„ÛŒÙ†ÛŒÙ¹Ø§ Ú©ÛŒ Ø¢ÙˆØ§Ø² ØŒ Ø³Ù…Ù¾Ø³Ù†...  easy      multihop\n",
      "5   Ø§Ù…Ø±ÛŒÚ©ÛŒ Ø¨Ø§Ø³Ú©Ù¹ Ø¨Ø§Ù„ Ù¹ÛŒÙ… Ù†Û’ Ø§Ø³ Ø³Ø§Ù„ Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº Ø´Ú©Ø§Ú¯...  easy      multihop\n",
      "6   Ù„ÛŒÙ¹Ø±ÙˆØ¨ Ø¨Ø±ÛŒÙˆØ±ÛŒ Ú©Ù…Ù¾Ù†ÛŒ ØŒ Ø¬Ùˆ 1839 Ù…ÛŒÚº Ù‚Ø§Ø¦Ù… ÛÙˆØ¦ÛŒ ØªÚ¾...  easy      multihop\n",
      "7   Ø§Ù…Ø±ÛŒÚ©ÛŒ ÙÙˆØ¬ Ú©Û’ Ø§ÛŒÚ© Ø§ÙØ³Ø± Ø§ÙˆØ± Ù…Ù†Ø´ÛŒØ§Øª Ú©Û’ Ø§Ø³Ù…Ú¯Ù„Ù†Ú¯ Ú©...  easy      multihop\n",
      "8   Ø§ÛŒÙ†ÚˆØ±ÛŒÙˆ Ù…Ø§Ø±Ù¹Ù† ÚˆÙˆØ¨Ø± ØŒ Ø§ÛŒÚ© Ø§Ù…Ø±ÛŒÚ©ÛŒ Ù…Ø®Ù„ÙˆØ· Ù…Ø§Ø±Ø´Ù„ Ø¢Ø±...  easy      multihop\n",
      "9   Ø§ÛŒÙ…ÛŒÙ†Ù… Ú©Û’ Ø§Ù„Ø¨Ù… Ù…Ø§Ø±Ø´Ù„ Ù…ÛŒØªÚ¾Ø±Ø² Ø§ÛŒÙ„ Ù¾ÛŒ 2 Ù¾Ø± Ú©ÙˆÙ† Ø³Ø§...  easy      multihop\n",
      "11  ÚˆÛŒØ§Ù† ÙˆÙ„Ú©Ù†Ø² Ø§ÙˆØ± Ø§Ø³Ù¹ÛŒÙˆÙ† Ø§Ø³Ù¾ÛŒÙ„Ø¨Ø±Ú¯ Ù†ÛŒÙˆ ÛØ§Ù„ÛŒ ÙˆÙˆÚˆ Ú©Û’...  easy      multihop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Classification Function ---\n",
    "def classify_urdu_question(urdu_question):\n",
    "    prompt = f\"\"\"\n",
    "You are a reasoning expert. Your task is to classify the complexity of a question written in Urdu into one of two categories:\n",
    "\n",
    "- \"singlehop\": A question that can be answered with one reasoning step or simple fact retrieval.\n",
    "- \"multihop\": A question that requires multiple reasoning steps or combining information from different parts.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Question (Urdu): Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "Classification: singlehop\n",
    "\n",
    "Example 2:\n",
    "Question (Urdu): ÙˆÛ Ø´Ø®Øµ Ú©ÙˆÙ† ØªÚ¾Ø§ Ø¬Ùˆ Ù‚Ø§Ø¦Ø¯ Ø§Ø¹Ø¸Ù… Ú©Û’ Ø¨Ø¹Ø¯ Ú¯ÙˆØ±Ù†Ø± Ø¬Ù†Ø±Ù„ Ø¨Ù†Ø§ØŸ\n",
    "Classification: singlehop\n",
    "\n",
    "Example 3:\n",
    "Question (Urdu): ÙˆÛ Ø³Ø§Ø¦Ù†Ø³Ø¯Ø§Ù† Ú©ÙˆÙ† ÛÛ’ Ø¬Ø³ Ù†Û’ Ù†Ø¸Ø±ÛŒÛ Ø§Ø±ØªÙ‚Ø§Ø¡ Ù¾ÛŒØ´ Ú©ÛŒØ§ØŒ Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ Ø§Ø«Ø± Ø¨ÛŒØ³ÙˆÛŒÚº ØµØ¯ÛŒ Ú©ÛŒ Ø­ÛŒØ§ØªÛŒØ§Øª Ù¾Ø± Ú©ÛŒØ§ Ù¾Ú‘Ø§ØŸ\n",
    "Classification: multihop\n",
    "\n",
    "Now classify the following question:\n",
    "\n",
    "Question (Urdu): {urdu_question}\n",
    "Classification:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3:8b',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reply = response['message']['content'].strip().lower()\n",
    "\n",
    "        # Normalize output\n",
    "        if 'multihop' in reply:\n",
    "            return 'multihop'\n",
    "        elif 'singlehop' in reply or 'simple' in reply:\n",
    "            return 'singlehop'\n",
    "\n",
    "        print(f\"âš ï¸ Unexpected response: {reply}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing question: {urdu_question}\\nâ†ª {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "# --- Load Dataset ---\n",
    "df = pd.read_csv(\"C:\\\\hammad workings\\\\Thesis\\\\Multihop for Urdu\\\\Multihop for Urdu\\\\Dataset\\\\Hotpotqa\\\\1000_paras_100_queries\\\\translated_dataset_100_qna.csv\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'translated_question', 'level'}\n",
    "if not required_columns.issubset(df.columns):\n",
    "    raise ValueError(f\"Missing one of the required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "# Enable progress bar for apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create temporary working DataFrame\n",
    "df_temp = df.copy()\n",
    "df_temp['question_type'] = df_temp['translated_question'].progress_apply(classify_urdu_question)\n",
    "\n",
    "# --- Evaluation Logic ---\n",
    "def is_correct(row):\n",
    "    level = str(row['level']).strip().lower()\n",
    "    q_type = str(row['question_type']).strip().lower()\n",
    "\n",
    "    if level == 'easy' and q_type == 'singlehop':\n",
    "        return True\n",
    "    elif level in {'medium', 'hard'} and q_type == 'multihop':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_temp['correct_classification'] = df_temp.apply(is_correct, axis=1)\n",
    "\n",
    "# --- Evaluation Summary ---\n",
    "total = len(df_temp)\n",
    "correct = df_temp['correct_classification'].sum()\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"\\nâœ… Total questions: {total}\")\n",
    "print(f\"âœ… Correctly classified: {correct}\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# --- Per-level Accuracy ---\n",
    "print(\"\\nğŸ” Accuracy by level:\")\n",
    "for level in df_temp['level'].unique():\n",
    "    subset = df_temp[df_temp['level'].str.lower() == level.lower()]\n",
    "    correct_subset = subset['correct_classification'].sum()\n",
    "    total_subset = len(subset)\n",
    "    acc = correct_subset / total_subset * 100 if total_subset else 0\n",
    "    print(f\"  {level}: {acc:.2f}% ({correct_subset}/{total_subset})\")\n",
    "\n",
    "# --- Optional: Display misclassified examples ---\n",
    "print(\"\\nâŒ Misclassified questions:\")\n",
    "print(df_temp[df_temp['correct_classification'] == False][['translated_question', 'level', 'question_type']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d974d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  translated_question   level question_type\n",
      "0   Ø³ÛŒØ±ÛŒ Ø¨ÛŒ 2017 - 2017 (Ø§Ø³Ù¾Ø§Ù†Ø³Ø±Ø´Ù¾ Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ú©ÛŒ Ø¨Ù†...    easy      multihop\n",
      "1   \"Ø¢Ú©Ø³ÙÙˆØ±Úˆ Ú©Ø§Ù„Ø¬ Ú©Û’ Ø§ÛŒÚ© Ø³Ø§ØªÚ¾ÛŒ Ø§ÛŒÙ„Ú© Ù†ÛŒÙ„Ø± ÚˆÚ©Ù† Ù†Û’ Ú©Û...    easy      multihop\n",
      "3   Ú©ÙˆÙ† Ø³Ø§ Ú©Ø±Ø¯Ø§Ø± ØŒ ÚˆÛŒÙ† Ú©Ø§Ø³Ù¹ÛŒÙ„ÛŒÙ†ÛŒÙ¹Ø§ Ú©ÛŒ Ø¢ÙˆØ§Ø² ØŒ Ø³Ù…Ù¾Ø³Ù†...    easy      multihop\n",
      "4      Ú©ÙˆÙ† ØªÚ¾Ø§ Ø§ÛŒÚ© Ø­ØµÛ S#arpØŒ Ù„ÛŒ Ji-hye ÛŒØ§ Ú©Ø±Ù¹Ø³ Ø±Ø§Ø¦Ù¹ØŸ    easy      multihop\n",
      "5   Ø§Ù…Ø±ÛŒÚ©ÛŒ Ø¨Ø§Ø³Ú©Ù¹ Ø¨Ø§Ù„ Ù¹ÛŒÙ… Ù†Û’ Ø§Ø³ Ø³Ø§Ù„ Ú©Û’ Ø¢Ø®Ø± Ù…ÛŒÚº Ø´Ú©Ø§Ú¯...    easy      multihop\n",
      "6   Ù„ÛŒÙ¹Ø±ÙˆØ¨ Ø¨Ø±ÛŒÙˆØ±ÛŒ Ú©Ù…Ù¾Ù†ÛŒ ØŒ Ø¬Ùˆ 1839 Ù…ÛŒÚº Ù‚Ø§Ø¦Ù… ÛÙˆØ¦ÛŒ ØªÚ¾...    easy      multihop\n",
      "7   Ø§Ù…Ø±ÛŒÚ©ÛŒ ÙÙˆØ¬ Ú©Û’ Ø§ÛŒÚ© Ø§ÙØ³Ø± Ø§ÙˆØ± Ù…Ù†Ø´ÛŒØ§Øª Ú©Û’ Ø§Ø³Ù…Ú¯Ù„Ù†Ú¯ Ú©...    easy      multihop\n",
      "8   Ø§ÛŒÙ†ÚˆØ±ÛŒÙˆ Ù…Ø§Ø±Ù¹Ù† ÚˆÙˆØ¨Ø± ØŒ Ø§ÛŒÚ© Ø§Ù…Ø±ÛŒÚ©ÛŒ Ù…Ø®Ù„ÙˆØ· Ù…Ø§Ø±Ø´Ù„ Ø¢Ø±...    easy      multihop\n",
      "9   Ø§ÛŒÙ…ÛŒÙ†Ù… Ú©Û’ Ø§Ù„Ø¨Ù… Ù…Ø§Ø±Ø´Ù„ Ù…ÛŒØªÚ¾Ø±Ø² Ø§ÛŒÙ„ Ù¾ÛŒ 2 Ù¾Ø± Ú©ÙˆÙ† Ø³Ø§...    easy      multihop\n",
      "12  Ø§ÛŒÙ…ÛŒ Ù„ÛŒ Ø§ÙˆØ± Ù¹ÛŒ Ø§ÛŒØ³ Ø§ÛŒÙ… ØŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ú©ÛŒ Ú©Ø³ ØµÙ†Ù Ø³Û’ Ù…...    easy      multihop\n",
      "13  Ø±ÛŒØ§Ø³ØªÛØ§Ø¦Û’ Ù…ØªØ­Ø¯Û Ø§Ù…Ø±ÛŒÚ©Û Ú©Û’ ÛÙˆØ§Ø¦ÛŒ Ø§ÚˆÛ’ Ù¾Ø± ØŒ ÛŒÙˆÙ†Ø§Ø¦...    easy      multihop\n",
      "14  Ù†ÛŒÚ©ÙˆÙ„ Ù…Ú†Ù„ Ø§ÛŒÚ© Ø§Ù…Ø±ÛŒÚ©ÛŒ Ù…ÙˆØ³Ù…ÛŒØ§ØªÛŒ Ù…Ø§ÛØ± ÛÛŒÚº Ø¬Ùˆ Ù¾ÛÙ„ÛŒ...    easy      multihop\n",
      "15  \"Ù¾Ø³ÛŒÙˆÚˆÙˆÙ¹Ø³ÙˆÚ¯Ø§ ØŒ Ø¬Ùˆ Ø´Ù…Ø§Ù„ÛŒ Ø§Ù…Ø±ÛŒÚ©Û Ø§ÙˆØ± Ù…Ø´Ø±Ù‚ÛŒ Ø§ÛŒØ´ÛŒØ§...    easy      multihop\n",
      "16  Ø¬ÙˆØ¢Ù† Ø³ÛŒØ¨Ø³Ù¹ÛŒÙ† Ú©ÛŒØ¨Ù„ Ø§ÙˆØ± Ø¬Ø§Ù† Ù„Ø§Ø¦ÛŒÚˆ Ú©ÙˆÙ† Ø³Ø§ Ú©Ú¾ÛŒÙ„ Ú©Ú¾...    easy      multihop\n",
      "17  Ú©ÛŒÙ… Ù„ÛŒÙ¾ Ú©ÙˆÙ† Ø³Ø§ Ø¬Ù†ÙˆØ¨ÛŒ Ú©ÙˆØ±ÛŒØ§Ø¦ÛŒ Ú¯Ø±Ù„ Ú¯Ø±ÙˆÙ¾ Ú©Ø§ Ø§ÛŒÚ© Ø§...    easy      multihop\n",
      "18  Ø¨Ù„ÛŒÚ© Ø§Ø³Ù¹ÙˆÙ† Ú†ÛŒØ±ÛŒ Ø§ÙˆØ± Ø¬ÛŒÙ† Ù„ÛŒÙˆØ² Ø§ÛŒØ²Ø¨Ù„ Ø¯ÙˆÙ†ÙˆÚº Ø¨Ø±Ø·Ø§Ù†...  medium     singlehop\n",
      "19  Ù†ÛŒÙˆØ²ÛŒ Ù„ÛŒÙ†Úˆ Ú©ÛŒ Ø¨Ø§Ø¯Ø´Ø§ÛØª Ú©ÛŒ Ø§ÛŒÚ© Ø§ÛŒØ³ÛŒ Ø±ÛÙ†Ù…Ø§ ÛÛ’ Ø¬Ø³ ...  medium     singlehop\n",
      "32  \"Ø§Ø³ ÙˆÛŒØ¨ Ø³Ø§Ø¦Ù¹ Ú©Ø§ Ù†Ø§Ù… \"\"Ù¾ÛŒÙ†ÛŒÙ„ÙˆÙ¾ Ù¾Ø±Ù†Ø³Ø³ Ø¢Ù Ù¾ÛŒÙ¹Ø³\"\" ...  medium     singlehop\n",
      "40  \"Ø¢Ø¦Ø±Ù„ÛŒÙ†Úˆ Ú©Û’ Ù‚Ø§Ù†ÙˆÙ† Ø³Ø§Ø²ÙˆÚº Ù†Û’ \"\"Ø¢Ø¦Ø±Ù„ÛŒÙ†Úˆ Ú©ÙˆÙ†Ø³Ù„\"\" Ú©...  medium     singlehop\n",
      "44  Ù„ÛŒÚ© ÚˆØ³Ù¹Ø±Ú©Ù¹ Ù†ÛŒØ´Ù†Ù„ Ù¾Ø§Ø±Ú© Ù…ÛŒÚº Ù…Ø®ØµÙˆØµ Ú¯Ø±Ù†Û’ Ú©ÛŒ Ø§ÙˆÙ†Ú†Ø§Ø¦...  medium     singlehop\n",
      "47  ÙØ±Ø§Ù†Ø³ÛŒØ³ÛŒ ÙÙ¹ Ø¨Ø§Ù„Ø± Ø§ÙˆØ± ÙÙ¹ Ø¨Ø§Ù„ Ú©ÙˆÚ† Ù…Ø§Ø¦ÛŒÚ©Ù„ Ø³ÛŒÙˆÛŒÙ†Ø³Ù†...  medium     singlehop\n",
      "50  Ú©ÙˆÙ†Ø³ÛŒ Ú©Ø§Ù„Ø¬ Ù¹ÛŒÙ… Ø¨Ú¾Ø§Ø¦ÛŒÙˆÚº Ú©ÙˆÙ„Ù¹ Ø§ÙˆØ± Ú©ÛŒØ³ McCoy Ú©Û’ Ù„...    hard     singlehop\n",
      "55           Pandian Ø§ÙˆØ± Ø§Ù„Ù¾Ø§Ø¦Ù† Mastiff Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† ÙØ±Ù‚    hard     singlehop\n",
      "57         Ú©ÙˆÙ† Ø¨Ú‘Ø§ ÛÛ’ØŒ Ø±Ø§Ø¨Ø±Ù¹ Lindstedt ÛŒØ§ Ù¾ÛŒÙ¹ NorvalØŸ    hard     singlehop\n",
      "58  ÚˆÛŒÙ†ÛŒÙ„ Ù…ÛŒØ³ÛŒ Ù†Û’ 'Ø¯ÛŒ Ú©ÙˆØ¦ÛŒÙ†Ø² Ú¯Ø§Ø±ÚˆØ²' Ù…ÛŒÚº Ú©ÙˆÙ† Ø³Û’ Ø§Ø¯Ø§...    hard     singlehop\n",
      "61  Ø¨Ú¾Ø§Ø±Øª Ú©ÛŒ Ø§ÛŒÚ© Ø§ÛŒØ³ÛŒ Ø±ÛŒØ§Ø³Øª Ø¬Ø³ Ú©Ø§ Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ø¨Ú¾Ø§Ù†Úˆ...    hard     singlehop\n",
      "63  Ù„ÛŒÚ© ÙˆÙˆÚˆ Ø±Ù†Ú† Ø§ÛŒÚ© Ù…Ø§Ø³Ù¹Ø± Ù¾Ù„Ø§Ù†Úˆ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº ...    hard     singlehop\n",
      "73    ÙÙ„Ù… Ø³ÛŒÚ©Ø³ ÚˆØ±Ø§Ø¦ÛŒÙˆ Ù…ÛŒÚº Ø§Ù…Ø§Ù†ÚˆØ§ Ú©Ø±Ùˆ Ú©Ø§ Ú©Ø±Ø¯Ø§Ø± Ú©ÛŒØ§ ÛÛ’ØŸ    hard     singlehop\n",
      "74  Ø§Ø³ Ù†Û’ 1889 Ù…ÛŒÚº Ø¨Ø±Ù…Ù†Ú¯Ú¾Ù… ØŒ Ø§Ù†Ú¯Ù„ÛŒÙ†Úˆ Ù…ÛŒÚº 2010 Ø§Û’ Ø§...    hard     singlehop\n",
      "75  Ù…Ø´ÛŒ Ú¯Ù† Ú©Û’ Ø³Ø§Ø¨Ù‚ Ú¯ÙˆØ±Ù†Ø± ÛØ§ÙˆØ±Úˆ Ø§ÛŒÚˆÙ„Ø³Ù† Ù†Û’ Ø§Ù¾Ù†Û’ Ø§Ù†ØªØ®...    hard     singlehop\n",
      "81  Ú©ÙˆÙ† Ø³ÛŒ ÙÙ„Ù… Ú©Ø§ Ù¾Ø±ÛŒÙ…ÛŒØ¦Ø± Ù¾ÛÙ„Û’ ÛÙˆØ§ØŒ Ù¾ÙˆÚ©ÛŒÙ…ÙˆÙ†: Ø¢Ø±Ø³ÛŒØ¦...    hard     singlehop\n",
      "84            Ù…Ø³Ø¬Ø¯ Ø§Ù„Ø­Ø³Ù† Ú©Ø§ Ù‚ÛŒØ§Ù… Ú©Ø³ Ø®Ù„ÛŒÙÛ Ù†Û’ Ú©ÛŒØ§ ØªÚ¾Ø§ØŸ    hard     singlehop\n",
      "97   Ú©ÙˆÙ† Ø³Ø§ Ú©Ú¾Ù„Ø§Ú‘ÛŒ 36 ÙˆÛŒÚº Ù¹ÙˆÙ„ÙˆÙ† Ù¹ÙˆØ±Ù†Ø§Ù…Ù†Ù¹ Ø¬ÛŒØª Ø³Ú©ØªØ§ ÛÛ’ØŸ    hard     singlehop\n"
     ]
    }
   ],
   "source": [
    "print(df_temp[df_temp['correct_classification'] == False][['translated_question', 'level', 'question_type']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1646b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2      True\n",
       "3      True\n",
       "4     False\n",
       "      ...  \n",
       "93    False\n",
       "94    False\n",
       "95    False\n",
       "96     True\n",
       "97    False\n",
       "Name: correct_classification, Length: 98, dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['correct_classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d8dc8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             _multihop\n",
       "1     Question: What is the best title for this sent...\n",
       "2                                             singlehop\n",
       "3                                             singlehop\n",
       "4                                not enough information\n",
       "                            ...                        \n",
       "93                                               simple\n",
       "94                                               simple\n",
       "95                                            singlehop\n",
       "96                                             multihop\n",
       "97                                            singlehop\n",
       "Name: question_type, Length: 98, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['question_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76a145db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\hammad workings\\Thesis\\Multihop for Urdu\\Multihop for Urdu\\code\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\23030013\\.cache\\huggingface\\hub\\models--bigscience--bloomz-3b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-3b\"  # or bloom-1b7/bloom-3b\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afdd8fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Break the following Urdu question into 2 logical sub-questions needed to answer it:\n",
      "Question: Ø§Ú¯Ø± Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ø²ÛŒØ§Ø¯Û ÛÛ’ Ø§ÙˆØ± ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù†Ù…ÙˆÙ†ÛŒØ§ Ú©Ø§ Ø³Ø¨Ø¨ Ø¨Ù† Ø³Ú©ØªÛŒ ÛÛ’ØŒ ØªÙˆ Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº Ø±ÛÙ†Û’ ÙˆØ§Ù„ÙˆÚº Ú©Ùˆ Ù†Ù…ÙˆÙ†ÛŒØ§ Ø³Û’ Ø¨Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ§ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "* Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ø²ÛŒØ§Ø¯Û ÛÛ’\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "Break the following Urdu question into 2 logical sub-questions needed to answer it:\n",
    "Question: Ø§Ú¯Ø± Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ø²ÛŒØ§Ø¯Û ÛÛ’ Ø§ÙˆØ± ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù†Ù…ÙˆÙ†ÛŒØ§ Ú©Ø§ Ø³Ø¨Ø¨ Ø¨Ù† Ø³Ú©ØªÛŒ ÛÛ’ØŒ ØªÙˆ Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº Ø±ÛÙ†Û’ ÙˆØ§Ù„ÙˆÚº Ú©Ùˆ Ù†Ù…ÙˆÙ†ÛŒØ§ Ø³Û’ Ø¨Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©ÛŒØ§ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=300)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1616b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull('mistral:latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e7effd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def decompose_urdu_query(urdu_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "**Role**: You are an expert Urdu linguistic analyst specializing in question decomposition. Your task is to break down complex Urdu questions into their fundamental components.\n",
    "\n",
    "**Task Instructions**:\n",
    "1. Carefully analyze the given Urdu question to identify its core components\n",
    "2. Extract exactly 2 sub-questions that:\n",
    "   - Are necessary to answer the main question\n",
    "   - Cover distinct aspects of the problem\n",
    "   - Have clear logical progression (answer to q1 helps answer q2)\n",
    "3. Both sub-questions must:\n",
    "   - Be in proper Urdu language\n",
    "   - Be grammatically correct\n",
    "   - Be clear and concise\n",
    "   - Use relevant domain terminology\n",
    "\n",
    "**Output Format Requirements**:\n",
    "- Use EXACTLY this format:\n",
    "  q1: [Ù¾ÛÙ„Ø§ Ø°ÛŒÙ„ÛŒ Ø³ÙˆØ§Ù„]\n",
    "  q2: [Ø¯ÙˆØ³Ø±Ø§ Ø°ÛŒÙ„ÛŒ Ø³ÙˆØ§Ù„]\n",
    "- Each sub-question must be on a new line\n",
    "- Do not include any additional commentary or explanation\n",
    "- Do not number the questions (use only q1:/q2: prefixes)\n",
    "\n",
    "**Example 1**:\n",
    "Input: Ø§Ú¯Ø± Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ø¯ÛÙ„ÛŒ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÛ’ Ø§ÙˆØ± ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù¾Ú¾ÛŒÙ¾Ú¾Ú‘ÙˆÚº Ú©Û’ Ú©ÛŒÙ†Ø³Ø± Ú©Ø§ Ø³Ø¨Ø¨ Ø¨Ù† Ø³Ú©ØªÛŒ ÛÛ’ØŒ ØªÙˆ Ù„Ø§ÛÙˆØ± Ú©Û’ Ø±ÛØ§Ø¦Ø´ÛŒÙˆÚº Ú©Ùˆ Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø·Ø¨ÛŒ Ú†ÛŒÚ© Ø§Ù¾ Ú©Ø±ÙˆØ§Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
    "Output:\n",
    "q1: Ù„Ø§ÛÙˆØ± Ø§ÙˆØ± Ø¯ÛÙ„ÛŒ Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "q2: ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù¾Ú¾ÛŒÙ¾Ú¾Ú‘ÙˆÚº Ú©Û’ Ú©ÛŒÙ†Ø³Ø± Ú©Ø§ Ø³Ø¨Ø¨ Ú©ÛŒØ³Û’ Ø¨Ù†ØªÛŒ ÛÛ’ØŸ\n",
    "\n",
    "**Example 2**:\n",
    "Input: Ø§Ú¯Ø± Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ù†Ø±Ø® 30% Ø¨Ú‘Ú¾ Ú¯Ø¦Û’ ÛÛŒÚº Ø§ÙˆØ± ÛŒÛ ØµÙ†Ø¹ØªÙˆÚº Ú©Ùˆ Ù…ØªØ§Ø«Ø± Ú©Ø± Ø±ÛØ§ ÛÛ’ØŒ ØªÙˆ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø³Ø¨Ø³ÚˆÛŒØ§Úº Ø¯ÛŒÙ†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
    "Output:\n",
    "q1: Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ù†Ø±Ø®ÙˆÚº Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "q2: Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ù…ÛÙ†Ú¯Û’ ÛÙˆÙ†Û’ Ø³Û’ ØµÙ†Ø¹ØªÙˆÚº Ù¾Ø± Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…Ø±ØªØ¨ ÛÙˆ Ø±ÛÛ’ ÛÛŒÚºØŸ\n",
    "\n",
    "**Current Task**:\n",
    "Input: {urdu_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='mistral',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = response['response'].strip()\n",
    "        \n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "        \n",
    "        return result if len(result) == 2 else {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fbc45220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1: Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø¬Ø§Ù… Ú©ÛŒ Ø´Ú©Ø§ÛŒØ§Øª Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ù†Ù‚Ù„ Ùˆ Ø­Ù…Ù„ Ù¾Ø§Ù„ÛŒØ³ÛŒÙˆÚº Ú©Û’ ÙˆØ¬Û Ø³Û’ Ù¹Ø±ÛŒÙÚ© Ø¬Ø§Ù… Ú©ÛŒ Ø´Ú©Ø§ÛŒØª Ú©ÛŒ Ø¨Ú‘Ú¾Ù†Û’ Ø³Û’ Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…Ø±ØªØ¨ ÛÙˆ Ø±ÛÛ’ ÛÛŒÚºØŸ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Correct usage:\n",
    "your_query = \"Ø§Ú¯Ø± Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø¬Ø§Ù… Ú©ÛŒ Ø´Ú©Ø§ÛŒØ§Øª 50% Ø¨Ú‘Ú¾ Ú¯Ø¦ÛŒ ÛÛŒÚº Ø§ÙˆØ± ÛŒÛ Ø´ÛØ± Ú©ÛŒ Ù†Ù‚Ù„ Ùˆ Ø­Ù…Ù„ Ú©ÛŒ Ù†Ø§Ú©Ø§ÙÛŒ Ù¾Ø§Ù„ÛŒØ³ÛŒÙˆÚº Ú©ÛŒ ÙˆØ¬Û Ø³Û’ ÛÛ’ØŒ ØªÙˆ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ Ù†Ø¦Û’ Ù¹Ø±ÛŒÙÚ© Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…ØªØ¹Ø§Ø±Ù Ú©Ø±ÙˆØ§Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "result = decompose_urdu_query(your_query)\n",
    "if result:\n",
    "    print(f\"q1: {result.get('q1', 'Not generated')}\")\n",
    "    print(f\"q2: {result.get('q2', 'Not generated')}\")\n",
    "else:\n",
    "    print(\"Failed to decompose the query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a57f638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environmental Policy\n",
    "your_query_1 = \"Ø§Ú¯Ø± Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù…ÛŒÚº Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø´Ø±Ø­ 40% Ø¨Ú‘Ú¾ Ú¯Ø¦ÛŒ ÛÛ’ Ø§ÙˆØ± ÛŒÛ Ø²Ø±Ø§Ø¹Øª Ú©Ùˆ Ù…ØªØ§Ø«Ø± Ú©Ø± Ø±ÛÛŒ ÛÛ’ØŒ ØªÙˆ Ù¾Ù†Ø¬Ø§Ø¨ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ ÙÙˆØ±ÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "# 2. Public Health\n",
    "your_query_2 = \"Ø§Ú¯Ø± Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº ÚˆÛŒÙ†Ú¯ÛŒ Ú©Û’ Ú©ÛŒØ³Ø² Ù…ÛŒÚº 150% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ø¨Ø§Ø±Ø´ÙˆÚº Ú©Ø§ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù†Ù…ÙˆÙ†Û Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø´ÛØ±ÛŒ Ø§Ù†ØªØ¸Ø§Ù…ÛŒÛ Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø§Ø­ØªÛŒØ§Ø·ÛŒ ØªØ¯Ø§Ø¨ÛŒØ± Ø§Ø®ØªÛŒØ§Ø± Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "# 3. Education Reform\n",
    "your_query_3 = \"Ø§Ú¯Ø± Ù¾Ù†Ø¬Ø§Ø¨ Ú©Û’ Ø³Ø±Ú©Ø§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ÙˆÚº Ù…ÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¦ÛŒ Ú©Ø§ Ù…Ø¹ÛŒØ§Ø± 60% Ø·Ù„Ø¨Û Ú©Û’ Ù„ÛŒÛ’ Ù†Ø§Ú©Ø§ÙÛŒ ÛÛ’ Ø§ÙˆØ± Ø§Ø³Ø§ØªØ°Û Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ù…ÛŒÚº Ú©Ù…ÛŒ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ØªØ¹Ù„ÛŒÙ…ÛŒ Ø¨Ø¬Ù¹ Ù…ÛŒÚº Ú©Ø³ Ø´Ø¹Ø¨Û’ Ú©Ùˆ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒ Ø¬Ø§Ù†ÛŒ Ú†Ø§ÛÛŒÛ’ØŸ\"\n",
    "\n",
    "# 4. Economic Development\n",
    "your_query_4 = \"Ø§Ú¯Ø± Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª 25% Ú©Ù… ÛÙˆ Ú¯Ø¦ÛŒ ÛÛŒÚº Ø§ÙˆØ± Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª Ù¾Ø± Ù¾ÙˆØ±Ø§ Ù†Û Ø§ØªØ±Ù†Ø§ Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ÙˆØ²Ø§Ø±Øª ØªØ¬Ø§Ø±Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ù†Ø¦ÛŒ Ù¾Ø§Ù„ÛŒØ³ÛŒØ§Úº Ø¨Ù†Ø§Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "# 5. Urban Planning\n",
    "your_query_5 = \"Ø§Ú¯Ø± Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº 30% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ø³Ú‘Ú©ÙˆÚº Ú©ÛŒ Ù†Ø§Ù‚Øµ ÚˆÛŒØ²Ø§Ø¦Ù†Ù†Ú¯ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø³ÛŒ ÚˆÛŒ Ø§Û’ Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ ØªØ¹Ù…ÛŒØ±Ø§ØªÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "# 6. Agricultural Crisis\n",
    "your_query_6 = \"Ø§Ú¯Ø± Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ù…ÛŒÚº 20% Ú©Ù…ÛŒ ÙˆØ§Ù‚Ø¹ ÛÙˆØ¦ÛŒ ÛÛ’ Ø§ÙˆØ± Ú©Ø³Ø§Ù†ÙˆÚº Ú©Ùˆ Ø¬Ø¯ÛŒØ¯ Ø§Ø¯ÙˆÛŒØ§Øª ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ù†Û ÛÙˆÙ†Ø§ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø²Ø±Ø¹ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ø§Ø¯Ø§Ø±Û’ Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª ÙØ±Ø§ÛÙ… Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "# 7. Energy Sector\n",
    "your_query_7 = \"Ø§Ú¯Ø± Ù„ÙˆÚˆ Ø´ÛŒÚˆÙ†Ú¯ Ù…ÛŒÚº 45% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ø±Ø§Ù†Û’ Ø¬Ù†Ø±ÛŒÙ¹Ø±Ø² Ú©Ø§ Ù†Ø§Ú©Ø§Ø±Û ÛÙˆÙ†Ø§ Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ÙˆØ²Ø§Ø±Øª ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ Ù†Ø¦Û’ Ù…Ù†ØµÙˆØ¨Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\"\n",
    "\n",
    "# 8. Healthcare Access\n",
    "your_query_8 = \"Ø§Ú¯Ø± Ø¯ÛŒÛÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº 70% Ø¢Ø¨Ø§Ø¯ÛŒ Ú©Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØµØ­Øª Ú©ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª Ù…ÛŒØ³Ø± Ù†ÛÛŒÚº Ø§ÙˆØ± ÚˆØ§Ú©Ù¹Ø±ÙˆÚº Ú©ÛŒ Ú©Ù…ÛŒ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ØµÙˆØ¨Ø§Ø¦ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ ØªØ±Ø¨ÛŒØªÛŒ Ø§Ø³Ú©ÛŒÙ…ÛŒÚº Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21666cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Query 1:\n",
      "Ø§Ú¯Ø± Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù…ÛŒÚº Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø´Ø±Ø­ 40% Ø¨Ú‘Ú¾ Ú¯Ø¦ÛŒ ÛÛ’ Ø§ÙˆØ± ÛŒÛ Ø²Ø±Ø§Ø¹Øª Ú©Ùˆ Ù…ØªØ§Ø«Ø± Ú©Ø± Ø±ÛÛŒ ÛÛ’ØŒ ØªÙˆ Ù¾Ù†Ø¬Ø§Ø¨ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ ÙÙˆØ±ÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù…ÛŒÚº Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø´Ø±Ø­ 40% Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø²Ø±Ø§Ø¹Øª Ú©Ùˆ Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù…ÛŒÚº Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…ØªØ§Ø«Ø± Ú©Ø±Ù†Û’ Ø³Û’ Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…Ø±ØªØ¨ ÛÙˆ Ø±ÛÛŒ ÛÛŒÚºØŸ\n",
      "\n",
      "Testing Query 2:\n",
      "Ø§Ú¯Ø± Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº ÚˆÛŒÙ†Ú¯ÛŒ Ú©Û’ Ú©ÛŒØ³Ø² Ù…ÛŒÚº 150% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ø¨Ø§Ø±Ø´ÙˆÚº Ú©Ø§ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù†Ù…ÙˆÙ†Û Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø´ÛØ±ÛŒ Ø§Ù†ØªØ¸Ø§Ù…ÛŒÛ Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø§Ø­ØªÛŒØ§Ø·ÛŒ ØªØ¯Ø§Ø¨ÛŒØ± Ø§Ø®ØªÛŒØ§Ø± Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº ÚˆÛŒÙ†Ú¯ÛŒ Ú©Û’ Ú©ÛŒØ³Ø² Ù…ÛŒÚº 150% Ø§Ø¶Ø§ÙÛ Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø¨Ø§Ø±Ø´ÙˆÚº Ú©Ø§ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù†Ù…ÙˆÙ†Û Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û Ø³Û’ Ú©ÛŒØ³Û’ Ø­Ø§Ù„Øª Ù¾Ú¾Ø§Ú‘ØªÛŒ ÛÛ’ Ø´ÛØ±ÛŒ Ø§Ù†ØªØ¸Ø§Ù…ÛŒÛØŸ\n",
      "\n",
      "Testing Query 3:\n",
      "Ø§Ú¯Ø± Ù¾Ù†Ø¬Ø§Ø¨ Ú©Û’ Ø³Ø±Ú©Ø§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ÙˆÚº Ù…ÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¦ÛŒ Ú©Ø§ Ù…Ø¹ÛŒØ§Ø± 60% Ø·Ù„Ø¨Û Ú©Û’ Ù„ÛŒÛ’ Ù†Ø§Ú©Ø§ÙÛŒ ÛÛ’ Ø§ÙˆØ± Ø§Ø³Ø§ØªØ°Û Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ù…ÛŒÚº Ú©Ù…ÛŒ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ØªØ¹Ù„ÛŒÙ…ÛŒ Ø¨Ø¬Ù¹ Ù…ÛŒÚº Ú©Ø³ Ø´Ø¹Ø¨Û’ Ú©Ùˆ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒ Ø¬Ø§Ù†ÛŒ Ú†Ø§ÛÛŒÛ’ØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ù¾Ù†Ø¬Ø§Ø¨ Ú©Û’ Ø³Ø±Ú©Ø§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ÙˆÚº Ù…ÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¦ÛŒ Ú©Ø§ Ù…Ø¹ÛŒØ§Ø± 60% Ø·Ù„Ø¨Û Ú©Û’ Ù„Ø¦Û’ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ø²Ø¯Û Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ù…ÛŒÚº Ú©Ù…ÛŒ Ú©Ùˆ Ú©Ø³ Ø´Ø¹Ø¨Û’ Ú©Û’ ÙˆØ¬Û Ø³Û’ ÛÛ’ØŸ\n",
      "\n",
      "Testing Query 4:\n",
      "Ø§Ú¯Ø± Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª 25% Ú©Ù… ÛÙˆ Ú¯Ø¦ÛŒ ÛÛŒÚº Ø§ÙˆØ± Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª Ù¾Ø± Ù¾ÙˆØ±Ø§ Ù†Û Ø§ØªØ±Ù†Ø§ Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ÙˆØ²Ø§Ø±Øª ØªØ¬Ø§Ø±Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ù†Ø¦ÛŒ Ù¾Ø§Ù„ÛŒØ³ÛŒØ§Úº Ø¨Ù†Ø§Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª Ú©ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ú©ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù¾Ø± 25% Ú©Ù… ÛÙˆØ¦ÛŒ ÛÛ’ ÛŒØ§ Ù†ÛÛŒÚºØŸ\n",
      "q2: Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª Ù¾Ø± Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª Ù…ÛŒÚº Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª ÛÛŒÚºØŸ\n",
      "\n",
      "Testing Query 5:\n",
      "Ø§Ú¯Ø± Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº 30% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ø³Ú‘Ú©ÙˆÚº Ú©ÛŒ Ù†Ø§Ù‚Øµ ÚˆÛŒØ²Ø§Ø¦Ù†Ù†Ú¯ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø³ÛŒ ÚˆÛŒ Ø§Û’ Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ ØªØ¹Ù…ÛŒØ±Ø§ØªÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº 30% Ø§Ø¶Ø§ÙÛ Ø´ÛÙˆØ± Ú©Û’ Ù¾Ø§Ù†Ú†Ú¾Û’ Ù‚Ø³Ù… Ú©Û’ Ø·Ø¨Ù‚Û’ Ú©ÛŒØ§ Ú¯Ø¦Û’ ÛÛŒÚºØŸ\n",
      "q2: Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº 30% Ø§Ø¶Ø§ÙÛ Ú©Û’ Ø¹Ù„Ø§ÙˆÛØŒ Ø³Ú‘Ú©ÙˆÚº Ú©ÛŒ Ù†Ø§Ù‚Øµ ÚˆÛŒØ²Ø§Ø¦Ù†Ù†Ú¯ Ú©ÛŒ ÙˆØ¬Û Ø³Û’ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Ú†Ú¾ Ú©Ø§Ù… Ú©ÛŒ Ø¬Ø§Ù†Û’ Ú©Û’ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø± Ù…Ø±ØªØ¨ ÛÛŒÚºØŸ\n",
      "\n",
      "Testing Query 6:\n",
      "Ø§Ú¯Ø± Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ù…ÛŒÚº 20% Ú©Ù…ÛŒ ÙˆØ§Ù‚Ø¹ ÛÙˆØ¦ÛŒ ÛÛ’ Ø§ÙˆØ± Ú©Ø³Ø§Ù†ÙˆÚº Ú©Ùˆ Ø¬Ø¯ÛŒØ¯ Ø§Ø¯ÙˆÛŒØ§Øª ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ù†Û ÛÙˆÙ†Ø§ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø²Ø±Ø¹ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ø§Ø¯Ø§Ø±Û’ Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª ÙØ±Ø§ÛÙ… Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ú¯Ù†Ø¯Ù… Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ù…ÛŒÚº 20% Ú©Ù…ÛŒ ÙˆØ§Ù‚Ø¹ ÛÙˆØ¦ÛŒ ÛÛ’ Ú©ÛŒØ§ Ù…Ø¹Ù„ÙˆÙ… ÛÛ’ØŸ\n",
      "q2: Ø¬Ø¯ÛŒØ¯ Ø§Ø¯ÙˆÛŒØ§Øª ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ù†Û ÛÙˆÙ†Û’ Ú©ÛŒ ÙˆØ¬Û Ø³Û’ Ø²Ø±Ø¹ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ø§Ø¯Ø§Ø±Û’ Ù¾Ø± Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…Ø±ØªØ¨ ÛÙˆØ¦ÛŒ ÛÛŒÚºØŸ\n",
      "\n",
      "Testing Query 7:\n",
      "Ø§Ú¯Ø± Ù„ÙˆÚˆ Ø´ÛŒÚˆÙ†Ú¯ Ù…ÛŒÚº 45% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ø±Ø§Ù†Û’ Ø¬Ù†Ø±ÛŒÙ¹Ø±Ø² Ú©Ø§ Ù†Ø§Ú©Ø§Ø±Û ÛÙˆÙ†Ø§ Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ÙˆØ²Ø§Ø±Øª ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ Ù†Ø¦Û’ Ù…Ù†ØµÙˆØ¨Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ù„ÙˆÚˆ Ø´ÛŒÚˆÙ†Ú¯ Ù…ÛŒÚº 45% Ø§Ø¶Ø§ÙÛ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ù¾Ø±Ø§Ù†Û’ Ø¬Ù†Ø±ÛŒÙ¹Ø±Ø² Ú©Ø§ Ù†Ø§Ú©Ø§Ø±Û ÛÙˆÙ†Û’ Ø³Û’ ØªØ¹Ù„Ù‚ Ø±Ú©Ú¾Ù†Û’ ÙˆØ§Ù„Û’ Ù†Ø¦Û’ Ù…Ù†ØµÙˆØ¨Û’ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û Ú©ÛŒØ³Û’ ÛÙˆØªÛŒ ÛÛŒÚºØŸ\n",
      "\n",
      "Testing Query 8:\n",
      "Ø§Ú¯Ø± Ø¯ÛŒÛÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº 70% Ø¢Ø¨Ø§Ø¯ÛŒ Ú©Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØµØ­Øª Ú©ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª Ù…ÛŒØ³Ø± Ù†ÛÛŒÚº Ø§ÙˆØ± ÚˆØ§Ú©Ù¹Ø±ÙˆÚº Ú©ÛŒ Ú©Ù…ÛŒ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ØµÙˆØ¨Ø§Ø¦ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ ØªØ±Ø¨ÛŒØªÛŒ Ø§Ø³Ú©ÛŒÙ…ÛŒÚº Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ø¯ÛŒÛÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº 70% Ø¢Ø¨Ø§Ø¯ÛŒ Ú©Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØµØ­Øª Ú©ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª Ù…ÛŒØ³Ø± ÛÛŒÚº ÛŒØ§ Ù†ÛÛŒÚºØŸ\n",
      "q2: ÚˆØ§Ú©Ù¹Ø±ÙˆÚº Ú©ÛŒ Ú©Ù…ÛŒ Ú©Ùˆ Ø¢Ø¨Ø§Ø¯ÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ ÙˆØ¬Û Ø¯ÛŒØªÛŒ ÛÛ’ØŸ\n"
     ]
    }
   ],
   "source": [
    "queries = [your_query_1, your_query_2, your_query_3, your_query_4, your_query_5, your_query_6, your_query_7, your_query_8]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nTesting Query {i}:\")\n",
    "    print(query)\n",
    "    result = decompose_urdu_query(query)\n",
    "    print(\"\\nDecomposition:\")\n",
    "    print(f\"q1: {result.get('q1', 'N/A')}\")\n",
    "    print(f\"q2: {result.get('q2', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6faa4824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.pull('llama3:8b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2eb6bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def decompose_urdu_query(urdu_query: str) -> dict:\n",
    "    \"\"\"Returns dictionary with q1 and q2 keys containing sub-questions\"\"\"\n",
    "    refined_prompt = f\"\"\"\n",
    "**Role**: You are an expert Urdu linguistic analyst specializing in question decomposition. Your task is to break down complex Urdu questions into their fundamental components.\n",
    "\n",
    "**Task Instructions**:\n",
    "1. Carefully analyze the given Urdu question to identify its core components\n",
    "2. Extract exactly 2 sub-questions that:\n",
    "   - Are necessary to answer the main question\n",
    "   - Cover distinct aspects of the problem\n",
    "   - Have clear logical progression (answer to q1 helps answer q2)\n",
    "3. Both sub-questions must:\n",
    "   - Be in proper Urdu language\n",
    "   - Be grammatically correct\n",
    "   - Be clear and concise\n",
    "   - Use relevant domain terminology\n",
    "\n",
    "**Output Format Requirements**:\n",
    "- Use EXACTLY this format:\n",
    "  q1: [Ù¾ÛÙ„Ø§ Ø°ÛŒÙ„ÛŒ Ø³ÙˆØ§Ù„]\n",
    "  q2: [Ø¯ÙˆØ³Ø±Ø§ Ø°ÛŒÙ„ÛŒ Ø³ÙˆØ§Ù„]\n",
    "- Each sub-question must be on a new line\n",
    "- Do not include any additional commentary or explanation\n",
    "- Do not number the questions (use only q1:/q2: prefixes)\n",
    "\n",
    "**Example 1**:\n",
    "Input: Ø§Ú¯Ø± Ù„Ø§ÛÙˆØ± Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ø¯ÛÙ„ÛŒ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÛÛ’ Ø§ÙˆØ± ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù¾Ú¾ÛŒÙ¾Ú¾Ú‘ÙˆÚº Ú©Û’ Ú©ÛŒÙ†Ø³Ø± Ú©Ø§ Ø³Ø¨Ø¨ Ø¨Ù† Ø³Ú©ØªÛŒ ÛÛ’ØŒ ØªÙˆ Ù„Ø§ÛÙˆØ± Ú©Û’ Ø±ÛØ§Ø¦Ø´ÛŒÙˆÚº Ú©Ùˆ Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø·Ø¨ÛŒ Ú†ÛŒÚ© Ø§Ù¾ Ú©Ø±ÙˆØ§Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
    "Output:\n",
    "q1: Ù„Ø§ÛÙˆØ± Ø§ÙˆØ± Ø¯ÛÙ„ÛŒ Ù…ÛŒÚº ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø³Ø·Ø­ Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "q2: ÙØ¶Ø§Ø¦ÛŒ Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù¾Ú¾ÛŒÙ¾Ú¾Ú‘ÙˆÚº Ú©Û’ Ú©ÛŒÙ†Ø³Ø± Ú©Ø§ Ø³Ø¨Ø¨ Ú©ÛŒØ³Û’ Ø¨Ù†ØªÛŒ ÛÛ’ØŸ\n",
    "\n",
    "**Example 2**:\n",
    "Input: Ø§Ú¯Ø± Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ù†Ø±Ø® 30% Ø¨Ú‘Ú¾ Ú¯Ø¦Û’ ÛÛŒÚº Ø§ÙˆØ± ÛŒÛ ØµÙ†Ø¹ØªÙˆÚº Ú©Ùˆ Ù…ØªØ§Ø«Ø± Ú©Ø± Ø±ÛØ§ ÛÛ’ØŒ ØªÙˆ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø³Ø¨Ø³ÚˆÛŒØ§Úº Ø¯ÛŒÙ†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
    "Output:\n",
    "q1: Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ù†Ø±Ø®ÙˆÚº Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
    "q2: Ø¨Ø¬Ù„ÛŒ Ú©Û’ Ù…ÛÙ†Ú¯Û’ ÛÙˆÙ†Û’ Ø³Û’ ØµÙ†Ø¹ØªÙˆÚº Ù¾Ø± Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…Ø±ØªØ¨ ÛÙˆ Ø±ÛÛ’ ÛÛŒÚºØŸ\n",
    "\n",
    "**Current Task**:\n",
    "Input: {urdu_query}\n",
    "Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model='llama3:8b',\n",
    "            prompt=refined_prompt,\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_ctx': 2048\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        output = response['response'].strip()\n",
    "        \n",
    "        result = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('q1:'):\n",
    "                result['q1'] = line[3:].strip()\n",
    "            elif line.startswith('q2:'):\n",
    "                result['q2'] = line[3:].strip()\n",
    "        \n",
    "        return result if len(result) == 2 else {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Decomposition error: {str(e)}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6d938ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Query 1:\n",
      "Ø§Ú¯Ø± Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù…ÛŒÚº Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø´Ø±Ø­ 40% Ø¨Ú‘Ú¾ Ú¯Ø¦ÛŒ ÛÛ’ Ø§ÙˆØ± ÛŒÛ Ø²Ø±Ø§Ø¹Øª Ú©Ùˆ Ù…ØªØ§Ø«Ø± Ú©Ø± Ø±ÛÛŒ ÛÛ’ØŒ ØªÙˆ Ù¾Ù†Ø¬Ø§Ø¨ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ ÙÙˆØ±ÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù…ÛŒÚº Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ú©ÛŒ Ø´Ø±Ø­ Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ø³Û’ Ø²Ø±Ø§Ø¹Øª Ú©Ùˆ Ù…ØªØ§Ø«Ø± Ú©Ø± Ø±ÛÛŒ ÛÛ’ØŒ ØªÙˆ Ù¾Ù†Ø¬Ø§Ø¨ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ ÙÙ„Ø§Ø­ÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ù„ÛŒÙ†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Testing Query 2:\n",
      "Ø§Ú¯Ø± Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº ÚˆÛŒÙ†Ú¯ÛŒ Ú©Û’ Ú©ÛŒØ³Ø² Ù…ÛŒÚº 150% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ø¨Ø§Ø±Ø´ÙˆÚº Ú©Ø§ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù†Ù…ÙˆÙ†Û Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø´ÛØ±ÛŒ Ø§Ù†ØªØ¸Ø§Ù…ÛŒÛ Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø§Ø­ØªÛŒØ§Ø·ÛŒ ØªØ¯Ø§Ø¨ÛŒØ± Ø§Ø®ØªÛŒØ§Ø± Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº ÚˆÛŒÙ†Ú¯ÛŒ Ú©Û’ Ú©ÛŒØ³Ø² Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø¨Ø§Ø±Ø´ÙˆÚº Ú©Ø§ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù†Ù…ÙˆÙ†Û ÚˆÛŒÙ†Ú¯ÛŒ Ú©Û’ Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û Ú©ÛŒØ³Û’ Ø¨Ù†ØªÛŒ ÛÛ’ØŸ\n",
      "\n",
      "Testing Query 3:\n",
      "Ø§Ú¯Ø± Ù¾Ù†Ø¬Ø§Ø¨ Ú©Û’ Ø³Ø±Ú©Ø§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ÙˆÚº Ù…ÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¦ÛŒ Ú©Ø§ Ù…Ø¹ÛŒØ§Ø± 60% Ø·Ù„Ø¨Û Ú©Û’ Ù„ÛŒÛ’ Ù†Ø§Ú©Ø§ÙÛŒ ÛÛ’ Ø§ÙˆØ± Ø§Ø³Ø§ØªØ°Û Ú©ÛŒ ØªØ±Ø¨ÛŒØª Ù…ÛŒÚº Ú©Ù…ÛŒ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ØªØ¹Ù„ÛŒÙ…ÛŒ Ø¨Ø¬Ù¹ Ù…ÛŒÚº Ú©Ø³ Ø´Ø¹Ø¨Û’ Ú©Ùˆ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒ Ø¬Ø§Ù†ÛŒ Ú†Ø§ÛÛŒÛ’ØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ù¾Ù†Ø¬Ø§Ø¨ Ú©Û’ Ø³Ø±Ú©Ø§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ÙˆÚº Ù…ÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¦ÛŒ Ú©Ø§ Ù…Ø¹ÛŒØ§Ø± Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: ØªØ¹Ù„ÛŒÙ…ÛŒ Ø¨Ø¬Ù¹ Ù…ÛŒÚº ØªØ±Ø¨ÛŒØª Ú©ÛŒ Ú©Ù…ÛŒ Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Ø§Ø«Ø±Ø§Øª Ù…Ø±ØªØ¨ ÛÙˆ Ø±ÛÛ’ ÛÛŒÚºØŸ\n",
      "\n",
      "Testing Query 4:\n",
      "Ø§Ú¯Ø± Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª 25% Ú©Ù… ÛÙˆ Ú¯Ø¦ÛŒ ÛÛŒÚº Ø§ÙˆØ± Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª Ù¾Ø± Ù¾ÙˆØ±Ø§ Ù†Û Ø§ØªØ±Ù†Ø§ Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ÙˆØ²Ø§Ø±Øª ØªØ¬Ø§Ø±Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ù†Ø¦ÛŒ Ù¾Ø§Ù„ÛŒØ³ÛŒØ§Úº Ø¨Ù†Ø§Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª Ù¾Ø± Ù¾ÙˆØ±Ø§ Ù†Û Ø§ØªØ±Ù†Ø§ Ø³Û’ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ø¨Ø±Ø¢Ù…Ø¯Ø§Øª Ù…ÛŒÚº Ú©Ù…ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û Ú©ÛŒØ³Û’ ÛÛ’ØŸ\n",
      "\n",
      "Testing Query 5:\n",
      "Ø§Ú¯Ø± Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº 30% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ø³Ú‘Ú©ÙˆÚº Ú©ÛŒ Ù†Ø§Ù‚Øµ ÚˆÛŒØ²Ø§Ø¦Ù†Ù†Ú¯ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø³ÛŒ ÚˆÛŒ Ø§Û’ Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ ØªØ¹Ù…ÛŒØ±Ø§ØªÛŒ Ù…Ø¹ÛŒØ§Ø±Ø§Øª ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ù…ÛŒÚº Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ø³Ú‘Ú©ÙˆÚº Ú©ÛŒ Ù†Ø§Ù‚Øµ ÚˆÛŒØ²Ø§Ø¦Ù†Ù†Ú¯ Ù¹Ø±ÛŒÙÚ© Ø­Ø§Ø¯Ø«Ø§Øª Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ ÙˆØ¬Û Ú©ÛŒØ³Û’ Ø¨Ù†ØªÛŒ ÛÛ’ØŸ\n",
      "\n",
      "Testing Query 6:\n",
      "Ø§Ú¯Ø± Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ù…ÛŒÚº 20% Ú©Ù…ÛŒ ÙˆØ§Ù‚Ø¹ ÛÙˆØ¦ÛŒ ÛÛ’ Ø§ÙˆØ± Ú©Ø³Ø§Ù†ÙˆÚº Ú©Ùˆ Ø¬Ø¯ÛŒØ¯ Ø§Ø¯ÙˆÛŒØ§Øª ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ù†Û ÛÙˆÙ†Ø§ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ Ø²Ø±Ø¹ÛŒ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ Ø§Ø¯Ø§Ø±Û’ Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª ÙØ±Ø§ÛÙ… Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ù…ÛŒÚº 20% Ú©Ù…ÛŒ ÙˆØ§Ù‚Ø¹ ÛÙˆØ¦ÛŒ ÛÛ’ØŒ ØªÙˆ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ú©Ø³Ø§Ù†ÙˆÚº Ú©Ùˆ Ø¬Ø¯ÛŒØ¯ Ø§Ø¯ÙˆÛŒØ§Øª ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ù†Û ÛÙˆÙ†Ø§ Ú¯Ù†Ø¯Ù… Ú©ÛŒ Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ù…ÛŒÚº Ú©Ù…ÛŒ Ú©ÛŒ ÙˆØ¬Û Ú©ÛŒØ³Û’ Ø¨Ù†ØªÛŒ ÛÛ’ØŸ\n",
      "\n",
      "Testing Query 7:\n",
      "Ø§Ú¯Ø± Ù„ÙˆÚˆ Ø´ÛŒÚˆÙ†Ú¯ Ù…ÛŒÚº 45% Ø§Ø¶Ø§ÙÛ ÛÙˆØ§ ÛÛ’ Ø§ÙˆØ± Ù¾Ø±Ø§Ù†Û’ Ø¬Ù†Ø±ÛŒÙ¹Ø±Ø² Ú©Ø§ Ù†Ø§Ú©Ø§Ø±Û ÛÙˆÙ†Ø§ Ø§Ø³ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ÙˆØ²Ø§Ø±Øª ØªÙˆØ§Ù†Ø§Ø¦ÛŒ Ú©Ùˆ Ú©ÙˆÙ† Ø³Û’ Ù†Ø¦Û’ Ù…Ù†ØµÙˆØ¨Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†Û’ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ù„ÙˆÚˆ Ø´ÛŒÚˆÙ†Ú¯ Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯Û Ø´Ø±Ø­ Ú©ÛŒØ§ ÛÛ’ØŸ\n",
      "q2: Ù¾Ø±Ø§Ù†Û’ Ø¬Ù†Ø±ÛŒÙ¹Ø±Ø² Ú©Ø§ Ù†Ø§Ú©Ø§Ø±Û ÛÙˆÙ†Ø§ Ù„ÙˆÚˆ Ø´ÛŒÚˆÙ†Ú¯ Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ’ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬Û Ú©ÛŒØ³Û’ ÛÛ’ØŸ\n",
      "\n",
      "Testing Query 8:\n",
      "Ø§Ú¯Ø± Ø¯ÛŒÛÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº 70% Ø¢Ø¨Ø§Ø¯ÛŒ Ú©Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØµØ­Øª Ú©ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª Ù…ÛŒØ³Ø± Ù†ÛÛŒÚº Ø§ÙˆØ± ÚˆØ§Ú©Ù¹Ø±ÙˆÚº Ú©ÛŒ Ú©Ù…ÛŒ Ø§Ø³ Ú©ÛŒ ÙˆØ¬Û ÛÛ’ØŒ ØªÙˆ ØµÙˆØ¨Ø§Ø¦ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Ùˆ Ú©ÙˆÙ† Ø³ÛŒ ØªØ±Ø¨ÛŒØªÛŒ Ø§Ø³Ú©ÛŒÙ…ÛŒÚº Ø´Ø±ÙˆØ¹ Ú©Ø±Ù†ÛŒ Ú†Ø§ÛØ¦ÛŒÚºØŸ\n",
      "\n",
      "Decomposition:\n",
      "q1: Ø¯ÛŒÛÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº Ø¢Ø¨Ø§Ø¯ÛŒ Ú©Ø§ 70% Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØµØ­Øª Ú©ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª Ø³Û’ Ù…Ø­Ø±ÙˆÙ… ÛÛ’ØŸ\n",
      "q2: ÚˆØ§Ú©Ù¹Ø±ÙˆÚº Ú©ÛŒ Ú©Ù…ÛŒ Ø¯ÛŒÛÛŒ Ø¹Ù„Ø§Ù‚ÙˆÚº Ù…ÛŒÚº ØµØ­Øª Ú©ÛŒ Ø³ÛÙˆÙ„ÛŒØ§Øª ÙØ±Ø§ÛÙ… Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ú©Ø³ Ù‚Ø³Ù… Ú©Û’ Challenges Ú©Ùˆ Ù¾ÛŒØ´ Ú©Ø±Ø¯ÛŒØªÛŒ ÛÛ’ØŸ\n"
     ]
    }
   ],
   "source": [
    "queries = [your_query_1, your_query_2, your_query_3, your_query_4, your_query_5, your_query_6, your_query_7, your_query_8]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nTesting Query {i}:\")\n",
    "    print(query)\n",
    "    result = decompose_urdu_query(query)\n",
    "    print(\"\\nDecomposition:\")\n",
    "    print(f\"q1: {result.get('q1', 'N/A')}\")\n",
    "    print(f\"q2: {result.get('q2', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "896eb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def query_context_relevance_check(query_urdu: str, context_urdu: str) -> bool:\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in understanding the meaning and relevance of questions and their supporting information.\n",
    "\n",
    "Your task is to check whether a given *context* is relevant to a given *question*. Both the question and the context are written in Urdu.\n",
    "\n",
    "- If the context provides information that could help answer or understand the question, reply: True\n",
    "- If the context is not helpful or unrelated to the question, reply: False\n",
    "\n",
    "---\n",
    "\n",
    "Here is the input:\n",
    "\n",
    "Question (Urdu): {query_urdu}\n",
    "\n",
    "Context (Urdu): {context_urdu}\n",
    "\n",
    "Is the context relevant to the question?\n",
    "Answer (True/False):\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3:8b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    answer = response['message']['content'].strip().lower()\n",
    "    return 'true' in answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a49c9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_test_cases = [\n",
    "    {\n",
    "        \"query\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù‚ÙˆÙ…ÛŒ Ù¾Ú¾ÙˆÙ„ Ú©ÙˆÙ† Ø³Ø§ ÛÛ’ØŸ\",\n",
    "        \"context\": \"Ú©Ø±Ø§Ú†ÛŒ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø³Ø¨ Ø³Û’ Ø¨Ú‘Ø§ Ø´ÛØ± ÛÛ’Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ø§Ø±Ø¯Ùˆ Ø²Ø¨Ø§Ù† Ú©ÛŒ Ø§Ø¨ØªØ¯Ø§ Ú©ÛØ§Úº ÛÙˆØ¦ÛŒØŸ\",\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ú©Ø±Ú©Ù¹ Ù¹ÛŒÙ… Ù†Û’ ÙˆØ±Ù„Úˆ Ú©Ù¾ 1992 Ù…ÛŒÚº Ø¬ÛŒØªØ§Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ù‚Ø§Ø¦Ø¯Ø§Ø¹Ø¸Ù… Ú©ÛŒ ØªØ§Ø±ÛŒØ® Ù¾ÛŒØ¯Ø§Ø¦Ø´ Ú©ÛŒØ§ ÛÛ’ØŸ\",\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ù¾Ø±Ú†Ù… Ø¯Ùˆ Ø±Ù†Ú¯ÙˆÚº Ù¾Ø± Ù…Ø´ØªÙ…Ù„ ÛÛ’: Ø³Ø¨Ø² Ø§ÙˆØ± Ø³ÙÛŒØ¯Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ø®Ù„Ø§ Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„Ø§ Ø§Ù†Ø³Ø§Ù† Ú©ÙˆÙ† ØªÚ¾Ø§ØŸ\",\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ø¨Ù„Ù†Ø¯ Ù¾ÛØ§Ú‘ Ú©Û’ Ù¹Ùˆ ÛÛ’Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ Ù¾ÛÙ„Û’ ØµØ¯Ø± Ú©ÙˆÙ† ØªÚ¾Û’ØŸ\",\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û ØªØ± Ù„ÙˆÚ¯ Ø§Ø±Ø¯Ùˆ Ø§ÙˆØ± Ù¾Ù†Ø¬Ø§Ø¨ÛŒ Ø¨ÙˆÙ„ØªÛ’ ÛÛŒÚºÛ”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ø¹Ù„Ø§Ù…Û Ø§Ù‚Ø¨Ø§Ù„ Ú©ÛŒ ØªØ¹Ù„ÛŒÙ… Ú©ÛØ§Úº Ø³Û’ ÛÙˆØ¦ÛŒØŸ\",\n",
    "        \"context\": \"Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº Ø³Ø¨ Ø³Û’ Ù…Ø´ÛÙˆØ± Ø³Ø§Ø­Ù„ Ú©Ù„ÙÙ¹Ù† ÛÛ’Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ø¨Ø±ØµØºÛŒØ± Ú©ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ø¨ ÛÙˆØ¦ÛŒØŸ\",\n",
    "        \"context\": \"Ø¯Ø±ÛŒØ§Ø¦Û’ Ø³Ù†Ø¯Ú¾ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Ø§ Ø³Ø¨ Ø³Û’ Ø¨Ú‘Ø§ Ø¯Ø±ÛŒØ§ ÛÛ’Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ù„Ø§ÛÙˆØ± Ú©Ø³ Ø¯Ø±ÛŒØ§ Ú©Û’ Ú©Ù†Ø§Ø±Û’ ÙˆØ§Ù‚Ø¹ ÛÛ’ØŸ\",\n",
    "        \"context\": \"Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ú©Ùˆ 1960 Ù…ÛŒÚº Ø¯Ø§Ø±Ø§Ù„Ø­Ú©ÙˆÙ…Øª Ø¨Ù†Ø§ÛŒØ§ Ú¯ÛŒØ§ ØªÚ¾Ø§Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ø³Ø¨ Ø³Û’ Ø¨Ú‘ÛŒ ÛŒÙˆÙ†ÛŒÙˆØ±Ø³Ù¹ÛŒ Ú©ÙˆÙ† Ø³ÛŒ ÛÛ’ØŸ\",\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ú†Ø§Ø±ÙˆÚº Ù…ÙˆØ³Ù… ÛÙˆØªÛ’ ÛÛŒÚº: Ú¯Ø±Ù…ÛŒØŒ Ø³Ø±Ø¯ÛŒØŒ Ø®Ø²Ø§ÚºØŒ Ø§ÙˆØ± Ø¨ÛØ§Ø±Û”\",\n",
    "        \"expected\": False\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Ø¨Ø§Ø¨Ø§Ø¦Û’ Ù‚ÙˆÙ… Ú©Ø§ Ø®Ø·Ø§Ø¨ Ú©Ø³Û’ Ø¯ÛŒØ§ Ú¯ÛŒØ§ØŸ\",\n",
    "        \"context\": \"Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©ÛŒ Ø³Ø¨ Ø³Û’ Ù„Ù…Ø¨ÛŒ Ø´Ø§ÛØ±Ø§Û Ù‚Ø±Ø§Ù‚Ø±Ù… ÛØ§Ø¦ÛŒ ÙˆÛ’ ÛÛ’Û”\",\n",
    "        \"expected\": False\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6bfb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Expected: False, Got: False, âœ…\n",
      "Test 2: Expected: False, Got: False, âœ…\n",
      "Test 3: Expected: False, Got: False, âœ…\n",
      "Test 4: Expected: False, Got: False, âœ…\n",
      "Test 5: Expected: False, Got: False, âœ…\n",
      "Test 6: Expected: False, Got: False, âœ…\n",
      "Test 7: Expected: False, Got: False, âœ…\n",
      "Test 8: Expected: False, Got: False, âœ…\n",
      "Test 9: Expected: False, Got: False, âœ…\n",
      "Test 10: Expected: False, Got: False, âœ…\n"
     ]
    }
   ],
   "source": [
    "for i, case in enumerate(false_test_cases, start=1):\n",
    "    result = query_context_relevance_check(case[\"query\"], case[\"context\"])\n",
    "    print(f\"Test {i}: Expected: {case['expected']}, Got: {result}, {'âœ…' if result == case['expected'] else 'âŒ'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed992f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
